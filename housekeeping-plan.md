# ğŸ§¹ Plan de Housekeeping - Upgraded Happiness

## ğŸ¯ **Objetivo**: Organizar 93 archivos sin romper el sistema tricapa funcionando

## ğŸ“‹ **Estructura Propuesta** (Basada en tu audit)

```
upgraded-happiness/
â”œâ”€â”€ core/                           # 8 componentes sistema principal
â”‚   â”œâ”€â”€ lightweight_ml_detector.py
â”‚   â”œâ”€â”€ simple_firewall_agent.py
â”‚   â”œâ”€â”€ geoip_enricher.py
â”‚   â”œâ”€â”€ real_zmq_dashboard_with_firewall.py
â”‚   â”œâ”€â”€ promiscuous_agent.py
â”‚   â”œâ”€â”€ promiscuous_agent_v2.py
â”‚   â”œâ”€â”€ fixed_service_sniffer.py
â”‚   â””â”€â”€ enhanced_network_feature_extractor.py
â”‚
â”œâ”€â”€ ml_pipeline/                    # Pipeline de Machine Learning
â”‚   â”œâ”€â”€ trainers/
â”‚   â”‚   â”œâ”€â”€ sniffer_compatible_retrainer.py    # â†’ rf_production_sniffer_compatible.joblib
â”‚   â”‚   â”œâ”€â”€ train_specialized_models.py        # â†’ web/internal normal detectors
â”‚   â”‚   â”œâ”€â”€ advanced_trainer.py
â”‚   â”‚   â”œâ”€â”€ cicids_retrainer.py
â”‚   â”‚   â””â”€â”€ cicids_traditional_processor.py
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ model_analyzer_sniffer.py
â”‚   â”‚   â”œâ”€â”€ validate_ensemble_models.py
â”‚   â”‚   â””â”€â”€ extract_required_features.py
â”‚   â””â”€â”€ data_generators/
â”‚       â”œâ”€â”€ traffic_generator.py               # Ã‰PICO web crawler (?)
â”‚       â”œâ”€â”€ create_specialized_datasets.py     # Procesador de datos raw
â”‚       â””â”€â”€ [sniffer interno por identificar]
â”‚
â”œâ”€â”€ models/                         # Modelos organizados por tipo
â”‚   â”œâ”€â”€ production/                 # ğŸš€ PRODUCTION READY
â”‚   â”‚   â”œâ”€â”€ rf_production_sniffer_compatible.joblib      # 10.1MB - Detector ataques
â”‚   â”‚   â”œâ”€â”€ rf_production_sniffer_compatible_scaler.joblib
â”‚   â”‚   â”œâ”€â”€ web_normal_detector.joblib                   # 2.5MB - TrÃ¡fico web normal
â”‚   â”‚   â””â”€â”€ internal_normal_detector.joblib              # 2.3MB - TrÃ¡fico interno normal
â”‚   â””â”€â”€ archive/                    # Modelos experimentales/corruptos
â”‚       â””â”€â”€ [modelos no-production]
â”‚
â”œâ”€â”€ datasets/                       # Datos organizados por fuente
â”‚   â”œâ”€â”€ clean/                      # Datasets validados
â”‚   â”‚   â”œâ”€â”€ cicids_2017_processed.csv           # 1044.1MB - Para ataques
â”‚   â”‚   â””â”€â”€ specialized/
â”‚   â”‚       â”œâ”€â”€ web_normal_detector.csv         # Generado por Ã©pico crawler
â”‚   â”‚       â””â”€â”€ internal_normal_detector.csv    # Capturado localmente
â”‚   â””â”€â”€ corrupted/                  # Datasets problemÃ¡ticos
â”‚       â”œâ”€â”€ UNSW-NB15.csv
â”‚       â””â”€â”€ [otros datasets corruptos]
â”‚
â”œâ”€â”€ config/                         # 13 configuraciones JSON
â”œâ”€â”€ scripts/                        # 12 scripts bash/utilidades
â”œâ”€â”€ docs/                          # DocumentaciÃ³n
â”‚   â”œâ”€â”€ model_traceability.md      # Trazabilidad completa de modelos
â”‚   â””â”€â”€ data_generation_strategy.md # Tu estrategia Ã‰PICA de datos
â””â”€â”€ archive/                       # 33 archivos experimentales
```

## ğŸš¨ **ARCHIVOS CRÃTICOS** (NO TOCAR sin backup)

### Sistema Core (8 archivos)
- `lightweight_ml_detector.py` - Detector principal ML
- `promiscuous_agent_v2.py` - Sniffer principal (36.7KB)
- `real_zmq_dashboard_with_firewall.py` - Dashboard ZeroMQ

### Modelos de ProducciÃ³n (4 archivos)
- `rf_production_sniffer_compatible.joblib` - Modelo ataques (10.1MB)
- `web_normal_detector.joblib` - Modelo web normal (2.5MB) 
- `internal_normal_detector.joblib` - Modelo interno (2.3MB)
- `rf_production_sniffer_compatible_scaler.joblib`

### Trainers Identificados (3 archivos)
- `sniffer_compatible_retrainer.py` - EntrenÃ³ modelo de ataques
- `train_specialized_models.py` - EntrenÃ³ modelos normales
- `create_specialized_datasets.py` - ProcesÃ³ datos raw

## ğŸ“‹ **Secuencia de Housekeeping**

### Fase 1: InvestigaciÃ³n (ACTUAL)
- [ ] Identificar script del Ã©pico web crawler
- [ ] Localizar sniffer de captura interna  
- [ ] Mapear datasets generados
- [ ] Verificar trazabilidad completa

### Fase 2: Backup y PreparaciÃ³n
- [ ] `git commit -am "Pre-housekeeping snapshot"`
- [ ] Crear rama `housekeeping/file-organization`
- [ ] Backup completo local
- [ ] Verificar que sistema tricapa funciona

### Fase 3: ReorganizaciÃ³n Conservativa
- [ ] Crear nuevos directorios
- [ ] Mover archivos NO-CRÃTICOS primero
- [ ] Actualizar imports progresivamente
- [ ] Probar sistema despuÃ©s de cada batch

### Fase 4: ActualizaciÃ³n de DocumentaciÃ³n
- [ ] README.md con trazabilidad de modelos
- [ ] ROADMAP.md realista hacia 1.0.0
- [ ] Documentar estrategia Ã©pica de datos

## âš ï¸ **Reglas de Oro**

1. **NUNCA** mover archivos CORE sin probar
2. **SIEMPRE** actualizar imports despuÃ©s de mover
3. **BACKUP** antes de cada cambio mayor
4. **PROBAR** sistema tricapa despuÃ©s de reorganizar
5. **DOCUMENTAR** cada cambio para rollback

## ğŸ¯ **Criterio de Ã‰xito**

âœ… Sistema tricapa sigue funcionando  
âœ… Trazabilidad de modelos documentada  
âœ… Estructura clara y mantenible  
âœ… README/ROADMAP actualizados  
âœ… Base sÃ³lida para Protobuf v3.1