#!/usr/bin/env python3
"""
advanced_trainer_no_dns.py - VERSI√ìN CON TIMING DETALLADO
ü§ñ Enhanced ML Trainer para Upgraded-Happiness - MULTI-DATASET SIN DNS BIAS
- Filtrado autom√°tico de DNS para evitar sesgo
- Soporte m√∫ltiples datasets con mapeo correcto
- TIMING DETALLADO para entrenamientos largos
- Logging de progreso y ETAs
"""

import os
import json
import zipfile
import argparse
from pathlib import Path
from collections import Counter
import joblib
from datetime import datetime, timedelta
import geoip2.database
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from kaggle.api.kaggle_api_extended import KaggleApi
import shap
import time


# -----------------------------------------------------------------------------
# ‚è∞ CLASE DE TIMING DETALLADO
# -----------------------------------------------------------------------------
class DetailedTimer:
    """Clase para manejar timing detallado de entrenamientos largos"""

    def __init__(self):
        self.start_time = time.time()
        self.phase_times = {}
        self.current_phase = None
        self.phase_start = None

    def start_phase(self, phase_name: str):
        """Inicia una nueva fase con timing"""
        if self.current_phase:
            self.end_phase()

        self.current_phase = phase_name
        self.phase_start = time.time()
        elapsed = self.get_elapsed_time()
        print(f"\n‚è∞ [{elapsed}] INICIANDO: {phase_name}")

    def end_phase(self):
        """Termina la fase actual"""
        if self.current_phase and self.phase_start:
            duration = time.time() - self.phase_start
            self.phase_times[self.current_phase] = duration
            print(f"‚úÖ [{self.format_duration(duration)}] COMPLETADO: {self.current_phase}")
            self.current_phase = None
            self.phase_start = None

    def get_elapsed_time(self) -> str:
        """Obtiene tiempo transcurrido total"""
        elapsed = time.time() - self.start_time
        return self.format_duration(elapsed)

    def format_duration(self, seconds: float) -> str:
        """Formatea duraci√≥n en formato legible"""
        if seconds < 60:
            return f"{seconds:.1f}s"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.1f}m"
        else:
            hours = seconds / 3600
            minutes = (seconds % 3600) / 60
            return f"{hours:.1f}h {minutes:.0f}m"

    def log_progress(self, message: str):
        """Log de progreso con timestamp"""
        elapsed = self.get_elapsed_time()
        print(f"üìä [{elapsed}] {message}")

    def estimate_remaining(self, current_step: int, total_steps: int) -> str:
        """Estima tiempo restante basado en progreso"""
        if current_step == 0:
            return "calculando..."

        elapsed = time.time() - self.start_time
        rate = elapsed / current_step
        remaining_steps = total_steps - current_step
        estimated_remaining = rate * remaining_steps

        eta = datetime.now() + timedelta(seconds=estimated_remaining)
        return f"ETA: {eta.strftime('%H:%M:%S')} ({self.format_duration(estimated_remaining)} restante)"

    def print_summary(self):
        """Imprime resumen final de tiempos"""
        total_time = time.time() - self.start_time
        print(f"\n‚è∞ RESUMEN DE TIEMPOS - TOTAL: {self.format_duration(total_time)}")
        print("=" * 60)

        for phase, duration in self.phase_times.items():
            percentage = (duration / total_time) * 100
            print(f"  üìä {phase:<30} {self.format_duration(duration):>10} ({percentage:.1f}%)")

        print("=" * 60)
        finish_time = datetime.now()
        print(f"üèÅ Completado: {finish_time.strftime('%Y-%m-%d %H:%M:%S')}")


# Instancia global del timer
timer = DetailedTimer()


# -----------------------------------------------------------------------------
# üìÅ CONFIGURACI√ìN CENTRALIZADA
# -----------------------------------------------------------------------------
def load_config():
    config_path = Path("config-advanced-trainer-RF-Agression-NoAgression.json")
    if not config_path.exists():
        raise FileNotFoundError("No se encontr√≥ el archivo de configuraci√≥n.")
    with open(config_path, "r") as f:
        return json.load(f)


# -----------------------------------------------------------------------------
# üìö DICCIONARIO DE CARACTER√çSTICAS (PARA REPRODUCIBILIDAD)
# -----------------------------------------------------------------------------
FEATURE_DESCRIPTIONS = {
    # Caracter√≠sticas b√°sicas de flujo
    'dur': 'Duraci√≥n de la conexi√≥n en segundos',
    'proto': 'Protocolo de transporte (codificado num√©ricamente)',
    'service': 'Servicio de red (codificado num√©ricamente)',
    'state': 'Estado de la conexi√≥n (codificado num√©ricamente)',
    'spkts': 'N√∫mero de paquetes enviados por el origen',
    'dpkts': 'N√∫mero de paquetes enviados por el destino',
    'sbytes': 'N√∫mero de bytes enviados por el origen',
    'dbytes': 'N√∫mero de bytes enviados por el destino',
    'rate': 'Tasa de paquetes por segundo',
    'sttl': 'Time-to-live (TTL) del origen',
    'dttl': 'Time-to-live (TTL) del destino',
    'sload': 'Carga de datos del origen (bytes/segundo)',
    'dload': 'Carga de datos del destino (bytes/segundo)',
    'sloss': 'P√©rdida de paquetes del origen',
    'dloss': 'P√©rdida de paquetes del destino',
    'sinpkt': 'Intervalo entre paquetes entrantes (origen)',
    'dinpkt': 'Intervalo entre paquetes entrantes (destino)',

    # Caracter√≠sticas derivadas
    'packet_imbalance': 'Ratio entre paquetes origen/destino',
    'byte_imbalance': 'Ratio entre bytes origen/destino',
    'loss_ratio': 'Ratio de p√©rdida de paquetes del origen',

    # Caracter√≠sticas temporales
    'hour': 'Hora del d√≠a en que ocurri√≥ la conexi√≥n (0-23)',
    'day_of_week': 'D√≠a de la semana (0=Lunes, 6=Domingo)',
    'is_weekend': 'Indica si ocurri√≥ en fin de semana (1=S√≠, 0=No)',

    # Geolocalizaci√≥n
    'src_country': 'C√≥digo de pa√≠s del origen (codificado num√©ricamente)',
    'src_asn': 'N√∫mero de sistema aut√≥nomo (ASN) del origen',
    'country_risk': 'Puntuaci√≥n de riesgo del pa√≠s de origen (0-1)',
    'distance_km': 'Distancia en km desde la IP de origen a la sede central',

    # Caracter√≠sticas adicionales de seguridad
    'conn_state_abnormal': 'Indica si el estado de conexi√≥n es anormal (1=S√≠, 0=No)',
    'high_port_activity': 'Indica si se usaron puertos altos (>1024) en origen o destino'
}


# -----------------------------------------------------------------------------
# üåç GEOENRIQUECIMIENTO
# -----------------------------------------------------------------------------
class GeoEnricher:
    def __init__(self, config):
        self.geoip_city = geoip2.database.Reader(config['geo']['city_db_path'])
        self.geoip_country = geoip2.database.Reader(config['geo']['country_db_path'])
        self.hq_coords = tuple(config['geo']['hq_coords'])
        self.country_risk_scores = config['geo'].get('country_risk_scores', {})
        self.ip_cache = {}

    def enrich_ip(self, ip):
        if ip in self.ip_cache:
            return self.ip_cache[ip]

        try:
            response = self.geoip_city.city(ip)
            result = {
                'country': response.country.iso_code,
                'city': response.city.name,
                'asn': response.traits.autonomous_system_number,
                'lat': response.location.latitude,
                'lon': response.location.longitude,
                'country_risk': self.country_risk_scores.get(response.country.iso_code, 0.5),
                'distance_km': self.calculate_distance(
                    response.location.latitude,
                    response.location.longitude
                )
            }
        except:
            try:
                response = self.geoip_country.country(ip)
                result = {
                    'country': response.country.iso_code,
                    'city': "Unknown",
                    'asn': 0,
                    'lat': 0,
                    'lon': 0,
                    'country_risk': self.country_risk_scores.get(response.country.iso_code, 0.5),
                    'distance_km': 0
                }
            except:
                result = {
                    'country': "UNKNOWN",
                    'city': "Unknown",
                    'asn': 0,
                    'lat': 0,
                    'lon': 0,
                    'country_risk': 0.5,
                    'distance_km': 0
                }

        self.ip_cache[ip] = result
        return result

    def calculate_distance(self, lat, lon):
        if lat == 0 or lon == 0:
            return 0

        hq_lat, hq_lon = self.hq_coords
        lat_diff = abs(lat - hq_lat)
        lon_diff = abs(lon - hq_lon)
        return 111 * np.sqrt(lat_diff ** 2 + lon_diff ** 2)


# -----------------------------------------------------------------------------
# üö´ FILTRO DNS - NUEVA FUNCI√ìN CR√çTICA
# -----------------------------------------------------------------------------
def filter_dns_traffic(df, dataset_name):
    """
    Filtra el tr√°fico DNS para evitar sesgo de que DNS = ataque
    """
    initial_count = len(df)

    # Detectar columnas de servicio posibles
    service_columns = []
    for col in ['service', 'Service', 'protocol_type', 'service_name']:
        if col in df.columns:
            service_columns.append(col)

    if not service_columns:
        print(f"   ‚ö†Ô∏è  No se encontr√≥ columna de servicio en {dataset_name}")
        return df

    # Filtrar DNS de todas las columnas de servicio detectadas
    dns_filtered = 0
    for col in service_columns:
        if col in df.columns:
            # Contar DNS antes del filtro
            dns_mask = df[col].astype(str).str.lower().str.contains('dns', na=False)
            dns_count = dns_mask.sum()

            if dns_count > 0:
                print(f"   üö´ Filtrando {dns_count} filas DNS de columna '{col}' en {dataset_name}")

                # Analizar labels antes de eliminar
                if 'label' in df.columns or 'Label' in df.columns:
                    label_col = 'label' if 'label' in df.columns else 'Label'
                    dns_labels = df[dns_mask][label_col].value_counts()
                    print(f"      üìä DNS labels: {dict(dns_labels)}")

                # Filtrar DNS
                df = df[~dns_mask]
                dns_filtered += dns_count

    final_count = len(df)
    total_removed = initial_count - final_count

    print(f"   ‚úÖ {dataset_name}: {initial_count} ‚Üí {final_count} filas ({total_removed} DNS removidas)")

    return df


# -----------------------------------------------------------------------------
# ‚¨áÔ∏è CARGA DE DATASETS COMBINADOS (MODIFICADO)
# -----------------------------------------------------------------------------
def load_combined_datasets(config):
    dataset_names = config['ml'].get('datasets', ["UNSW-NB15"])
    datasets = []

    for name in dataset_names:
        try:
            print(f"\n[üìÅ] Procesando dataset: {name}")
            df = load_dataset(config, name)

            # NUEVO: Filtrar DNS ANTES de continuar
            print(f"[üö´] Filtrando tr√°fico DNS de {name}...")
            df = filter_dns_traffic(df, name)

            df['dataset_source'] = name
            datasets.append(df)
            print(f"[‚úÖ] {name} procesado con {len(df)} registros (sin DNS)")

            # Verificar etiquetas en el dataset despu√©s del filtro
            if 'label' in df.columns:
                print(f"  - Distribuci√≥n de etiquetas post-filtro: {dict(df['label'].value_counts())}")
            elif 'Label' in df.columns:
                print(f"  - Distribuci√≥n de etiquetas post-filtro: {dict(df['Label'].value_counts())}")
            else:
                print("  - No se encontr√≥ columna 'label' o 'Label'")

        except Exception as e:
            print(f"[‚ùå] Error cargando {name}: {str(e)}")

    if not datasets:
        raise ValueError("No se pudo cargar ning√∫n dataset")

    # Combinar datasets
    combined = pd.concat(datasets, ignore_index=True)
    print(f"\n[üìä] Total de registros combinados (sin DNS): {len(combined)}")

    # Verificar distribuci√≥n por dataset
    print(f"[üìã] Registros por dataset:")
    for name in dataset_names:
        count = len(combined[combined['dataset_source'] == name])
        print(f"  - {name}: {count:,} registros")

    return combined


def load_dataset(config, dataset_name, max_rows=0):
    base_data_dir = Path("./data")
    base_data_dir.mkdir(exist_ok=True)

    # ACTUALIZADO: Configuraciones para los 3 datasets
    dataset_config = {
        "UNSW-NB15": {
            "kaggle": "mrwellsdavid/unsw-nb15",
            "csv_file": "UNSW_NB15_training-set.csv",
            "label_col": "label"
        },
        "CSE-CIC-IDS2018": {
            "kaggle": "solarmainframe/ids-intrusion-csv",
            "csv_file": "Train_data.csv",
            "label_col": "Label"  # Ser√° detectado autom√°ticamente
        },
        "TON-IoT": {
            "kaggle": "programmer3/ton-iot-network-intrusion-dataset",
            "csv_file": "TON_IoT_Network_dataset.csv",
            "label_col": "label"
        }
    }

    ds_cfg = dataset_config[dataset_name]
    default_csv = base_data_dir / f"{dataset_name}.csv"

    # Intentar carga local
    if default_csv.exists():
        print(f"[üìÅ] Cargando {dataset_name} desde: {default_csv}")
        df = pd.read_csv(default_csv, nrows=max_rows if max_rows > 0 else None)

        # Verificar etiquetas durante la carga
        if ds_cfg["label_col"] in df.columns:
            label_counts = df[ds_cfg["label_col"]].value_counts()
            print(f"  - Distribuci√≥n inicial de etiquetas: {dict(label_counts)}")
        else:
            print(f"  - No se encontr√≥ columna de etiqueta: {ds_cfg['label_col']}")

        return df

    # Descargar desde Kaggle
    try:
        print(f"[‚¨áÔ∏è] Descargando {dataset_name} desde Kaggle...")
        api = KaggleApi()
        api.authenticate()
        api.dataset_download_files(ds_cfg["kaggle"], path=base_data_dir, quiet=False)

        # Buscar archivo ZIP descargado
        possible_zip_names = [
            f"{dataset_name}.zip",
            f"{ds_cfg['kaggle'].split('/')[-1]}.zip"
        ]

        zip_file = None
        for zip_name in possible_zip_names:
            potential_zip = base_data_dir / zip_name
            if potential_zip.exists():
                zip_file = potential_zip
                break

        if not zip_file:
            # Buscar cualquier ZIP reci√©n descargado
            zip_files = list(base_data_dir.glob("*.zip"))
            if zip_files:
                zip_file = max(zip_files, key=os.path.getctime)  # M√°s reciente
            else:
                raise FileNotFoundError(f"No se encontr√≥ archivo ZIP para {dataset_name}")

        print(f"[üì¶] Extrayendo {zip_file}...")
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            zip_ref.extractall(base_data_dir)

        # Buscar archivo CSV
        csv_path = None

        # Primero buscar el archivo espec√≠fico
        for file in base_data_dir.glob("**/*.csv"):
            if ds_cfg["csv_file"] in file.name:
                csv_path = file
                break

        # Si no se encuentra, tomar el CSV m√°s grande
        if not csv_path:
            csv_files = list(base_data_dir.glob("**/*.csv"))
            if csv_files:
                csv_path = max(csv_files, key=lambda x: x.stat().st_size)
                print(f"[üìÑ] Usando CSV m√°s grande encontrado: {csv_path.name}")

        if not csv_path:
            raise FileNotFoundError(f"No se encontr√≥ archivo CSV para {dataset_name}")

        print(f"[‚úÖ] {dataset_name} cargado desde: {csv_path}")
        df = pd.read_csv(csv_path, nrows=max_rows if max_rows > 0 else None)

        # Verificar etiquetas durante la carga
        if ds_cfg["label_col"] in df.columns:
            label_counts = df[ds_cfg["label_col"]].value_counts()
            print(f"  - Distribuci√≥n inicial de etiquetas: {dict(label_counts)}")
        else:
            print(f"  - No se encontr√≥ columna de etiqueta: {ds_cfg['label_col']}")

        # Guardar copia local
        df.to_csv(default_csv, index=False)
        print(f"[üíæ] Guardado copia local en: {default_csv}")

        return df

    except Exception as e:
        print(f"[‚ùå] Error al descargar {dataset_name}: {e}")
        raise


# -----------------------------------------------------------------------------
# üßπ PREPROCESAMIENTO AVANZADO (SIN CAMBIOS)
# -----------------------------------------------------------------------------
def advanced_preprocessing(df, geo_enricher):
    # 1. Manejo de caracter√≠sticas temporales
    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

    # 2. Enriquecimiento geogr√°fico
    print("[üåç] Enriqueciendo datos con geolocalizaci√≥n...")

    # Ampliar posibles nombres de columnas IP
    possible_ip_columns = [
        'src_ip', 'source_ip', 'ip_src', 'ip', 'srcaddr', 'Source IP',
        'src_ipv4', 'source_address', 'src_address', 'ip_source',
        'sourceip', 'srcip', 'source_ipv4', 'src_ipv4'
    ]

    ip_col = None
    for col in possible_ip_columns:
        if col in df.columns:
            ip_col = col
            print(f"[üîç] Usando columna IP: {ip_col}")
            break

    if ip_col is None:
        print("[‚ö†Ô∏è] No se encontr√≥ columna de direcci√≥n IP. Usando IPs dummy.")
        df['ip_dummy'] = "0.0.0.0"
        ip_col = 'ip_dummy'
    else:
        print(f"[üìç] Columna IP detectada: {ip_col}")

    # Solo hacer enriquecimiento si tenemos IPs reales
    if ip_col != 'ip_dummy':
        print(f"[üåê] Procesando {df[ip_col].nunique()} IPs √∫nicas...")
        geo_data = []
        unique_ips = df[ip_col].unique()

        for i, ip in enumerate(unique_ips):
            if i % 1000 == 0:
                print(f"  - Procesando IP {i + 1}/{len(unique_ips)}")
            geo_data.append((ip, geo_enricher.enrich_ip(ip)))

        geo_df = pd.DataFrame(
            [data[1] for data in geo_data],
            index=[data[0] for data in geo_data]
        )

        df = df.merge(geo_df, left_on=ip_col, right_index=True, how='left')
    else:
        print("[‚è©] Saltando enriquecimiento geogr√°fico - sin IPs reales")
        # A√±adir valores dummy
        df['src_country'] = "UNKNOWN"
        df['src_asn'] = 0
        df['country_risk'] = 0.5
        df['distance_km'] = 0

    # 3. Normalizaci√≥n de protocolos
    if 'proto' in df.columns:
        df['proto'] = df['proto'].str.lower().str.replace('[^a-z0-9]', '', regex=True)

    # 4. Eliminaci√≥n de columnas problem√°ticas (CORREGIDO - NO eliminar Label)
    drop_cols = ['id', 'attack_cat', 'flow_id', 'Unnamed: 0']
    # NOTA: NO eliminamos 'Label' porque contiene las etiquetas de clasificaci√≥n en CSE-CIC-IDS2018
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')

    # 5. Codificaci√≥n de variables categ√≥ricas
    cat_cols = ['proto', 'service', 'state', 'country', 'city']
    for col in cat_cols:
        if col in df.columns:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))

    # 6. Manejo de valores faltantes
    df = df.fillna(0)

    # 7. Eliminar columnas no num√©ricas residuales
    non_numeric_cols = ['timestamp', 'src_ip', 'dst_ip', 'city', 'country', 'ip_dummy']
    for col in non_numeric_cols:
        if col in df.columns:
            df = df.drop(columns=[col], errors='ignore')

    # 8. Eliminar columnas con nombres duplicados
    df = df.loc[:, ~df.columns.duplicated()]

    return df


# -----------------------------------------------------------------------------
# ‚öñÔ∏è BALANCEO H√çBRIDO (SIN CAMBIOS)
# -----------------------------------------------------------------------------
def hybrid_balancing(X, y):
    print("[‚öñÔ∏è] Aplicando balanceo h√≠brido...")

    # Contar muestras por clase
    class_counts = Counter(y)
    print(f"[üìä] Distribuci√≥n inicial: {class_counts}")

    if len(class_counts) < 2:
        raise ValueError("Solo se encontr√≥ una clase. Se necesitan ambas clases para balancear.")

    majority_class = max(class_counts, key=class_counts.get)
    minority_class = min(class_counts, key=class_counts.get)

    # Estrategia de balanceo
    sampling_strategy = {
        majority_class: int(class_counts[majority_class] * 0.7),
        minority_class: min(class_counts[majority_class] * 2, class_counts[minority_class] * 10)
    }

    print(f"[‚öñÔ∏è] Estrategia de balanceo: {sampling_strategy}")

    # Pipeline de balanceo
    pipeline = Pipeline([
        ('oversample', SMOTE(sampling_strategy={minority_class: sampling_strategy[minority_class]},
                             random_state=42)),
        ('undersample', RandomUnderSampler(sampling_strategy={majority_class: sampling_strategy[majority_class]},
                                           random_state=42))
    ])

    X_res, y_res = pipeline.fit_resample(X, y)
    print(f"[‚öñÔ∏è] Distribuci√≥n despu√©s de balanceo: {Counter(y_res)}")
    return X_res, y_res


# -----------------------------------------------------------------------------
# üß† ENTRENAMIENTO DE ALTA PRECISI√ìN (SIN CAMBIOS)
# -----------------------------------------------------------------------------
def train_high_precision_rf(X_train, y_train, config):
    # Par√°metros optimizados para m√°xima precisi√≥n
    rf_params = {
        'n_estimators': 2000,
        'max_depth': 120,
        'min_samples_split': 2,
        'min_samples_leaf': 1,
        'max_features': 'log2',
        'bootstrap': True,
        'oob_score': True,
        'n_jobs': -1,
        'random_state': 42,
        'class_weight': 'balanced_subsample',
        'ccp_alpha': 0.0001,
        'max_samples': 0.8
    }

    # Sobreescribir con configuraci√≥n si existe
    if 'random_forest' in config['ml']['models']:
        rf_params.update(config['ml']['models']['random_forest'])

    print("[üéØ] Entrenando RandomForest para m√°xima precisi√≥n:")
    for k, v in rf_params.items():
        print(f"  - {k}: {v}")

    # Validaci√≥n cruzada estratificada con timing detallado
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    fold_scores = []
    models = []

    total_folds = 5
    timer.log_progress(f"Iniciando validaci√≥n cruzada: {total_folds} folds")

    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):
        fold_start = time.time()

        X_fold_train, y_fold_train = X_train[train_idx], y_train[train_idx]
        X_val, y_val = X_train[val_idx], y_train[val_idx]

        timer.log_progress(f"Fold {fold + 1}/{total_folds}: Entrenando con {len(X_fold_train):,} muestras...")

        model = RandomForestClassifier(**rf_params)
        model.fit(X_fold_train, y_fold_train)

        # Calcular precisi√≥n en el fold de validaci√≥n
        y_pred = model.predict(X_val)
        accuracy = np.mean(y_pred == y_val)
        fold_scores.append(accuracy)
        models.append(model)

        fold_duration = time.time() - fold_start
        remaining_time = timer.estimate_remaining(fold + 1, total_folds)

        print(
            f"[üîç] Fold {fold + 1} - Precisi√≥n: {accuracy:.4f} - Tiempo: {timer.format_duration(fold_duration)} - {remaining_time}")

    # Seleccionar el mejor modelo
    best_idx = np.argmax(fold_scores)
    best_model = models[best_idx]
    print(f"[üèÜ] Mejor fold: {best_idx + 1} con precisi√≥n {fold_scores[best_idx]:.4f}")

    # Entrenar modelo final con todos los datos
    timer.log_progress("Entrenando modelo final con todos los datos...")
    final_model = RandomForestClassifier(**rf_params)
    final_model.fit(X_train, y_train)

    # Usar SHAP para explicabilidad
    timer.log_progress("Generando explicaciones SHAP (muestra de 1000)...")
    explainer = shap.TreeExplainer(final_model)
    shap_values = explainer.shap_values(X_train[:1000])  # Muestra representativa

    return final_model, explainer


# -----------------------------------------------------------------------------
# üìä EVALUACI√ìN AVANZADA (SIN CAMBIOS)
# -----------------------------------------------------------------------------
def advanced_evaluation(model, X_test, y_test):
    print("[üß™] Evaluando modelo...")
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    print("\n[üìä] Matriz de Confusi√≥n:")
    print(confusion_matrix(y_test, y_pred))

    print("\n[üìã] Reporte de Clasificaci√≥n:")
    print(classification_report(y_test, y_pred))

    # Calcular AUC-ROC y AUC-PR
    roc_auc = roc_auc_score(y_test, y_proba)
    precision, recall, _ = precision_recall_curve(y_test, y_proba)
    pr_auc = auc(recall, precision)

    print(f"[üìà] AUC-ROC: {roc_auc:.4f}")
    print(f"[üìà] AUC-PR: {pr_auc:.4f}")

    # Calcular m√©tricas espec√≠ficas para seguridad
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
    precision_pos = tp / (tp + fp) if (tp + fp) > 0 else 0

    print("\n[üîí] M√©tricas de Seguridad:")
    print(f"  - Tasa de Falsos Positivos (FPR): {fpr:.4f}")
    print(f"  - Tasa de Falsos Negativos (FNR): {fnr:.4f}")
    print(f"  - Tasa de Detecci√≥n: {detection_rate:.4f}")
    print(f"  - Precisi√≥n en Amenazas: {precision_pos:.4f}")

    return {
        'roc_auc': roc_auc,
        'pr_auc': pr_auc,
        'fpr': fpr,
        'fnr': fnr,
        'detection_rate': detection_rate,
        'precision_pos': precision_pos
    }


# -----------------------------------------------------------------------------
# üíæ GUARDADO DE ARTEFACTOS (SIN CAMBIOS)
# -----------------------------------------------------------------------------
def save_advanced_artifacts(model, explainer, config, features, geo_enricher, evaluation_metrics, scaler):
    os.makedirs("models", exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_dir = Path("models") / f"model_{timestamp}"
    model_dir.mkdir(exist_ok=True)

    # 1. Guardar modelo
    model_path = model_dir / "model.pkl"
    joblib.dump(model, model_path)

    # 2. Guardar explainer SHAP
    explainer_path = model_dir / "shap_explainer.pkl"
    joblib.dump(explainer, explainer_path)

    # 3. Guardar metadatos detallados (ACTUALIZADO con timing)
    metadata = {
        "training_date": timestamp,
        "dns_filtered": True,  # NUEVO
        "datasets_used": config['ml'].get('datasets', []),  # NUEVO
        "timing_info": {  # NUEVO
            "total_training_time": timer.get_elapsed_time(),
            "phase_times": timer.phase_times,
            "training_start": datetime.fromtimestamp(timer.start_time).isoformat()
        },
        "feature_set": {
            "features": features,
            "descriptions": {feat: FEATURE_DESCRIPTIONS.get(feat, "Descripci√≥n no disponible")
                             for feat in features}
        },
        "model_params": model.get_params(),
        "geo_config": {
            "hq_coords": config['geo']['hq_coords'],
            "country_risk_scores": geo_enricher.country_risk_scores
        },
        "evaluation_metrics": evaluation_metrics,
        "scapy_feature_template": generate_scapy_template(features)
    }

    metadata_path = model_dir / "metadata.json"
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=4, default=str)

    # 4. Guardar sistema de reputaci√≥n ASN
    asn_reputation = build_asn_reputation(model, features)
    asn_path = model_dir / "asn_reputation.pkl"
    joblib.dump(asn_reputation, asn_path)

    # 5. Guardar escalador
    scaler_path = model_dir / "scaler.pkl"
    joblib.dump(scaler, scaler_path)

    print(f"[üíæ] Artefactos guardados en: {model_dir}")
    print(f"  - Modelo: {model_path}")
    print(f"  - Metadatos: {metadata_path}")
    print(f"  - Escalador: {scaler_path}")
    print(f"  - SHAP Explainer: {explainer_path}")
    print(f"  - Reputaci√≥n ASN: {asn_path}")

    timer.log_progress(f"Todos los artefactos guardados en {model_dir}")


def generate_scapy_template(features):
    """Genera plantilla para captura Scapy"""
    template = {
        "required_features": [],
        "feature_mapping": {}
    }

    scapy_mapping = {
        'dur': 'packet.time_delta',
        'proto': 'packet[IP].proto',
        'spkts': 'packet_count_src',
        'dpkts': 'packet_count_dst',
        'sbytes': 'packet_length_src',
        'dbytes': 'packet_length_dst',
        'sttl': 'packet[IP].ttl',
        # ... otros mapeos
    }

    for feat in features:
        if feat in scapy_mapping:
            template['required_features'].append(feat)
            template['feature_mapping'][feat] = scapy_mapping[feat]
        elif feat in ['packet_imbalance', 'byte_imbalance', 'loss_ratio']:
            template['feature_mapping'][feat] = f"Calculated from other features"
        elif feat in FEATURE_DESCRIPTIONS:
            template['required_features'].append(feat)
            template['feature_mapping'][feat] = "Custom calculation"

    return template


def build_asn_reputation(model, features):
    # Este ser√≠a un proceso m√°s complejo en producci√≥n
    # Aqu√≠ un ejemplo simplificado
    return {
        "last_updated": datetime.now().isoformat(),
        "risk_factors": ["known_malicious", "high_attack_rate"],
        "version": "1.0"
    }


# -----------------------------------------------------------------------------
# üöÄ FUNCI√ìN PRINCIPAL (MODIFICADA PARA M√öLTIPLES DATASETS)
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# üöÄ FUNCI√ìN PRINCIPAL CON TIMING DETALLADO
# -----------------------------------------------------------------------------
def main():
    print("üöÄ ADVANCED TRAINER v2.3 - MULTI-DATASET CON TIMING DETALLADO")
    print("=" * 70)
    start_time = datetime.now()
    print(f"üïê Iniciado: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")

    timer.start_phase("Configuraci√≥n inicial")
    config = load_config()
    geo_enricher = GeoEnricher(config)
    timer.end_phase()

    # Cargar y combinar datasets (CON FILTRO DNS)
    timer.start_phase("Carga y combinaci√≥n de datasets")
    print("[üîç] Cargando y combinando datasets...")
    print(f"[üìã] Datasets configurados: {config['ml']['datasets']}")
    df = load_combined_datasets(config)
    timer.log_progress(f"Datasets combinados: {len(df):,} registros")
    timer.end_phase()

    # Preprocesamiento avanzado
    timer.start_phase("Preprocesamiento avanzado")
    print("\n[üßπ] Realizando preprocesamiento avanzado...")
    df = advanced_preprocessing(df, geo_enricher)
    timer.log_progress(f"Preprocesamiento completado: {df.shape}")
    timer.end_phase()

    # Asignaci√≥n robusta de etiquetas (MEJORADA PARA M√öLTIPLES DATASETS)
    timer.start_phase("Mapeo de etiquetas")
    print("[üè∑Ô∏è] Asignando etiquetas unificadas...")

    # Definir mapeos de etiquetas por dataset (CORREGIDO FINALMENTE)
    label_mappings = {
        'UNSW-NB15': ('label', lambda x: x),  # Mantener valores originales
        'CSE-CIC-IDS2018': ('Label', lambda x: 0 if str(x).lower() == 'benign' else 1),  # CORRECTO: Label may√∫scula
        'TON-IoT': ('label', lambda x: 1 if x != 'normal' else 0)
    }

    # Inicializar columna de etiqueta unificada
    df['unified_label'] = np.nan

    for name in config['ml']['datasets']:
        if name in label_mappings:
            col_name, converter = label_mappings[name]

            if col_name in df.columns:
                print(f"[üîñ] Mapeando etiquetas para {name} usando columna '{col_name}'")

                # Aplicar conversi√≥n solo al subconjunto del dataset
                mask = df['dataset_source'] == name
                df.loc[mask, 'unified_label'] = df.loc[mask, col_name].apply(converter)

                # Verificar distribuci√≥n despu√©s del mapeo
                subset = df[df['dataset_source'] == name]
                counts = subset['unified_label'].value_counts()
                print(f"  - Distribuci√≥n despu√©s de mapeo: {dict(counts)}")
            else:
                print(f"[‚ö†Ô∏è] Columna '{col_name}' no encontrada en {name}")
                # Debug: mostrar columnas disponibles
                subset = df[df['dataset_source'] == name]
                if not subset.empty:
                    print(f"  - Columnas disponibles: {list(subset.columns)}")
                    # Buscar columnas parecidas
                    similar_cols = [c for c in subset.columns if 'label' in c.lower()]
                    if similar_cols:
                        print(f"  - Columnas similares encontradas: {similar_cols}")
                continue
        else:
            print(f"[‚ö†Ô∏è] No se encontr√≥ mapeo para {name}")
            continue

    # Verificar que todas las etiquetas est√©n asignadas
    if df['unified_label'].isna().any():
        missing_count = df['unified_label'].isna().sum()
        print(f"[‚ùå] Error: {missing_count} filas sin etiqueta asignada")

        # Mostrar diagn√≥stico
        print("[üîç] Diagn√≥stico por dataset:")
        for name in config['ml']['datasets']:
            subset = df[df['dataset_source'] == name]
            if not subset.empty:
                missing_in_subset = subset['unified_label'].isna().sum()
                print(f"  - {name}: {missing_in_subset} filas sin etiqueta")

        raise ValueError("Algunas filas no tienen etiqueta asignada")

    # Verificar distribuci√≥n global
    class_counts = df['unified_label'].value_counts()
    print(f"\n[üìä] Distribuci√≥n global de etiquetas:\n{class_counts}")

    # Calcular porcentajes
    total = len(df)
    normal_pct = (class_counts.get(0.0, 0) / total) * 100
    attack_pct = (class_counts.get(1.0, 0) / total) * 100
    timer.log_progress(f"Distribuci√≥n final: {normal_pct:.1f}% normal, {attack_pct:.1f}% ataques")

    if len(class_counts) < 2:
        print("[üîç] Analizando distribuci√≥n por dataset:")
        for name in config['ml']['datasets']:
            subset = df[df['dataset_source'] == name]
            if not subset.empty:
                counts = subset['unified_label'].value_counts()
                print(f"  - {name}: {dict(counts)}")

        raise ValueError(
            "Solo se encontr√≥ una clase en los datos. Se necesitan ambas clases (benigno/maligno) para entrenar.")
    timer.end_phase()

    # Preparar datos para entrenamiento
    timer.start_phase("Preparaci√≥n de datos")
    print("[‚öôÔ∏è] Preparando datos para entrenamiento...")
    X = df.drop(columns=['unified_label', 'dataset_source'])
    y = df['unified_label']

    print(f"[üî¢] Dimensiones iniciales: X={X.shape}, y={y.shape}")

    # Filtrar solo caracter√≠sticas num√©ricas esenciales
    print("[üîç] Filtrando caracter√≠sticas num√©ricas...")
    numeric_features = [
        'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes',
        'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt',
        'packet_imbalance', 'byte_imbalance', 'loss_ratio', 'hour', 'day_of_week', 'is_weekend',
        'src_country', 'src_asn', 'country_risk', 'distance_km',
        'conn_state_abnormal', 'high_port_activity'
    ]

    # Mantener solo las columnas que existen y son num√©ricas
    available_features = [col for col in numeric_features if col in X.columns]
    X = X[available_features]
    feature_names = list(X.columns)
    print(f"[üî¢] Caracter√≠sticas finales ({len(feature_names)}): {feature_names}")

    # Verificar que todas las columnas sean num√©ricas
    non_numeric = X.select_dtypes(exclude=['number']).columns
    if not non_numeric.empty:
        print(f"[‚ö†Ô∏è] Columnas no num√©ricas detectadas: {list(non_numeric)}")
        print("[üîÑ] Codificando caracter√≠sticas categ√≥ricas...")
        for col in non_numeric:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))

    # Verificar clases antes del balanceo
    class_dist = Counter(y)
    print(f"[‚öñÔ∏è] Distribuci√≥n antes de balanceo: {class_dist}")
    print(f"[üîç] Valores √∫nicos en y: {np.unique(y)}")
    timer.end_phase()

    # Balanceo h√≠brido
    timer.start_phase("Balanceo de datos")
    print("[‚öñÔ∏è] Aplicando balanceo h√≠brido...")
    try:
        X_balanced, y_balanced = hybrid_balancing(X.values, y.values)
        balanced_dist = Counter(y_balanced)
        timer.log_progress(f"Balanceo completado: {dict(balanced_dist)}")
    except Exception as e:
        print(f"[‚ùå] Error en balanceo: {e}")
        print("[üîç] Verificando distribuci√≥n por dataset:")
        for name in config['ml']['datasets']:
            subset = df[df['dataset_source'] == name]
            if not subset.empty:
                counts = subset['unified_label'].value_counts()
                print(f"  - {name}: {dict(counts)}")
        raise
    timer.end_phase()

    # Divisi√≥n de datos
    timer.start_phase("Divisi√≥n de datos")
    print("[‚úÇÔ∏è] Dividiendo datos...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_balanced, y_balanced,
        test_size=0.25,
        stratify=y_balanced,
        random_state=42
    )

    train_size = len(X_train)
    test_size = len(X_test)
    timer.log_progress(f"Divisi√≥n completada: {train_size:,} entrenamiento, {test_size:,} prueba")
    timer.end_phase()

    # Escalado de caracter√≠sticas
    timer.start_phase("Escalado de caracter√≠sticas")
    print("[üìè] Escalando caracter√≠sticas...")
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    timer.end_phase()

    # Entrenamiento de alta precisi√≥n
    timer.start_phase("Entrenamiento del modelo")
    print("[üß†] Entrenando modelo de alta precisi√≥n...")
    timer.log_progress("Iniciando entrenamiento RandomForest con validaci√≥n cruzada...")
    model, explainer = train_high_precision_rf(X_train, y_train, config)
    timer.log_progress("Entrenamiento del modelo completado")
    timer.end_phase()

    # Evaluaci√≥n avanzada
    timer.start_phase("Evaluaci√≥n del modelo")
    print("[üìä] Evaluando modelo...")
    eval_metrics = advanced_evaluation(model, X_test, y_test)
    timer.end_phase()

    # Guardar artefactos con gesti√≥n de caracter√≠sticas
    timer.start_phase("Guardado de artefactos")
    print("[üíæ] Guardando artefactos...")
    save_advanced_artifacts(model, explainer, config, feature_names, geo_enricher, eval_metrics, scaler)

    # Guardar feature order
    with open("models/feature_order.txt", "w") as f:
        f.write("\n".join(feature_names))
    timer.end_phase()

    # Resumen final
    timer.print_summary()

    final_time = datetime.now()
    total_duration = final_time - start_time

    print(f"\n‚úÖ ENTRENAMIENTO AVANZADO COMPLETADO CON √âXITO!")
    print(f"üö´ DNS filtrado de todos los datasets para evitar sesgo")
    print(f"üìä Datasets utilizados: {config['ml']['datasets']}")
    print(f"‚è∞ Tiempo total: {total_duration}")
    print(f"üèÅ Finalizado: {final_time.strftime('%Y-%m-%d %H:%M:%S')}")

    # Guardar resumen de timing
    timing_summary = {
        "start_time": start_time.isoformat(),
        "end_time": final_time.isoformat(),
        "total_duration_seconds": total_duration.total_seconds(),
        "phase_times": timer.phase_times,
        "datasets_used": config['ml']['datasets'],
        "final_distribution": dict(Counter(y_balanced)),
        "model_metrics": eval_metrics
    }

    with open("models/training_timing_summary.json", "w") as f:
        json.dump(timing_summary, f, indent=2, default=str)

    print(f"üìä Resumen de timing guardado en: models/training_timing_summary.json")


# -----------------------------------------------------------------------------
# üèÅ EJECUCI√ìN
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Entrenador avanzado de modelos de seguridad - SIN DNS")
    parser.add_argument("--max_rows", type=int, default=0,
                        help="L√≠mite de filas a cargar por dataset (0 = sin l√≠mite)")
    parser.add_argument("--use_local", action="store_true",
                        help="Usar solo datasets locales sin descargar")
    args = parser.parse_args()

    main()