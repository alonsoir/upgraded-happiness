#!/usr/bin/env python3
"""
Comprehensive Project Audit - Upgraded Happiness
AnÃ¡lisis completo respetando TODA la arquitectura del sistema
"""

import os
import glob
import subprocess
from datetime import datetime
from pathlib import Path


def read_makefile_targets():
    """Lee el Makefile para entender la estructura oficial del proyecto"""

    print("ğŸ“‹ ANALIZANDO MAKEFILE PARA ESTRUCTURA OFICIAL")
    print("=" * 60)

    makefile_files = []
    makefile_targets = []

    if os.path.exists('Makefile'):
        try:
            with open('Makefile', 'r') as f:
                content = f.read()

            print("âœ… Makefile encontrado - extrayendo informaciÃ³n oficial...")

            # Buscar archivos mencionados en el Makefile
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('#') or not line:
                    continue

                # Buscar archivos .py mencionados
                if '.py' in line and not line.startswith('\t'):
                    # Extraer nombres de archivos .py
                    words = line.split()
                    for word in words:
                        if word.endswith('.py') and not word.startswith('-'):
                            makefile_files.append(word.strip(':'))

                # Buscar targets/objetivos
                if ':' in line and not line.startswith('\t') and not '=' in line:
                    target = line.split(':')[0].strip()
                    if target and not target.startswith('.'):
                        makefile_targets.append(target)

            if makefile_files:
                print("ğŸ ARCHIVOS PYTHON EN MAKEFILE:")
                for py_file in sorted(set(makefile_files)):
                    status = "âœ… EXISTS" if os.path.exists(py_file) else "âŒ MISSING"
                    print(f"   {status} {py_file}")

            if makefile_targets:
                print(f"\nğŸ¯ TARGETS EN MAKEFILE:")
                for target in sorted(set(makefile_targets)):
                    print(f"   ğŸ“Œ {target}")

        except Exception as e:
            print(f"âš ï¸  Error leyendo Makefile: {e}")
    else:
        print("âš ï¸  Makefile no encontrado")

    print()
    return makefile_files, makefile_targets


def analyze_all_project_files():
    """AnÃ¡lisis exhaustivo de TODOS los archivos del proyecto"""

    print("ğŸ” ANÃLISIS EXHAUSTIVO DE ARCHIVOS DEL PROYECTO")
    print("=" * 60)

    file_categories = {
        'SYSTEM_CORE': {
            'description': 'ğŸ† SISTEMA CORE - Network/ML/Security (CRÃTICO)',
            'files': [],
            'patterns': [
                'simple_firewall_agent.py',
                'promiscuous_agent*.py',
                'geoip_enricher.py',
                'lightweight_ml_detector.py',
                'real_zmq_dashboard*.py',
                'fixed_service_sniffer.py',
                'enhanced_network_feature_extractor.py'
            ]
        },
        'ML_PIPELINE': {
            'description': 'ğŸ¤– PIPELINE ML - Entrenamiento/Modelos (CRÃTICO)',
            'files': [],
            'patterns': [
                '*trainer*.py',
                '*retrainer*.py',
                'model_analyzer*.py',
                'validate_ensemble*.py',
                'hybrid_dataset_generator.py'
            ]
        },
        'DATA_PIPELINE': {
            'description': 'ğŸ“Š PIPELINE DATOS - Descarga/Limpieza (CRÃTICO)',
            'files': [],
            'patterns': [
                '*processor*.py',
                '*cicids*.py',
                '*audit*.py',
                'process-raw-data*.py',
                'extract_required_features.py'
            ]
        },
        'TRAFFIC_CAPTURE': {
            'description': 'ğŸŒ CAPTURA TRÃFICO - GeneraciÃ³n datos entrenamiento (CRÃTICO)',
            'files': [],
            'patterns': [
                '*sniffer*.py',
                '*capture*.py',
                '*traffic*.py'
            ]
        },
        'CONFIGURATION': {
            'description': 'âš™ï¸ CONFIGURACIÃ“N - Sistema/Modelos (ESENCIAL)',
            'files': [],
            'patterns': ['*.json']
        },
        'DOCUMENTATION': {
            'description': 'ğŸ“š DOCUMENTACIÃ“N - README/ROADMAP (ESENCIAL)',
            'files': [],
            'patterns': ['README*', 'ROADMAP*', 'requirements.txt', 'Makefile', '*.md']
        },
        'SCRIPTS_SUPPORT': {
            'description': 'ğŸ”§ SCRIPTS SOPORTE - Bash/Utilidades (MANTENER)',
            'files': [],
            'patterns': ['*.sh', '*.bash']
        },
        'LEGACY_VALUABLE': {
            'description': 'ğŸ“¦ LEGACY VALIOSO - Versiones anteriores Ãºtiles',
            'files': [],
            'patterns': []  # Se llena manualmente
        },
        'EXPERIMENTAL': {
            'description': 'ğŸ§ª EXPERIMENTAL - Para evaluar',
            'files': [],
            'patterns': []  # Se llena manualmente
        }
    }

    # Obtener todos los archivos del proyecto
    all_files = []
    for ext in ['*.py', '*.json', '*.sh', '*.bash', '*.md', '*.txt', 'Makefile', 'ROADMAP', 'README']:
        all_files.extend(glob.glob(ext))

    # Categorizar archivos
    categorized = set()

    for category, info in file_categories.items():
        for pattern in info['patterns']:
            matches = glob.glob(pattern)
            for match in matches:
                if os.path.isfile(match) and match not in categorized:
                    file_size = os.path.getsize(match) / 1024
                    mod_time = datetime.fromtimestamp(os.path.getmtime(match))
                    lines = count_lines(match) if match.endswith('.py') else 0

                    info['files'].append({
                        'name': match,
                        'size_kb': file_size,
                        'modified': mod_time,
                        'lines': lines
                    })
                    categorized.add(match)

    # Archivos especÃ­ficos que sabemos son legacy pero valiosos
    legacy_valuable = [
        'debug_ml_network_sniffer.py',
        'auto_detect_ml_network_sniffer.py',
        'real_time_ml_network_sniffer.py'
    ]

    for legacy_file in legacy_valuable:
        if os.path.exists(legacy_file) and legacy_file not in categorized:
            file_size = os.path.getsize(legacy_file) / 1024
            mod_time = datetime.fromtimestamp(os.path.getmtime(legacy_file))
            lines = count_lines(legacy_file)

            file_categories['LEGACY_VALUABLE']['files'].append({
                'name': legacy_file,
                'size_kb': file_size,
                'modified': mod_time,
                'lines': lines
            })
            categorized.add(legacy_file)

    # Archivos restantes van a experimental
    for file in all_files:
        if file not in categorized and os.path.isfile(file):
            file_size = os.path.getsize(file) / 1024
            mod_time = datetime.fromtimestamp(os.path.getmtime(file))
            lines = count_lines(file) if file.endswith('.py') else 0

            file_categories['EXPERIMENTAL']['files'].append({
                'name': file,
                'size_kb': file_size,
                'modified': mod_time,
                'lines': lines
            })

    # Mostrar resultados
    total_files = 0
    for category, info in file_categories.items():
        if info['files']:
            print(f"\n{info['description']}")
            print("-" * len(info['description']))

            for file_info in sorted(info['files'], key=lambda x: x['modified'], reverse=True):
                lines_info = f", {file_info['lines']:>4d} lÃ­neas" if file_info['lines'] > 0 else ""
                print(f"   ğŸ“„ {file_info['name']:<40} "
                      f"({file_info['size_kb']:>6.1f}KB{lines_info}, "
                      f"{file_info['modified'].strftime('%m/%d %H:%M')})")
                total_files += 1

    print(f"\nğŸ“Š TOTAL ARCHIVOS ANALIZADOS: {total_files}")
    return file_categories


def count_lines(filename):
    """Cuenta lÃ­neas de cÃ³digo en un archivo"""
    try:
        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
            return sum(1 for line in f if line.strip() and not line.strip().startswith('#'))
    except:
        return 0


def analyze_models_and_data():
    """AnÃ¡lisis de modelos y datasets"""

    print("\nğŸ¤– ANÃLISIS DE MODELOS Y DATASETS")
    print("=" * 50)

    # Modelos
    model_files = glob.glob("models/*.joblib") + glob.glob("models/*/*.joblib")
    if model_files:
        print("ğŸ§  MODELOS ENCONTRADOS:")
        for model in sorted(model_files):
            size_mb = os.path.getsize(model) / (1024 * 1024)
            mod_time = datetime.fromtimestamp(os.path.getmtime(model))

            if 'sniffer_compatible' in model:
                status = "ğŸ† PRODUCTION READY"
            elif 'cicids' in model:
                status = "âœ… TRAINED ON CLEAN DATA"
            elif 'final' in model or 'unsw' in model.lower():
                status = "âš ï¸  TRAINED ON CORRUPTED DATA"
            else:
                status = "ğŸ”§ UTILITY MODEL"

            print(f"   {status:<25} {os.path.basename(model):<40} "
                  f"({size_mb:>5.1f}MB, {mod_time.strftime('%m/%d %H:%M')})")

    # Datasets grandes
    data_files = []
    for pattern in ["*.csv", "data/*.csv", "datasets/*.csv"]:
        data_files.extend(glob.glob(pattern))

    large_datasets = [f for f in data_files if os.path.getsize(f) > 10 * 1024 * 1024]  # > 10MB

    if large_datasets:
        print("\nğŸ“Š DATASETS ENCONTRADOS:")
        for data in sorted(large_datasets):
            size_mb = os.path.getsize(data) / (1024 * 1024)
            mod_time = datetime.fromtimestamp(os.path.getmtime(data))

            if 'cicids_2017_processed' in data:
                status = "ğŸ† CLEAN & PROCESSED"
            elif 'UNSW' in data and 'NB15' in data:
                status = "âŒ CORRUPTED (confirmed)"
            elif 'cicids' in data.lower():
                status = "âœ… CICIDS FAMILY"
            else:
                status = "ğŸ“Š DATASET"

            print(f"   {status:<25} {os.path.basename(data):<40} "
                  f"({size_mb:>6.1f}MB, {mod_time.strftime('%m/%d %H:%M')})")


def generate_organization_recommendations():
    """Recomendaciones de organizaciÃ³n conservadoras"""

    print("\nğŸ’¡ RECOMENDACIONES DE ORGANIZACIÃ“N")
    print("=" * 50)

    print("ğŸ¯ FILOSOFÃA: CONSERVAR TODO LO VALIOSO, SOLO ORGANIZAR")
    print()

    organization_plan = {
        "MANTENER TODO EL SISTEMA CORE": [
            "âœ… simple_firewall_agent.py â†’ MANTENER (componente core)",
            "âœ… promiscuous_agent*.py â†’ MANTENER (componente core)",
            "âœ… geoip_enricher.py â†’ MANTENER (componente core)",
            "âœ… lightweight_ml_detector.py â†’ MANTENER (componente core)",
            "âœ… real_zmq_dashboard*.py â†’ MANTENER (componente core)",
            "âœ… fixed_service_sniffer.py â†’ MANTENER (reciÃ©n arreglado)",
            "âœ… enhanced_network_feature_extractor.py â†’ MANTENER (features crÃ­ticas)"
        ],

        "MANTENER TODO EL PIPELINE ML": [
            "âœ… *trainer*.py â†’ MANTENER (esenciales para re-entrenamiento)",
            "âœ… *retrainer*.py â†’ MANTENER (capacidad de mejora continua)",
            "âœ… model_analyzer*.py â†’ MANTENER (validaciÃ³n de modelos)",
            "âœ… validate_ensemble*.py â†’ MANTENER (testing de modelos)"
        ],

        "MANTENER TODO EL PIPELINE DE DATOS": [
            "âœ… *processor*.py â†’ MANTENER (descarga/limpieza datasets)",
            "âœ… *cicids*.py â†’ MANTENER (acceso a datos limpios)",
            "âœ… *audit*.py â†’ MANTENER (validaciÃ³n de calidad de datos)",
            "âœ… extract_required_features.py â†’ MANTENER (pipeline features)"
        ],

        "MANTENER DOCUMENTACIÃ“N COMPLETA": [
            "âœ… README â†’ MANTENER (documentaciÃ³n principal)",
            "âœ… ROADMAP â†’ MANTENER (planificaciÃ³n proyecto)",
            "âœ… requirements.txt â†’ MANTENER (dependencias)",
            "âœ… Makefile â†’ MANTENER (automatizaciÃ³n)",
            "âœ… *.json â†’ MANTENER TODOS (configuraciones)"
        ],

        "ARCHIVAR (NO ELIMINAR) LEGACY VALIOSO": [
            "ğŸ“¦ debug_ml_network_sniffer.py â†’ archive/debugging/",
            "ğŸ“¦ real_time_ml_network_sniffer.py â†’ archive/versions/",
            "ğŸ“¦ models con datos corruptos â†’ archive/corrupted_models/",
            "ğŸ“¦ UNSW-NB15.csv â†’ archive/corrupted_datasets/"
        ],

        "ESTRUCTURA PROPUESTA CONSERVADORA": [
            "core/ â†’ Componentes sistema principal (firewall, agents, ML)",
            "ml_pipeline/ â†’ Todo el pipeline ML (trainers, analyzers)",
            "data_pipeline/ â†’ Scripts descarga/procesamiento datasets",
            "config/ â†’ Todas las configuraciones JSON",
            "models/ â†’ Modelos organizados por estado (production/, archive/)",
            "docs/ â†’ DocumentaciÃ³n (README, ROADMAP, etc.)",
            "scripts/ â†’ Scripts bash y utilidades",
            "archive/ â†’ Legacy valioso pero no en producciÃ³n activa"
        ]
    }

    for category, items in organization_plan.items():
        print(f"\n{category}:")
        for item in items:
            print(f"   {item}")


def generate_next_steps():
    """PrÃ³ximos pasos hacia RELEASE"""

    print(f"\nğŸš€ PRÃ“XIMOS PASOS HACIA RELEASE")
    print("=" * 50)

    next_steps = [
        "FASE 1 - ORGANIZACIÃ“N CONSERVADORA:",
        "   ğŸ—‚ï¸  Crear estructura de directorios sin mover archivos aÃºn",
        "   ğŸ“‹ Inventario completo de dependencias entre archivos",
        "   ğŸ”— Mapear todas las interconexiones del sistema",
        "",
        "FASE 2 - DOCUMENTACIÃ“N EXHAUSTIVA:",
        "   ğŸ“š Documentar cada componente del sistema",
        "   ğŸ§­ Actualizar ROADMAP con lecciones aprendidas",
        "   ğŸ“– Crear guÃ­as de uso para cada pipeline",
        "",
        "FASE 3 - TESTING COMPREHENSIVE:",
        "   ğŸ§ª Suite de tests para todo el sistema",
        "   âœ… ValidaciÃ³n de cada componente individualmente",
        "   ğŸ”„ Testing de integraciÃ³n completa",
        "",
        "FASE 4 - OPTIMIZACIÃ“N SIN ROMPER:",
        "   âš¡ Optimizaciones de performance",
        "   ğŸ“Š Mejoras de logging y monitoring",
        "   ğŸ”§ Configuraciones externalizadas",
        "",
        "FASE 5 - CONTAINERIZACIÃ“N Y CI/CD:",
        "   ğŸ³ Docker para todo el stack",
        "   ğŸš€ Pipeline de deployment",
        "   ğŸ“ˆ Monitoring en producciÃ³n",
        "",
        "FILOSOFÃA: 'SI FUNCIONA, NO LO ROMPAS - SOLO MEJÃ“RALO'"
    ]

    for step in next_steps:
        print(step)


def main():
    """FunciÃ³n principal del audit comprehensivo"""

    print("ğŸ” COMPREHENSIVE PROJECT AUDIT - UPGRADED HAPPINESS")
    print("ğŸ¯ Respetando TODA la arquitectura del sistema completo")
    print("=" * 80)
    print(f"Ejecutado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Directorio: {os.getcwd()}")
    print()

    # AnÃ¡lisis completo
    makefile_files, makefile_targets = read_makefile_targets()
    file_categories = analyze_all_project_files()
    analyze_models_and_data()
    generate_organization_recommendations()
    generate_next_steps()

    print(f"\nğŸ¯ RESUMEN EJECUTIVO:")
    print(f"   âœ… Sistema ML funcionando correctamente")
    print(f"   âœ… Pipeline completo de datos/entrenamiento identificado")
    print(f"   âœ… Arquitectura completa de red/security mapeada")
    print(f"   âœ… Todos los componentes crÃ­ticos preservados")
    print(f"   ğŸ¯ Objetivo: Organizar sin romper, preparar para RELEASE")

    print("\n" + "=" * 80)
    print("ğŸ† SISTEMA COMPREHENSIVE MAPEADO - LISTO PARA ORGANIZACIÃ“N")
    print("   FilosofÃ­a: Conservar todo lo valioso, solo mejorar la organizaciÃ³n")
    print("=" * 80)


if __name__ == "__main__":
    main()