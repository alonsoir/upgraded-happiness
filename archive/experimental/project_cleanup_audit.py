#!/usr/bin/env python3
"""
Comprehensive Project Audit - Upgraded Happiness
An√°lisis completo respetando TODA la arquitectura del sistema
"""

import os
import glob
import subprocess
from datetime import datetime
from pathlib import Path


def read_makefile_targets():
    """Lee el Makefile para entender la estructura oficial del proyecto"""

    print("üìã ANALIZANDO MAKEFILE PARA ESTRUCTURA OFICIAL")
    print("=" * 60)

    makefile_files = []
    makefile_targets = []

    if os.path.exists('Makefile'):
        try:
            with open('Makefile', 'r') as f:
                content = f.read()

            print("‚úÖ Makefile encontrado - extrayendo informaci√≥n oficial...")

            # Buscar archivos mencionados en el Makefile
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if line.startswith('#') or not line:
                    continue

                # Buscar archivos .py mencionados
                if '.py' in line and not line.startswith('\t'):
                    # Extraer nombres de archivos .py
                    words = line.split()
                    for word in words:
                        if word.endswith('.py') and not word.startswith('-'):
                            makefile_files.append(word.strip(':'))

                # Buscar targets/objetivos
                if ':' in line and not line.startswith('\t') and not '=' in line:
                    target = line.split(':')[0].strip()
                    if target and not target.startswith('.'):
                        makefile_targets.append(target)

            if makefile_files:
                print("üêç ARCHIVOS PYTHON EN MAKEFILE:")
                for py_file in sorted(set(makefile_files)):
                    status = "‚úÖ EXISTS" if os.path.exists(py_file) else "‚ùå MISSING"
                    print(f"   {status} {py_file}")

            if makefile_targets:
                print(f"\nüéØ TARGETS EN MAKEFILE:")
                for target in sorted(set(makefile_targets)):
                    print(f"   üìå {target}")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error leyendo Makefile: {e}")
    else:
        print("‚ö†Ô∏è  Makefile no encontrado")

    print()
    return makefile_files, makefile_targets


def analyze_all_project_files():
    """An√°lisis exhaustivo de TODOS los archivos del proyecto"""

    print("üîç AN√ÅLISIS EXHAUSTIVO DE ARCHIVOS DEL PROYECTO")
    print("=" * 60)

    file_categories = {
        'SYSTEM_CORE': {
            'description': 'üèÜ SISTEMA CORE - Network/ML/Security (CR√çTICO)',
            'files': [],
            'patterns': [
                'simple_firewall_agent.py',
                'promiscuous_agent*.py',
                'geoip_enricher.py',
                'lightweight_ml_detector.py',
                'real_zmq_dashboard*.py',
                'fixed_service_sniffer.py',
                'enhanced_network_feature_extractor.py'
            ]
        },
        'ML_PIPELINE': {
            'description': 'ü§ñ PIPELINE ML - Entrenamiento/Modelos (CR√çTICO)',
            'files': [],
            'patterns': [
                '*trainer*.py',
                '*retrainer*.py',
                'model_analyzer*.py',
                'validate_ensemble*.py',
                'hybrid_dataset_generator.py'
            ]
        },
        'DATA_PIPELINE': {
            'description': 'üìä PIPELINE DATOS - Descarga/Limpieza (CR√çTICO)',
            'files': [],
            'patterns': [
                '*processor*.py',
                '*cicids*.py',
                '*audit*.py',
                'process-raw-data*.py',
                'extract_required_features.py'
            ]
        },
        'TRAFFIC_CAPTURE': {
            'description': 'üåê CAPTURA TR√ÅFICO - Generaci√≥n datos entrenamiento (CR√çTICO)',
            'files': [],
            'patterns': [
                '*sniffer*.py',
                '*capture*.py',
                '*traffic*.py'
            ]
        },
        'CONFIGURATION': {
            'description': '‚öôÔ∏è CONFIGURACI√ìN - Sistema/Modelos (ESENCIAL)',
            'files': [],
            'patterns': ['*.json']
        },
        'DOCUMENTATION': {
            'description': 'üìö DOCUMENTACI√ìN - README/ROADMAP (ESENCIAL)',
            'files': [],
            'patterns': ['README*', 'ROADMAP*', 'requirements.txt', 'Makefile', '*.md']
        },
        'SCRIPTS_SUPPORT': {
            'description': 'üîß SCRIPTS SOPORTE - Bash/Utilidades (MANTENER)',
            'files': [],
            'patterns': ['*.sh', '*.bash']
        },
        'LEGACY_VALUABLE': {
            'description': 'üì¶ LEGACY VALIOSO - Versiones anteriores √∫tiles',
            'files': [],
            'patterns': []  # Se llena manualmente
        },
        'EXPERIMENTAL': {
            'description': 'üß™ EXPERIMENTAL - Para evaluar',
            'files': [],
            'patterns': []  # Se llena manualmente
        }
    }

    # Obtener todos los archivos del proyecto
    all_files = []
    for ext in ['*.py', '*.json', '*.sh', '*.bash', '*.md', '*.txt', 'Makefile', 'ROADMAP', 'README']:
        all_files.extend(glob.glob(ext))

    # Categorizar archivos
    categorized = set()

    for category, info in file_categories.items():
        for pattern in info['patterns']:
            matches = glob.glob(pattern)
            for match in matches:
                if os.path.isfile(match) and match not in categorized:
                    file_size = os.path.getsize(match) / 1024
                    mod_time = datetime.fromtimestamp(os.path.getmtime(match))
                    lines = count_lines(match) if match.endswith('.py') else 0

                    info['files'].append({
                        'name': match,
                        'size_kb': file_size,
                        'modified': mod_time,
                        'lines': lines
                    })
                    categorized.add(match)

    # Archivos espec√≠ficos que sabemos son legacy pero valiosos
    legacy_valuable = [
        'debug_ml_network_sniffer.py',
        'auto_detect_ml_network_sniffer.py',
        'real_time_ml_network_sniffer.py'
    ]

    for legacy_file in legacy_valuable:
        if os.path.exists(legacy_file) and legacy_file not in categorized:
            file_size = os.path.getsize(legacy_file) / 1024
            mod_time = datetime.fromtimestamp(os.path.getmtime(legacy_file))
            lines = count_lines(legacy_file)

            file_categories['LEGACY_VALUABLE']['files'].append({
                'name': legacy_file,
                'size_kb': file_size,
                'modified': mod_time,
                'lines': lines
            })
            categorized.add(legacy_file)

    # Archivos restantes van a experimental
    for file in all_files:
        if file not in categorized and os.path.isfile(file):
            file_size = os.path.getsize(file) / 1024
            mod_time = datetime.fromtimestamp(os.path.getmtime(file))
            lines = count_lines(file) if file.endswith('.py') else 0

            file_categories['EXPERIMENTAL']['files'].append({
                'name': file,
                'size_kb': file_size,
                'modified': mod_time,
                'lines': lines
            })

    # Mostrar resultados
    total_files = 0
    for category, info in file_categories.items():
        if info['files']:
            print(f"\n{info['description']}")
            print("-" * len(info['description']))

            for file_info in sorted(info['files'], key=lambda x: x['modified'], reverse=True):
                lines_info = f", {file_info['lines']:>4d} l√≠neas" if file_info['lines'] > 0 else ""
                print(f"   üìÑ {file_info['name']:<40} "
                      f"({file_info['size_kb']:>6.1f}KB{lines_info}, "
                      f"{file_info['modified'].strftime('%m/%d %H:%M')})")
                total_files += 1

    print(f"\nüìä TOTAL ARCHIVOS ANALIZADOS: {total_files}")
    return file_categories


def count_lines(filename):
    """Cuenta l√≠neas de c√≥digo en un archivo"""
    try:
        with open(filename, 'r', encoding='utf-8', errors='ignore') as f:
            return sum(1 for line in f if line.strip() and not line.strip().startswith('#'))
    except:
        return 0


def analyze_models_and_data():
    """An√°lisis de modelos y datasets"""

    print("\nü§ñ AN√ÅLISIS DE MODELOS Y DATASETS")
    print("=" * 50)

    # Modelos
    model_files = glob.glob("models/*.joblib") + glob.glob("models/*/*.joblib")
    if model_files:
        print("üß† MODELOS ENCONTRADOS:")
        for model in sorted(model_files):
            size_mb = os.path.getsize(model) / (1024 * 1024)
            mod_time = datetime.fromtimestamp(os.path.getmtime(model))

            if 'sniffer_compatible' in model:
                status = "üèÜ PRODUCTION READY"
            elif 'cicids' in model:
                status = "‚úÖ TRAINED ON CLEAN DATA"
            elif 'final' in model or 'unsw' in model.lower():
                status = "‚ö†Ô∏è  TRAINED ON CORRUPTED DATA"
            else:
                status = "üîß UTILITY MODEL"

            print(f"   {status:<25} {os.path.basename(model):<40} "
                  f"({size_mb:>5.1f}MB, {mod_time.strftime('%m/%d %H:%M')})")

    # Datasets grandes
    data_files = []
    for pattern in ["*.csv", "data/*.csv", "datasets/*.csv"]:
        data_files.extend(glob.glob(pattern))

    large_datasets = [f for f in data_files if os.path.getsize(f) > 10 * 1024 * 1024]  # > 10MB

    if large_datasets:
        print("\nüìä DATASETS ENCONTRADOS:")
        for data in sorted(large_datasets):
            size_mb = os.path.getsize(data) / (1024 * 1024)
            mod_time = datetime.fromtimestamp(os.path.getmtime(data))

            if 'cicids_2017_processed' in data:
                status = "üèÜ CLEAN & PROCESSED"
            elif 'UNSW' in data and 'NB15' in data:
                status = "‚ùå CORRUPTED (confirmed)"
            elif 'cicids' in data.lower():
                status = "‚úÖ CICIDS FAMILY"
            else:
                status = "üìä DATASET"

            print(f"   {status:<25} {os.path.basename(data):<40} "
                  f"({size_mb:>6.1f}MB, {mod_time.strftime('%m/%d %H:%M')})")


def generate_organization_recommendations():
    """Recomendaciones de organizaci√≥n conservadoras"""

    print("\nüí° RECOMENDACIONES DE ORGANIZACI√ìN")
    print("=" * 50)

    print("üéØ FILOSOF√çA: CONSERVAR TODO LO VALIOSO, SOLO ORGANIZAR")
    print()

    organization_plan = {
        "MANTENER TODO EL SISTEMA CORE": [
            "‚úÖ simple_firewall_agent.py ‚Üí MANTENER (componente core)",
            "‚úÖ promiscuous_agent*.py ‚Üí MANTENER (componente core)",
            "‚úÖ geoip_enricher.py ‚Üí MANTENER (componente core)",
            "‚úÖ lightweight_ml_detector.py ‚Üí MANTENER (componente core)",
            "‚úÖ real_zmq_dashboard*.py ‚Üí MANTENER (componente core)",
            "‚úÖ fixed_service_sniffer.py ‚Üí MANTENER (reci√©n arreglado)",
            "‚úÖ enhanced_network_feature_extractor.py ‚Üí MANTENER (features cr√≠ticas)"
        ],

        "MANTENER TODO EL PIPELINE ML": [
            "‚úÖ *trainer*.py ‚Üí MANTENER (esenciales para re-entrenamiento)",
            "‚úÖ *retrainer*.py ‚Üí MANTENER (capacidad de mejora continua)",
            "‚úÖ model_analyzer*.py ‚Üí MANTENER (validaci√≥n de modelos)",
            "‚úÖ validate_ensemble*.py ‚Üí MANTENER (testing de modelos)"
        ],

        "MANTENER TODO EL PIPELINE DE DATOS": [
            "‚úÖ *processor*.py ‚Üí MANTENER (descarga/limpieza datasets)",
            "‚úÖ *cicids*.py ‚Üí MANTENER (acceso a datos limpios)",
            "‚úÖ *audit*.py ‚Üí MANTENER (validaci√≥n de calidad de datos)",
            "‚úÖ extract_required_features.py ‚Üí MANTENER (pipeline features)"
        ],

        "MANTENER DOCUMENTACI√ìN COMPLETA": [
            "‚úÖ README ‚Üí MANTENER (documentaci√≥n principal)",
            "‚úÖ ROADMAP ‚Üí MANTENER (planificaci√≥n proyecto)",
            "‚úÖ requirements.txt ‚Üí MANTENER (dependencias)",
            "‚úÖ Makefile ‚Üí MANTENER (automatizaci√≥n)",
            "‚úÖ *.json ‚Üí MANTENER TODOS (configuraciones)"
        ],

        "ARCHIVAR (NO ELIMINAR) LEGACY VALIOSO": [
            "üì¶ debug_ml_network_sniffer.py ‚Üí archive/debugging/",
            "üì¶ real_time_ml_network_sniffer.py ‚Üí archive/versions/",
            "üì¶ models con datos corruptos ‚Üí archive/corrupted_models/",
            "üì¶ UNSW-NB15.csv ‚Üí archive/corrupted_datasets/"
        ],

        "ESTRUCTURA PROPUESTA CONSERVADORA": [
            "core/ ‚Üí Componentes sistema principal (firewall, agents, ML)",
            "ml_pipeline/ ‚Üí Todo el pipeline ML (trainers, analyzers)",
            "data_pipeline/ ‚Üí Scripts descarga/procesamiento datasets",
            "config/ ‚Üí Todas las configuraciones JSON",
            "models/ ‚Üí Modelos organizados por estado (production/, archive/)",
            "docs/ ‚Üí Documentaci√≥n (README, ROADMAP, etc.)",
            "scripts/ ‚Üí Scripts bash y utilidades",
            "archive/ ‚Üí Legacy valioso pero no en producci√≥n activa"
        ]
    }

    for category, items in organization_plan.items():
        print(f"\n{category}:")
        for item in items:
            print(f"   {item}")


def generate_next_steps():
    """Pr√≥ximos pasos hacia RELEASE"""

    print(f"\nüöÄ PR√ìXIMOS PASOS HACIA RELEASE")
    print("=" * 50)

    next_steps = [
        "FASE 1 - ORGANIZACI√ìN CONSERVADORA:",
        "   üóÇÔ∏è  Crear estructura de directorios sin mover archivos a√∫n",
        "   üìã Inventario completo de dependencias entre archivos",
        "   üîó Mapear todas las interconexiones del sistema",
        "",
        "FASE 2 - DOCUMENTACI√ìN EXHAUSTIVA:",
        "   üìö Documentar cada componente del sistema",
        "   üß≠ Actualizar ROADMAP con lecciones aprendidas",
        "   üìñ Crear gu√≠as de uso para cada pipeline",
        "",
        "FASE 3 - TESTING COMPREHENSIVE:",
        "   üß™ Suite de tests para todo el sistema",
        "   ‚úÖ Validaci√≥n de cada componente individualmente",
        "   üîÑ Testing de integraci√≥n completa",
        "",
        "FASE 4 - OPTIMIZACI√ìN SIN ROMPER:",
        "   ‚ö° Optimizaciones de performance",
        "   üìä Mejoras de logging y monitoring",
        "   üîß Configuraciones externalizadas",
        "",
        "FASE 5 - CONTAINERIZACI√ìN Y CI/CD:",
        "   üê≥ Docker para todo el stack",
        "   üöÄ Pipeline de deployment",
        "   üìà Monitoring en producci√≥n",
        "",
        "FILOSOF√çA: 'SI FUNCIONA, NO LO ROMPAS - SOLO MEJ√ìRALO'"
    ]

    for step in next_steps:
        print(step)


def main():
    """Funci√≥n principal del audit comprehensivo"""

    print("üîç COMPREHENSIVE PROJECT AUDIT - UPGRADED HAPPINESS")
    print("üéØ Respetando TODA la arquitectura del sistema completo")
    print("=" * 80)
    print(f"Ejecutado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Directorio: {os.getcwd()}")
    print()

    # An√°lisis completo
    makefile_files, makefile_targets = read_makefile_targets()
    file_categories = analyze_all_project_files()
    analyze_models_and_data()
    generate_organization_recommendations()
    generate_next_steps()

    print(f"\nüéØ RESUMEN EJECUTIVO:")
    print(f"   ‚úÖ Sistema ML funcionando correctamente")
    print(f"   ‚úÖ Pipeline completo de datos/entrenamiento identificado")
    print(f"   ‚úÖ Arquitectura completa de red/security mapeada")
    print(f"   ‚úÖ Todos los componentes cr√≠ticos preservados")
    print(f"   üéØ Objetivo: Organizar sin romper, preparar para RELEASE")

    print("\n" + "=" * 80)
    print("üèÜ SISTEMA COMPREHENSIVE MAPEADO - LISTO PARA ORGANIZACI√ìN")
    print("   Filosof√≠a: Conservar todo lo valioso, solo mejorar la organizaci√≥n")
    print("=" * 80)


if __name__ == "__main__":
    main()