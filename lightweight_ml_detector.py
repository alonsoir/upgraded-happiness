#!/usr/bin/env python3
"""
Lightweight ML Detector para Upgraded-Happiness (LIMPIO)
REFACTORIZADO: Lee TODA la configuraciÃ³n desde JSON
RESPONSABILIDAD ÃšNICA: AnÃ¡lisis ML + scoring de eventos
ELIMINADO: GeoIP logic (ahora en geoip_enricher.py)
CORREGIDO: ZMQ pattern PULL/PUSH para pipeline secuencial
"""

import zmq
import time
import logging
import threading
import numpy as np
import json
import os
import sys
import math
from collections import deque, defaultdict
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# Configurar logging bÃ¡sico (se reconfigurarÃ¡ desde JSON)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Importar protobuf - USAR ESTRUCTURAS REALES
try:
    from src.protocols.protobuf import network_event_extended_fixed_pb2

    PROTOBUF_AVAILABLE = True
    logger.info("âœ… Protobuf network_event_extended_fixed_pb2 importado desde src.protocols.protobuf")
except ImportError:
    try:
        import network_event_extended_fixed_pb2

        PROTOBUF_AVAILABLE = True
        logger.info("âœ… Protobuf network_event_extended_fixed_pb2 importado desde directorio local")
    except ImportError:
        PROTOBUF_AVAILABLE = False
        logger.error("âŒ Protobuf no disponible")

# Importar ML dependencies
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler

    ML_AVAILABLE = True
    logger.info("âœ… Scikit-learn disponible para ML")
except ImportError:
    ML_AVAILABLE = False
    logger.warning("âš ï¸  Scikit-learn no disponible - ML deshabilitado")


class SimpleMLModel:
    """Modelo ML simple para detecciÃ³n de anomalÃ­as configurado desde JSON"""

    def __init__(self, ml_config: Dict):
        """Inicializar modelo ML desde configuraciÃ³n JSON"""
        self.config = ml_config

        self.anomaly_detector = None
        self.scaler = StandardScaler()
        self.is_trained = False
        self.training_data = deque(maxlen=self.config.get('training', {}).get('min_training_samples', 1000))

        # CaracterÃ­sticas desde configuraciÃ³n
        self.feature_names = self.config.get('features', [
            'packet_size', 'dest_port', 'src_port',
            'hour', 'minute', 'is_weekend',
            'ip_entropy', 'port_frequency'
        ])

        # ConfiguraciÃ³n del modelo desde JSON
        self.confidence_threshold = self.config.get('confidence_threshold', 0.7)
        self.contamination_rate = self.config.get('training', {}).get('contamination_rate', 0.1)

        # EstadÃ­sticas para features
        self.ip_stats = defaultdict(int)
        self.port_stats = defaultdict(int)

        if ML_AVAILABLE:
            self.anomaly_detector = IsolationForest(
                contamination=self.contamination_rate,
                random_state=42,
                n_estimators=100
            )
            logger.info("ğŸ¤– Modelo ML inicializado desde configuraciÃ³n JSON")
        else:
            logger.warning("âš ï¸  Modelo ML no disponible - usando heurÃ­sticas")

    def extract_features(self, event_data: Dict) -> np.ndarray:
        """Extrae caracterÃ­sticas del evento para ML"""

        # CaracterÃ­sticas bÃ¡sicas
        packet_size = event_data.get('packet_size', 0)
        dest_port = event_data.get('dest_port', 0)
        src_port = event_data.get('src_port', 0)

        # CaracterÃ­sticas temporales
        now = datetime.now()
        hour = now.hour
        minute = now.minute
        is_weekend = 1 if now.weekday() >= 5 else 0

        # CaracterÃ­sticas de IP (entropÃ­a simple)
        source_ip = event_data.get('source_ip', '')
        ip_entropy = len(set(source_ip.replace('.', ''))) / max(len(source_ip), 1)

        # Frecuencia de puerto
        self.port_stats[dest_port] += 1
        port_frequency = self.port_stats[dest_port]

        return np.array([
            packet_size, dest_port, src_port,
            hour, minute, is_weekend,
            ip_entropy, port_frequency
        ])

    def train_or_update(self, features: np.ndarray):
        """Entrena o actualiza el modelo usando configuraciÃ³n"""
        if not ML_AVAILABLE:
            return

        self.training_data.append(features)

        min_samples = self.config.get('training', {}).get('min_training_samples', 100)

        # Entrenar cuando tengamos suficientes datos
        if len(self.training_data) >= min_samples and not self.is_trained:
            X = np.array(list(self.training_data))
            X_scaled = self.scaler.fit_transform(X)
            self.anomaly_detector.fit(X_scaled)
            self.is_trained = True
            logger.info("ğŸ¯ Modelo ML entrenado con %d muestras", len(self.training_data))

        # Reentrenar periÃ³dicamente segÃºn configuraciÃ³n
        retrain_samples = self.config.get('training', {}).get('retrain_interval_samples', 200)
        if self.is_trained and len(self.training_data) % retrain_samples == 0:
            X = np.array(list(self.training_data))
            X_scaled = self.scaler.fit_transform(X)
            self.anomaly_detector.fit(X_scaled)
            logger.info("ğŸ”„ Modelo ML reentrenado")

    def predict_anomaly(self, features: np.ndarray) -> Tuple[float, float]:
        """Predice anomalÃ­a y score de riesgo usando configuraciÃ³n"""

        if not ML_AVAILABLE or not self.is_trained:
            # Usar heurÃ­sticas simples
            return self._heuristic_prediction(features)

        try:
            # Usar modelo ML
            X_scaled = self.scaler.transform(features.reshape(1, -1))
            anomaly_score = self.anomaly_detector.decision_function(X_scaled)[0]

            # Normalizar score (-1 a 1) -> (0 a 1)
            anomaly_score = max(0, min(1, (1 - anomaly_score) / 2))

            # Calcular risk score basado en mÃºltiples factores
            risk_score = self._calculate_risk_score(features, anomaly_score)

            return anomaly_score, risk_score

        except Exception as e:
            logger.error("Error en predicciÃ³n ML: %s", e)
            return self._heuristic_prediction(features)

    def _heuristic_prediction(self, features: np.ndarray) -> Tuple[float, float]:
        """PredicciÃ³n heurÃ­stica cuando ML no estÃ¡ disponible"""

        packet_size, dest_port, src_port = features[0], features[1], features[2]

        anomaly_score = 0.0
        risk_score = 0.0

        # HeurÃ­stica 1: TamaÃ±o de paquete anÃ³malo
        if packet_size > 1500 or packet_size < 20:
            anomaly_score += 0.3

        # HeurÃ­stica 2: Puertos sospechosos
        suspicious_ports = [22, 23, 135, 139, 445, 1433, 3389, 5900]
        if dest_port in suspicious_ports:
            risk_score += 0.4

        # HeurÃ­stica 3: Puertos no estÃ¡ndar
        if dest_port > 49152 or (dest_port < 1024 and dest_port not in [80, 443, 22, 21]):
            anomaly_score += 0.2

        # HeurÃ­stica 4: Combinaciones sospechosas
        if dest_port == 22 and packet_size < 100:  # SSH con paquetes pequeÃ±os
            risk_score += 0.3

        return min(anomaly_score, 1.0), min(risk_score, 1.0)

    def _calculate_risk_score(self, features: np.ndarray, anomaly_score: float) -> float:
        """Calcula score de riesgo basado en mÃºltiples factores"""

        packet_size, dest_port, src_port = features[0], features[1], features[2]
        hour = features[3]

        risk_score = anomaly_score * 0.5  # Base del score de anomalÃ­a

        # Factor 1: Puertos de alto riesgo
        high_risk_ports = [22, 23, 135, 139, 445, 1433, 3389, 5900]
        if dest_port in high_risk_ports:
            risk_score += 0.3

        # Factor 2: Horario sospechoso (madrugada)
        if hour >= 0 and hour <= 5:
            risk_score += 0.1

        # Factor 3: TamaÃ±o de paquete anÃ³malo
        if packet_size > 1400:
            risk_score += 0.2

        # Factor 4: Puertos dinÃ¡micos como destino
        if dest_port > 49152:
            risk_score += 0.15

        return min(risk_score, 1.0)


class LightweightMLDetector:
    """Detector ML ligero configurado completamente desde JSON (SIN GeoIP)"""

    def __init__(self, config_file=None):
        """Inicializar detector desde configuraciÃ³n JSON"""
        self.config = self._load_config(config_file)
        self.config_file = config_file

        # Configurar logging desde JSON PRIMERO
        self._setup_logging()

        # Configuraciones de red desde JSON (NUEVA ARQUITECTURA)
        self.input_port = self.config['zmq']['input_port']  # 5560 desde geoip_enricher
        self.output_port = self.config['zmq']['output_port']  # 5561 hacia dashboard
        self.context_threads = self.config['zmq']['context_threads']
        self.high_water_mark = self.config['zmq']['high_water_mark']

        # ConfiguraciÃ³n ML desde JSON
        self.ml_config = self.config.get('ml', {})
        self.ml_enabled = self.ml_config.get('enabled', True)
        self.anomaly_threshold = self.ml_config.get('anomaly_threshold', 0.8)
        self.high_risk_threshold = self.ml_config.get('high_risk_threshold', 0.9)
        self.buffer_size = self.ml_config.get('buffer_size', 10000)
        self.batch_size = self.ml_config.get('batch_size', 100)

        # ConfiguraciÃ³n de processing desde JSON
        self.processing_config = self.config.get('processing', {})
        self.enable_heuristics = self.processing_config.get('enable_heuristics', True)
        self.enable_alerts = self.processing_config.get('enable_alerts', True)
        self.stats_interval = self.processing_config.get('stats_interval', 45)
        self.max_processing_time = self.processing_config.get('max_processing_time', 5.0)

        self.running = False

        # ZeroMQ setup desde configuraciÃ³n
        self.context = zmq.Context(self.context_threads)
        self.input_socket = None
        self.output_socket = None

        # Componente ML (ÃšNICA responsabilidad)
        self.ml_model = SimpleMLModel(self.ml_config)

        # EstadÃ­sticas (SIN coordenadas, solo ML)
        self.stats = {
            'events_processed': 0,
            'events_enriched': 0,
            'anomalies_detected': 0,
            'high_risk_events': 0,
            'handshakes_processed': 0,
            'start_time': time.time(),
            'model_predictions': 0,
            'heuristic_predictions': 0,
            'processing_errors': 0
        }

        # Buffer para procesamiento configurado desde JSON
        self.event_buffer = deque(maxlen=self.buffer_size)

        # Persistencia configurada desde JSON
        self.persistence_config = self.config.get('persistence', {})
        if self.persistence_config.get('save_predictions', False):
            self.predictions_file = self.persistence_config.get('predictions_file', 'data/predictions.jsonl')
            self.auto_save_interval = self.persistence_config.get('auto_save_interval', 300)
            self._setup_persistence()

        logger.info("ğŸ¤– LightweightMLDetector inicializado (LIMPIO - Sin GeoIP)")
        logger.info("Config file: %s", config_file or 'default config')
        logger.info("ğŸ“¡ Input port: %d (desde geoip_enricher)", self.input_port)
        logger.info("ğŸ“¤ Output port: %d (hacia dashboard)", self.output_port)
        logger.info("ğŸ§  ML enabled: %s", self.ml_enabled)
        logger.info("ğŸ“¦ Protobuf disponible: %s", PROTOBUF_AVAILABLE)
        logger.info("ğŸ¯ Anomaly threshold: %.2f", self.anomaly_threshold)
        logger.info("ğŸ§¹ LIMPIO: Sin GeoIP - solo anÃ¡lisis ML")

    def _load_config(self, config_file):
        """Cargar configuraciÃ³n desde archivo JSON (SIN secciÃ³n GeoIP)"""
        default_config = {
            "agent_info": {
                "name": "lightweight_ml_detector",
                "version": "1.0.0",
                "description": "Detector ML ligero para anÃ¡lisis de trÃ¡fico (sin GeoIP)"
            },
            "zmq": {
                "input_port": 5560,
                "output_port": 5561,
                "context_threads": 1,
                "high_water_mark": 1000
            },
            "ml": {
                "enabled": True,
                "anomaly_threshold": 0.8,
                "high_risk_threshold": 0.9,
                "models": ["IsolationForest"],
                "buffer_size": 10000,
                "batch_size": 100,
                "training_interval": 3600,
                "training": {
                    "min_training_samples": 1000,
                    "retrain_interval_samples": 200,
                    "contamination_rate": 0.1
                },
                "features": [
                    "packet_size", "dest_port", "src_port",
                    "hour", "minute", "is_weekend",
                    "ip_entropy", "port_frequency"
                ]
            },
            "processing": {
                "enable_heuristics": True,
                "enable_alerts": True,
                "stats_interval": 45,
                "max_processing_time": 5.0
            },
            "logging": {
                "level": "INFO",
                "file": "logs/ml_detector.log",
                "max_size": "10MB",
                "backup_count": 5,
                "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
                "console_output": True
            },
            "protobuf": {
                "enabled": True,
                "timeout": 1000,
                "retry_attempts": 3
            },
            "persistence": {
                "save_predictions": False,
                "predictions_file": "data/predictions.jsonl",
                "auto_save_interval": 300
            }
        }

        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)

                # Merge recursivo de configuraciones
                self._merge_config(default_config, user_config)
                logger.info(f"ğŸ“„ ConfiguraciÃ³n ML cargada desde {config_file}")

            except Exception as e:
                logger.error(f"âŒ Error cargando configuraciÃ³n ML: {e}")
                logger.info("âš ï¸ Usando configuraciÃ³n por defecto")
        else:
            if config_file:
                logger.warning(f"âš ï¸ Archivo de configuraciÃ³n ML no encontrado: {config_file}")
            logger.info("âš ï¸ Usando configuraciÃ³n ML por defecto")

        return default_config

    def _merge_config(self, base, update):
        """Merge recursivo de configuraciones"""
        for key, value in update.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                self._merge_config(base[key], value)
            else:
                base[key] = value

    def _setup_logging(self):
        """Configurar logging desde configuraciÃ³n JSON"""
        log_config = self.config.get('logging', {})

        # Configurar nivel
        level = getattr(logging, log_config.get('level', 'INFO').upper())
        logger.setLevel(level)

        # Limpiar handlers existentes
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)

        # Formatter desde configuraciÃ³n
        formatter = logging.Formatter(
            log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        )

        # Console handler si estÃ¡ habilitado
        if log_config.get('console_output', True):
            console_handler = logging.StreamHandler()
            console_handler.setLevel(level)
            console_handler.setFormatter(formatter)
            logger.addHandler(console_handler)

        # File handler si se especifica archivo
        if log_config.get('file'):
            # Crear directorio si no existe
            log_file = log_config['file']
            log_dir = os.path.dirname(log_file)
            if log_dir and not os.path.exists(log_dir):
                os.makedirs(log_dir, exist_ok=True)

            from logging.handlers import RotatingFileHandler
            file_handler = RotatingFileHandler(
                log_file,
                maxBytes=self._parse_size(log_config.get('max_size', '10MB')),
                backupCount=log_config.get('backup_count', 5)
            )
            file_handler.setLevel(level)
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)

    def _parse_size(self, size_str: str) -> int:
        """Parse size string (e.g., '10MB') to bytes"""
        if isinstance(size_str, int):
            return size_str

        size_str = size_str.upper()
        if size_str.endswith('MB'):
            return int(size_str[:-2]) * 1024 * 1024
        elif size_str.endswith('KB'):
            return int(size_str[:-2]) * 1024
        else:
            return int(size_str)

    def _setup_persistence(self):
        """Configurar persistencia desde JSON"""
        # Crear directorio para predictions si no existe
        predictions_dir = os.path.dirname(self.predictions_file)
        if predictions_dir and not os.path.exists(predictions_dir):
            os.makedirs(predictions_dir, exist_ok=True)

        # Abrir archivo de predicciones
        self.predictions_file_handle = None
        try:
            self.predictions_file_handle = open(self.predictions_file, 'a')
        except Exception as e:
            logger.warning(f"âš ï¸ No se pudo abrir archivo de predicciones: {e}")

    def start(self):
        """Inicia el detector ML usando configuraciÃ³n JSON - ZMQ PULL/PUSH CORREGIDO"""
        try:
            # âœ… CORREGIDO: Configurar sockets con patrÃ³n PULL/PUSH para pipeline
            self.input_socket = self.context.socket(zmq.PULL)  # â† CAMBIO: SUB â†’ PULL
            input_addr = f"tcp://localhost:{self.input_port}"
            self.input_socket.connect(input_addr)
            # âŒ ELIMINADO: self.input_socket.setsockopt(zmq.SUBSCRIBE, b"")  # No necesario para PULL
            self.input_socket.setsockopt(zmq.RCVTIMEO, 3000)
            self.input_socket.setsockopt(zmq.RCVHWM, self.high_water_mark)

            self.output_socket = self.context.socket(zmq.PUSH)  # â† CAMBIO: PUB â†’ PUSH
            output_addr = f"tcp://*:{self.output_port}"
            self.output_socket.bind(output_addr)
            self.output_socket.setsockopt(zmq.SNDHWM, self.high_water_mark)

            self.running = True

            print(f"\nğŸ¤– Lightweight ML Detector Started (LIMPIO) - ZMQ PULL/PUSH + TIMEOUT CORREGIDO")
            print(f"ğŸ“„ Config: {self.config_file or 'default'}")
            print(f"ğŸ“¡ Input: {input_addr} (desde geoip_enricher) - PULL socket")
            print(f"ğŸ“¤ Output: {output_addr} (hacia dashboard) - PUSH socket")
            print(f"ğŸ“¦ Protobuf: {'âœ… Available' if PROTOBUF_AVAILABLE else 'âŒ Not available'}")
            print(f"ğŸ§  ML: {'âœ… Available' if ML_AVAILABLE else 'âŒ Heuristics only'}")
            print(f"ğŸ§¹ GeoIP: âŒ ELIMINADO (responsabilidad del geoip_enricher)")
            print(f"ğŸ¯ Anomaly threshold: {self.anomaly_threshold}")
            print(f"âš ï¸ High risk threshold: {self.high_risk_threshold}")
            print(f"ğŸ“Š Buffer size: {self.buffer_size}")
            print(f"âš¡ Batch size: {self.batch_size}")
            print(f"ğŸ”” Alerts: {'âœ… Enabled' if self.enable_alerts else 'âŒ Disabled'}")
            print(f"ğŸ”§ ZMQ Pattern: PULL/PUSH (pipeline corregido)")
            print(f"â±ï¸ Recv Timeout: 3000ms (bug NOBLOCK corregido)")
            print(f"ğŸ” Debug ML Values: âœ… Enabled (logs NaN/inf detection)")
            print(f"ğŸ§¹ Value Sanitization: âœ… Enabled (clamp 0-1, clean NaN/inf)")
            print("=" * 70)

            # Thread principal de procesamiento
            processing_thread = threading.Thread(target=self._processing_loop, daemon=True)
            processing_thread.start()

            # Thread de estadÃ­sticas
            stats_thread = threading.Thread(target=self._stats_loop, args=(self.stats_interval,), daemon=True)
            stats_thread.start()

            # Thread de auto-guardado si estÃ¡ habilitado
            if self.persistence_config.get('save_predictions', False):
                save_thread = threading.Thread(target=self._auto_save_loop, daemon=True)
                save_thread.start()

            # Mantener vivo
            try:
                while self.running:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\nğŸ›‘ Stopping ML detector...")
                self.running = False

        except Exception as e:
            logger.error("Error starting ML detector: %s", e)
            raise
        finally:
            self.cleanup()

    def _processing_loop(self):
        """Loop principal de procesamiento (SOLO ML) - TIMEOUT CORREGIDO"""
        logger.info("ğŸ”„ Iniciando loop de procesamiento ML...")

        while self.running:
            try:
                # âœ… CORRECCIÃ“N: Usar timeout en lugar de NOBLOCK
                message = self.input_socket.recv()  # â† QUITADO zmq.NOBLOCK
                self.stats['events_processed'] += 1

                # Procesar evento (SOLO ML)
                enriched_event = self._process_event(message)

                if enriched_event is not None:  # â† CORREGIDO: usar 'is not None' en lugar de verificaciÃ³n truthy
                    # Enviar evento enriquecido
                    self.output_socket.send(enriched_event)
                    self.stats['events_enriched'] += 1
                    logger.debug(f"âœ… Evento enviado al dashboard: {len(enriched_event)} bytes")
                else:
                    logger.warning(f"âš ï¸ Evento no procesado/enviado - _process_event devolviÃ³ None")

            except zmq.Again:
                continue  # Timeout - continuar
            except Exception as e:
                logger.error("Error en processing loop: %s", e)
                self.stats['processing_errors'] += 1
                time.sleep(0.1)

    def _process_event(self, message: bytes) -> Optional[bytes]:
        """Procesa un evento individual - SOLO anÃ¡lisis ML"""

        if not PROTOBUF_AVAILABLE:
            logger.warning("Protobuf no disponible - no se puede procesar evento")
            return None

        try:
            # Parsear evento protobuf entrante (YA CON COORDENADAS del geoip_enricher)
            event = network_event_extended_fixed_pb2.NetworkEvent()
            event.ParseFromString(message)

            # Convertir a diccionario para procesamiento ML
            event_dict = {
                'event_id': event.event_id,
                'source_ip': event.source_ip,
                'target_ip': event.target_ip,
                'packet_size': event.packet_size,
                'dest_port': event.dest_port,
                'src_port': event.src_port,
                'agent_id': event.agent_id,
                'event_type': event.event_type,
                'description': event.description,
                'so_identifier': event.so_identifier,
                'node_hostname': event.node_hostname,
                'os_version': event.os_version,
                'firewall_status': event.firewall_status,
                'agent_version': event.agent_version,
                'is_initial_handshake': event.is_initial_handshake
            }

            # Procesar handshake inicial (pasar sin ML)
            if event.is_initial_handshake:
                self.stats['handshakes_processed'] += 1
                logger.info(f"ğŸ¤ Procesando handshake inicial de {event.agent_id} ({event.so_identifier})")

                # Para handshakes, solo pasamos la informaciÃ³n completa sin ML
                enriched_event = network_event_extended_fixed_pb2.NetworkEvent()
                enriched_event.CopyFrom(event)  # Copiar TODO incluyendo coordenadas

                return enriched_event.SerializeToString()

            # Para eventos normales, aplicar SOLO ML
            if not self.ml_enabled:
                # Si ML estÃ¡ deshabilitado, pasar evento sin cambios
                enriched_event = network_event_extended_fixed_pb2.NetworkEvent()
                enriched_event.CopyFrom(event)
                return enriched_event.SerializeToString()

            # Extraer features para ML
            features = self.ml_model.extract_features(event_dict)

            # Entrenar/actualizar modelo
            self.ml_model.train_or_update(features)

            # Predecir anomalÃ­a y riesgo
            anomaly_score, risk_score = self.ml_model.predict_anomaly(features)

            # ğŸ” DEBUG: Verificar valores ML antes de asignar
            logger.debug(f"ğŸ” Valores ML RAW: anomaly={anomaly_score} (type={type(anomaly_score)})")
            logger.debug(f"ğŸ” Valores ML RAW: risk={risk_score} (type={type(risk_score)})")
            logger.debug(
                f"ğŸ” Validez anomaly: nan={math.isnan(anomaly_score) if isinstance(anomaly_score, (int, float)) else 'N/A'}, inf={math.isinf(anomaly_score) if isinstance(anomaly_score, (int, float)) else 'N/A'}")
            logger.debug(
                f"ğŸ” Validez risk: nan={math.isnan(risk_score) if isinstance(risk_score, (int, float)) else 'N/A'}, inf={math.isinf(risk_score) if isinstance(risk_score, (int, float)) else 'N/A'}")

            # Sanitizar valores ML
            clean_anomaly_score = self._sanitize_float(anomaly_score)
            clean_risk_score = self._sanitize_float(risk_score)

            logger.debug(f"ğŸ” Valores ML CLEAN: anomaly={clean_anomaly_score}, risk={clean_risk_score}")

            # Actualizar estadÃ­sticas segÃºn configuraciÃ³n
            if ML_AVAILABLE and self.ml_model.is_trained:
                self.stats['model_predictions'] += 1
            else:
                self.stats['heuristic_predictions'] += 1

            # EstadÃ­sticas segÃºn umbrales de configuraciÃ³n
            if clean_anomaly_score > self.anomaly_threshold:
                self.stats['anomalies_detected'] += 1

            if clean_risk_score > self.high_risk_threshold:
                self.stats['high_risk_events'] += 1

            # Crear evento enriquecido PRESERVANDO coordenadas del geoip_enricher
            enriched_event = network_event_extended_fixed_pb2.NetworkEvent()

            # Copiar TODOS los campos originales (incluyendo coordenadas)
            enriched_event.CopyFrom(event)

            logger.debug(f"ğŸ” Evento copiado: event_id={enriched_event.event_id}")

            # SOLO AÃ‘ADIR enriquecimiento ML con valores sanitizados
            enriched_event.anomaly_score = clean_anomaly_score
            enriched_event.risk_score = clean_risk_score

            logger.debug(
                f"ğŸ” Valores ML asignados al protobuf: A={enriched_event.anomaly_score}, R={enriched_event.risk_score}")

            # Enriquecer descripciÃ³n con info ML
            if clean_anomaly_score > 0.5 or clean_risk_score > 0.5:
                ml_info = f"ML: A:{clean_anomaly_score:.2f} R:{clean_risk_score:.2f}"
                if event.description:
                    enriched_event.description = f"{ml_info} | {event.description}"
                else:
                    enriched_event.description = ml_info

            # Guardar predicciÃ³n si estÃ¡ configurado
            if self.persistence_config.get('save_predictions', False):
                self._save_prediction(event_dict, clean_anomaly_score, clean_risk_score)

            # ğŸ” DEBUG: Verificar serializaciÃ³n
            try:
                serialized_data = enriched_event.SerializeToString()
                logger.debug(f"ğŸ” SerializaciÃ³n: {len(serialized_data)} bytes")
                if len(serialized_data) == 0:
                    logger.error(f"âŒ SERIALIZACIÃ“N VACÃA - evento: {enriched_event}")
                    return None
                else:
                    logger.debug(f"âœ… SerializaciÃ³n exitosa: {len(serialized_data)} bytes")
            except Exception as e:
                logger.error(f"âŒ Error en serializaciÃ³n: {e}")
                return None

            logger.debug("ğŸ“Š Evento ML procesado: %s A:%.2f R:%.2f",
                         event.event_id, clean_anomaly_score, clean_risk_score)

            return serialized_data

        except Exception as e:
            logger.error("Error procesando evento: %s", e)
            self.stats['processing_errors'] += 1
            return None

    def _save_prediction(self, event_dict: Dict, anomaly_score: float, risk_score: float):
        """Guarda predicciÃ³n en archivo si estÃ¡ configurado"""
        if not hasattr(self, 'predictions_file_handle') or not self.predictions_file_handle:
            return

        try:
            prediction_record = {
                'timestamp': time.time(),
                'event_id': event_dict.get('event_id'),
                'source_ip': event_dict.get('source_ip'),
                'anomaly_score': float(anomaly_score),  # Asegurar tipo float
                'risk_score': float(risk_score),  # Asegurar tipo float
                'features': {
                    'packet_size': event_dict.get('packet_size'),
                    'dest_port': event_dict.get('dest_port'),
                    'src_port': event_dict.get('src_port')
                }
            }

            self.predictions_file_handle.write(json.dumps(prediction_record) + '\n')
            self.predictions_file_handle.flush()

        except Exception as e:
            logger.error(f"Error guardando predicciÃ³n: {e}")

    def _stats_loop(self, interval: int):
        """Loop de estadÃ­sticas configurado desde JSON"""
        while self.running:
            try:
                time.sleep(interval)
                self._print_stats()
            except Exception as e:
                logger.error("Error en stats loop: %s", e)

    def _auto_save_loop(self):
        """Loop de auto-guardado configurado desde JSON"""
        save_interval = self.auto_save_interval

        while self.running:
            try:
                time.sleep(save_interval)
                self._save_state()
            except Exception as e:
                logger.error(f"Error en auto-save: {e}")

    def _save_state(self):
        """Guarda estado segÃºn configuraciÃ³n"""
        try:
            state_file = self.persistence_config.get('state_file', 'data/ml_detector_state.json')

            # Crear directorio si no existe
            state_dir = os.path.dirname(state_file)
            if state_dir and not os.path.exists(state_dir):
                os.makedirs(state_dir, exist_ok=True)

            state = {
                'stats': self.stats,
                'ml_model_trained': self.ml_model.is_trained,
                'training_samples': len(self.ml_model.training_data),
                'config_file': self.config_file,
                'last_saved': time.time()
            }

            with open(state_file, 'w') as f:
                json.dump(state, f, indent=2)

        except Exception as e:
            logger.error(f"âŒ Error guardando estado ML: {e}")

    def _print_stats(self):
        """Imprime estadÃ­sticas configuradas desde JSON"""
        uptime = time.time() - self.stats['start_time']

        print(f"\nğŸ“Š ML Detector Stats (LIMPIO) - Uptime: {uptime:.0f}s")
        print(f"ğŸ“¥ Events Processed: {self.stats['events_processed']}")
        print(f"ğŸ“¤ Events Enriched: {self.stats['events_enriched']}")
        print(f"ğŸš¨ Anomalies Detected: {self.stats['anomalies_detected']}")
        print(f"âš ï¸ High Risk Events: {self.stats['high_risk_events']}")
        print(f"ğŸ¤ Handshakes Processed: {self.stats['handshakes_processed']}")
        print(f"ğŸ¤– ML Model Trained: {self.ml_model.is_trained}")
        print(f"ğŸ“š Training Samples: {len(self.ml_model.training_data)}")
        print(f"ğŸ¯ ML Predictions: {self.stats['model_predictions']}")
        print(f"ğŸ”§ Heuristic Predictions: {self.stats['heuristic_predictions']}")
        print(f"âŒ Processing Errors: {self.stats['processing_errors']}")
        print(f"ğŸ“„ Config: {self.config_file or 'default'}")
        print(f"ğŸ§¹ GeoIP: âŒ ELIMINADO - solo ML")
        print(f"ğŸ”§ ZMQ Pattern: PULL/PUSH (corregido)")
        print(f"â±ï¸ Recv Timeout: 3000ms (bug NOBLOCK corregido)")
        print(f"ğŸ” Debug ML Values: âœ… (NaN/inf detection)")
        print("-" * 50)

    def get_statistics(self) -> Dict:
        """Retorna estadÃ­sticas completas"""
        uptime = time.time() - self.stats['start_time']

        return {
            'uptime_seconds': uptime,
            'events_processed': self.stats['events_processed'],
            'events_enriched': self.stats['events_enriched'],
            'anomalies_detected': self.stats['anomalies_detected'],
            'high_risk_events': self.stats['high_risk_events'],
            'handshakes_processed': self.stats['handshakes_processed'],
            'ml_model_trained': self.ml_model.is_trained,
            'training_samples': len(self.ml_model.training_data),
            'model_predictions': self.stats['model_predictions'],
            'heuristic_predictions': self.stats['heuristic_predictions'],
            'processing_errors': self.stats['processing_errors'],
            'protobuf_available': PROTOBUF_AVAILABLE,
            'ml_available': ML_AVAILABLE,
            'config_file': self.config_file,
            'configuration': {
                'input_port': self.input_port,
                'output_port': self.output_port,
                'anomaly_threshold': self.anomaly_threshold,
                'high_risk_threshold': self.high_risk_threshold,
                'batch_size': self.batch_size,
                'buffer_size': self.buffer_size,
                'ml_enabled': self.ml_enabled,
                'alerts_enabled': self.enable_alerts
            }
        }

    def cleanup(self):
        """Limpia recursos y guarda estado final"""
        # Guardar estado final
        if self.persistence_config.get('save_predictions', False):
            self._save_state()

        # Cerrar archivo de predicciones
        if hasattr(self, 'predictions_file_handle') and self.predictions_file_handle:
            self.predictions_file_handle.close()

        if self.input_socket:
            self.input_socket.close()
        if self.output_socket:
            self.output_socket.close()
        if self.context:
            self.context.term()

    def _sanitize_float(self, value) -> float:
        """Sanitiza valores float para protobuf (elimina NaN, inf, clamp a 0-1)"""
        try:
            if value is None:
                return 0.0

            # Convertir a float nativo de Python
            float_val = float(value)

            # Verificar NaN e infinity
            if math.isnan(float_val) or math.isinf(float_val):
                logger.warning(f"âš ï¸ Valor ML problemÃ¡tico detectado: {value} (type={type(value)})")
                return 0.0

            # Clamp a rango vÃ¡lido 0-1
            return max(0.0, min(1.0, float_val))

        except (TypeError, ValueError, OverflowError) as e:
            logger.error(f"âŒ Error convirtiendo valor ML: {value} -> {e}")
            return 0.0


def main():
    """FunciÃ³n principal con configuraciÃ³n JSON completa"""
    import argparse

    parser = argparse.ArgumentParser(description='Lightweight ML Detector (LIMPIO - Sin GeoIP)')
    parser.add_argument('config_file', nargs='?',
                        default='lightweight_ml_detector_config.json',
                        help='Archivo de configuraciÃ³n JSON')
    parser.add_argument('--test-config', action='store_true',
                        help='Validar configuraciÃ³n y salir')

    args = parser.parse_args()

    if args.test_config:
        try:
            detector = LightweightMLDetector(config_file=args.config_file)
            print("âœ… ConfiguraciÃ³n JSON vÃ¡lida (LIMPIO)")
            stats = detector.get_statistics()
            print(f"ğŸ“¡ Input port: {stats['configuration']['input_port']}")
            print(f"ğŸ“¤ Output port: {stats['configuration']['output_port']}")
            print(f"ğŸ¯ Anomaly threshold: {stats['configuration']['anomaly_threshold']}")
            print(f"âš ï¸ High risk threshold: {stats['configuration']['high_risk_threshold']}")
            print(f"ğŸ§¹ GeoIP: âŒ ELIMINADO - responsabilidad del geoip_enricher.py")
            print(f"ğŸ”§ ZMQ Pattern: PULL/PUSH (corregido)")
            print(f"â±ï¸ Recv Timeout: 3000ms (bug NOBLOCK corregido)")
            print(f"ğŸ” Debug ML Values: âœ… (NaN/inf detection)")
            return 0
        except Exception as e:
            print(f"âŒ Error en configuraciÃ³n: {e}")
            return 1

    if not PROTOBUF_AVAILABLE:
        print("âŒ Error: Protobuf no disponible")
        print("ğŸ“¦ Instalar con: pip install protobuf")
        return 1

    try:
        detector = LightweightMLDetector(config_file=args.config_file)

        print(f"\nğŸ¤– ML Detector iniciado (LIMPIO):")
        stats = detector.get_statistics()
        print(f"   ğŸ“¡ Input port: {stats['configuration']['input_port']}")
        print(f"   ğŸ“¤ Output port: {stats['configuration']['output_port']}")
        print(f"   ğŸ¯ Anomaly threshold: {stats['configuration']['anomaly_threshold']}")
        print(f"   ğŸ“Š Buffer size: {stats['configuration']['buffer_size']}")
        print(f"   ğŸ§  ML enabled: {'âœ…' if stats['configuration']['ml_enabled'] else 'âŒ'}")
        print(f"   ğŸ§¹ GeoIP: âŒ ELIMINADO")
        print(f"   ğŸ”§ ZMQ Pattern: PULL/PUSH (corregido)")
        print(f"   â±ï¸ Recv Timeout: 3000ms (bug NOBLOCK corregido)")
        print(f"   ğŸ” Debug ML Values: âœ… (NaN/inf detection)")

        detector.start()

    except KeyboardInterrupt:
        print("\nğŸ‘‹ Goodbye!")
    except Exception as e:
        logger.error("Error fatal: %s", e)
        return 1
    finally:
        if 'detector' in locals():
            detector._print_stats()

    return 0


if __name__ == "__main__":
    sys.exit(main())