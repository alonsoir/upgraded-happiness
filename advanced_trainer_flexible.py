#!/usr/bin/env python3
"""
ADVANCED TRAINER FLEXIBLE - VERSIÃ“N MEJORADA CON BALANCEO INTELIGENTE
Resuelve los problemas de:
- Balanceo en datasets ya balanceados
- Manejo de valores NaN
- Overfitting con parÃ¡metros mÃ¡s conservadores
"""

import pandas as pd
import numpy as np
import argparse
import json
import joblib
import os
from pathlib import Path
from datetime import datetime
from collections import Counter
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
import warnings

warnings.filterwarnings('ignore')


def load_and_validate_data(csv_path, max_rows=None):
    """Carga y valida el dataset"""
    print(f"ğŸ“ Cargando dataset: {csv_path}")

    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"No se encontrÃ³ el archivo: {csv_path}")

    df = pd.read_csv(csv_path, nrows=max_rows)
    print(f"ğŸ“Š Dataset cargado: {len(df):,} registros")

    # ValidaciÃ³n bÃ¡sica
    if 'label' not in df.columns:
        raise ValueError("No se encontrÃ³ la columna 'label' en el dataset")

    # DistribuciÃ³n de etiquetas
    label_dist = df['label'].value_counts().to_dict()
    print(f"ğŸ“Š DistribuciÃ³n de etiquetas: {label_dist}")

    return df


def apply_smart_balancing(X, y, balance_threshold=0.15):
    """
    Aplica balanceo inteligente solo cuando es necesario
    """
    print(f"âš–ï¸ Evaluando necesidad de balanceo...")

    # Manejar valores NaN primero
    nan_count = np.isnan(X).sum() if hasattr(X, 'sum') else np.isnan(X).sum()
    if nan_count > 0:
        print(f"ğŸ”§ Imputando {nan_count} valores NaN...")
        imputer = SimpleImputer(strategy='median')
        X = imputer.fit_transform(X)
        print(f"âœ… Valores NaN imputados")

    # Verificar distribuciÃ³n
    distribution = Counter(y)
    print(f"ğŸ“Š DistribuciÃ³n inicial: {dict(distribution)}")

    # Calcular ratio de balance
    values = list(distribution.values())
    min_count, max_count = min(values), max(values)
    balance_ratio = min_count / max_count

    print(f"ğŸ“ Ratio de balance: {balance_ratio:.3f}")

    # Si estÃ¡ balanceado, no hacer nada
    if balance_ratio >= (1 - balance_threshold):
        print(f"âœ… Dataset suficientemente balanceado. Sin balanceo adicional.")
        return X, y

    # Aplicar balanceo gradual
    print(f"ğŸ”„ Aplicando balanceo gradual...")

    try:
        # Estrategia conservadora: solo SMOTE si hay mucho desbalance
        if balance_ratio < 0.5:
            # Muy desbalanceado - usar SMOTE + undersampling
            target_ratio = 0.7  # No perfectamente balanceado

            smote = SMOTE(
                random_state=42,
                sampling_strategy=target_ratio,
                k_neighbors=min(5, min_count - 1)
            )

            X_balanced, y_balanced = smote.fit_resample(X, y)

        else:
            # Ligeramente desbalanceado - solo SMOTE suave
            smote = SMOTE(
                random_state=42,
                sampling_strategy=0.8,  # No completamente balanceado
                k_neighbors=min(3, min_count - 1)
            )
            X_balanced, y_balanced = smote.fit_resample(X, y)

        final_distribution = Counter(y_balanced)
        print(f"ğŸ“Š DistribuciÃ³n despuÃ©s de balanceo: {dict(final_distribution)}")
        return X_balanced, y_balanced

    except Exception as e:
        print(f"âš ï¸ Error en balanceo: {str(e)}")
        print(f"ğŸ”„ Continuando sin balanceo...")
        return X, y


def prepare_features_robust(df, target_column='label'):
    """PreparaciÃ³n robusta de features"""
    print(f"ğŸ”§ Preparando features...")

    # Separar target
    y = df[target_column].values
    X_df = df.drop(columns=[target_column])

    # Identificar tipos de columnas
    numeric_columns = X_df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_columns = X_df.select_dtypes(exclude=[np.number]).columns.tolist()

    print(f"ğŸ”¢ Features numÃ©ricas: {len(numeric_columns)}")
    if categorical_columns:
        print(f"ğŸ”¤ Codificando {len(categorical_columns)} columnas categÃ³ricas...")

    # Procesar features
    processed_X = X_df.copy()

    # Limpiar infinitos en numÃ©ricas
    if numeric_columns:
        for col in numeric_columns:
            processed_X[col] = processed_X[col].replace([np.inf, -np.inf], np.nan)

    # Codificar categÃ³ricas
    for col in categorical_columns:
        processed_X[col] = processed_X[col].fillna('unknown')
        le = LabelEncoder()
        processed_X[col] = le.fit_transform(processed_X[col].astype(str))

    # Convertir a numpy
    X = processed_X.values.astype(np.float32)

    print(f"âœ… Features preparadas: X={X.shape}, y={len(y)}")

    return X, y, processed_X.columns.tolist()


def train_conservative_model(X_train, y_train):
    """Entrena modelo con parÃ¡metros conservadores para evitar overfitting"""
    print(f"ğŸ§  Entrenando Random Forest conservador...")

    # ParÃ¡metros mÃ¡s conservadores
    rf = RandomForestClassifier(
        n_estimators=300,  # Reducido de 2000
        max_depth=20,  # Muy reducido de 120
        min_samples_split=20,  # Aumentado significativamente
        min_samples_leaf=10,  # Aumentado significativamente
        max_features='sqrt',  # MÃ¡s conservador que 'log2'
        class_weight='balanced',  # Manejo interno de balance
        random_state=42,
        n_jobs=-1,
        oob_score=True,
        bootstrap=True,
        max_samples=0.8  # Usar solo 80% de datos en cada Ã¡rbol
    )

    rf.fit(X_train, y_train)
    print(f"ğŸ¯ Modelo entrenado - OOB Score: {rf.oob_score_:.4f}")

    # ValidaciÃ³n cruzada
    cv_scores = cross_val_score(rf, X_train, y_train, cv=3, scoring='accuracy')
    print(f"ğŸ” CV Scores promedio: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

    # Alerta si hay overfitting potencial
    if rf.oob_score_ > 0.98:
        print(f"âš ï¸ ALERTA: OOB Score muy alto ({rf.oob_score_:.4f}) - Posible overfitting!")
        print(f"   Considera usar parÃ¡metros aÃºn mÃ¡s conservadores")

    return rf


def comprehensive_evaluation(model, X_test, y_test, feature_names=None):
    """EvaluaciÃ³n comprehensiva del modelo"""
    print(f"ğŸ“Š Evaluando modelo...")

    # Predicciones
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

    # MÃ©tricas bÃ¡sicas
    print(f"\n[ğŸ“Š] Matriz de ConfusiÃ³n:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)

    print(f"\n[ğŸ“‹] Reporte de ClasificaciÃ³n:")
    print(classification_report(y_test, y_pred))

    # AUC si hay probabilidades
    if y_prob is not None:
        auc = roc_auc_score(y_test, y_prob)
        print(f"\n[ğŸ“ˆ] AUC-ROC: {auc:.4f}")

        # Alerta de overfitting
        if auc > 0.99:
            print(f"âš ï¸ ALERTA: AUC extremadamente alto ({auc:.4f}) - Probable overfitting!")

    # Feature importance
    if hasattr(model, 'feature_importances_') and feature_names:
        importances = model.feature_importances_
        feature_importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)

        print(f"\n[ğŸ”] Top 10 Features mÃ¡s importantes:")
        for _, row in feature_importance_df.head(10).iterrows():
            print(f"   {row['feature']}: {row['importance']:.4f}")

    return {
        'confusion_matrix': cm,
        'auc_roc': auc if y_prob is not None else None,
        'predictions': y_pred,
        'probabilities': y_prob
    }


def save_model_artifacts(model, scaler, feature_names, metrics, output_path):
    """Guarda todos los artefactos del modelo"""
    print(f"ğŸ’¾ Guardando artefactos del modelo...")

    # Crear directorio si no existe
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    # Guardar modelo
    joblib.dump(model, output_path)
    print(f"   âœ… Modelo: {output_path}")

    # Guardar scaler
    scaler_path = str(output_path).replace('.joblib', '_scaler.joblib')
    joblib.dump(scaler, scaler_path)
    print(f"   âœ… Scaler: {scaler_path}")

    # Guardar metadatos
    metadata = {
        'timestamp': datetime.now().isoformat(),
        'feature_names': feature_names,
        'model_params': model.get_params(),
        'oob_score': getattr(model, 'oob_score_', None),
        'metrics': {
            'auc_roc': metrics.get('auc_roc'),
            'confusion_matrix': metrics['confusion_matrix'].tolist()
        }
    }

    metadata_path = str(output_path).replace('.joblib', '_metadata.json')
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"   âœ… Metadata: {metadata_path}")


def main():
    parser = argparse.ArgumentParser(description='Advanced Trainer - VersiÃ³n Mejorada')
    parser.add_argument('--input_csv', required=True, help='Archivo CSV de entrada')
    parser.add_argument('--output_model', required=True, help='Archivo modelo de salida')
    parser.add_argument('--config_file', help='Archivo de configuraciÃ³n JSON')
    parser.add_argument('--max_rows', type=int, default=100000, help='MÃ¡ximo nÃºmero de filas')
    parser.add_argument('--balance_threshold', type=float, default=0.15,
                        help='Umbral para considerar dataset balanceado')

    args = parser.parse_args()

    print("ğŸš€ ADVANCED TRAINER MEJORADO v2.0")
    print("=" * 60)
    print(f"ğŸ• Iniciado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    try:
        # 1. Cargar datos
        df = load_and_validate_data(args.input_csv, args.max_rows)

        # 2. Preparar features
        X, y, feature_names = prepare_features_robust(df)

        # 3. Balanceo inteligente
        X_balanced, y_balanced = apply_smart_balancing(X, y, args.balance_threshold)

        # 4. DivisiÃ³n train/test
        print(f"âœ‚ï¸ Dividiendo datos...")
        X_train, X_test, y_train, y_test = train_test_split(
            X_balanced, y_balanced,
            test_size=0.25,
            random_state=42,
            stratify=y_balanced
        )
        print(f"ğŸ“Š DivisiÃ³n: {len(X_train):,} entrenamiento, {len(X_test):,} prueba")

        # 5. Escalado
        print(f"ğŸ“ Escalando features...")
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # 6. Entrenamiento
        model = train_conservative_model(X_train_scaled, y_train)

        # 7. EvaluaciÃ³n
        metrics = comprehensive_evaluation(model, X_test_scaled, y_test, feature_names)

        # 8. Guardar artefactos
        save_model_artifacts(model, scaler, feature_names, metrics, args.output_model)

        print(f"\nğŸ¯ RESUMEN FINAL")
        print("=" * 60)
        print(f"ğŸ“Š OOB Score: {getattr(model, 'oob_score_', 'N/A'):.4f}")
        print(f"ğŸ“ˆ AUC-ROC: {metrics.get('auc_roc', 'N/A'):.4f}")
        print(f"ğŸ’¾ Modelo guardado: {args.output_model}")
        print(f"âœ… ENTRENAMIENTO COMPLETADO!")

    except Exception as e:
        print(f"âŒ ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())