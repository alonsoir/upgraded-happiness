#!/usr/bin/env python3
"""
Sistema ML Ligero para Intel i9 + 32GB RAM
Optimizado para CPU, sin dependencias GPU
"""

import zmq
import sys
import os
import time
import json
import pandas as pd
import numpy as np
from datetime import datetime
from collections import defaultdict, deque
import pickle
import threading
import joblib

# ML optimizado para CPU
from sklearn.ensemble import (
    IsolationForest,
    RandomForestClassifier,
    GradientBoostingClassifier
)
from sklearn.cluster import DBSCAN, KMeans
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif

# XGBoost es muy eficiente en CPU
import xgboost as xgb

# Para an√°lisis temporal b√°sico sin GPU
from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import GaussianNB

sys.path.insert(0, os.getcwd())

try:
    from src.protocols.protobuf import network_event_pb2

    print("‚úÖ Protobuf importado exitosamente")
except ImportError as e:
    print(f"‚ùå Error importando protobuf: {e}")
    sys.exit(1)


class LightweightThreatDetector:
    def __init__(self, broker_address="tcp://localhost:5555"):
        self.broker_address = broker_address
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.SUB)
        self.socket.setsockopt(zmq.SUBSCRIBE, b"")

        # Optimizaci√≥n para CPU Intel i9
        self.cpu_config = {
            "n_jobs": 8,  # Usar 8 cores del i9
            "batch_size": 1000,  # Procesar en lotes
            "memory_limit": "24GB",  # Dejar 8GB para el SO
            "model_cache_size": 50,  # Cachear 50 modelos
            "feature_limit": 50,  # M√°ximo 50 features para eficiencia
        }

        # Modelos ligeros optimizados para CPU
        self.models = {
            "isolation_forest": None,
            "random_forest": None,
            "xgboost": None,
            "gradient_boost": None,
            "kmeans": None,
            "dbscan": None,
            "sgd_classifier": None,
            "naive_bayes": None
        }

        # Pipelines de procesamiento r√°pido
        self.processors = {
            "scaler": RobustScaler(),  # M√°s robusto que StandardScaler
            "pca": PCA(n_components=20),  # Reducir dimensionalidad
            "feature_selector": SelectKBest(score_func=f_classif, k=30)
        }

        # Cache para features frecuentes
        self.feature_cache = {}
        self.pattern_cache = deque(maxlen=10000)  # √öltimos 10k patrones

        # Ventana deslizante para entrenamiento incremental
        self.sliding_window = deque(maxlen=5000)  # 5k samples para reentrenamiento

        # Estad√≠sticas de rendimiento
        self.performance_stats = {
            "processing_time_ms": deque(maxlen=1000),
            "prediction_time_ms": deque(maxlen=1000),
            "memory_usage_mb": deque(maxlen=100),
            "cpu_utilization": deque(maxlen=100)
        }

        print(f"üß† DETECTOR ML LIGERO (Optimizado para Intel i9)")
        print(f"üíæ RAM disponible: 32GB (usando hasta 24GB)")
        print(f"üîß CPU cores: {self.cpu_config['n_jobs']}")
        print(f"üìä Batch size: {self.cpu_config['batch_size']}")
        print("=" * 60)

    def connect(self):
        """Conectar al broker ZeroMQ"""
        try:
            self.socket.connect(self.broker_address)
            print("‚úÖ Conectado al broker ZeroMQ")
            return True
        except Exception as e:
            print(f"‚ùå Error conectando: {e}")
            return False

    def extract_lightweight_features(self, event):
        """Extraer features optimizadas para procesamiento r√°pido"""
        # Cache check para IPs frecuentes
        cache_key = f"{event.source_ip}:{event.target_ip}:{event.dest_port}"
        if cache_key in self.feature_cache:
            cached_features = self.feature_cache[cache_key].copy()
            cached_features["timestamp"] = time.time()
            return cached_features

        # Features b√°sicas optimizadas
        features = {
            # Network features (m√°s importantes)
            "src_port": event.src_port,
            "dst_port": event.dest_port,
            "packet_size": event.packet_size,
            "port_diff": abs(event.dest_port - event.src_port) if event.src_port > 0 else 0,

            # IP features simplificadas
            "is_internal_src": 1 if self._is_internal_ip(event.source_ip) else 0,
            "is_internal_dst": 1 if self._is_internal_ip(event.target_ip) else 0,
            "ip_class": self._classify_ip_range(event.target_ip),

            # Port classification (m√°s eficiente que muchos booleanos)
            "port_category": self._categorize_port(event.dest_port),
            "src_port_category": self._categorize_port(event.src_port),

            # Temporal features b√°sicas
            "hour": datetime.now().hour,
            "is_business_hours": 1 if 9 <= datetime.now().hour <= 17 else 0,
            "is_weekend": 1 if datetime.now().weekday() >= 5 else 0,

            # Size features
            "size_category": min(4, event.packet_size // 256),  # 0-4 categories
            "is_large_packet": 1 if event.packet_size > 1500 else 0,

            # Protocol inference (r√°pido)
            "likely_protocol": self._infer_protocol(event.dest_port),

            # Metadata b√°sico
            "timestamp": time.time()
        }

        # Cache para IPs frecuentes (optimizaci√≥n)
        if len(self.feature_cache) < 1000:  # Limitar cache
            self.feature_cache[cache_key] = features.copy()

        return features

    def _is_internal_ip(self, ip):
        """Verificaci√≥n r√°pida de IP interna"""
        if not ip or ip == "unknown":
            return 0
        return 1 if ip.startswith(("192.168.", "10.", "172.")) else 0

    def _classify_ip_range(self, ip):
        """Clasificar rango de IP (0-5)"""
        if not ip or ip == "unknown":
            return 0

        if ip.startswith("192.168."):
            return 1  # LAN
        elif ip.startswith("10."):
            return 2  # Private Class A
        elif ip.startswith("172."):
            return 3  # Private Class B
        elif ip.startswith(("8.8.", "1.1.", "208.67.")):
            return 4  # Public DNS
        else:
            return 5  # Other public

    def _categorize_port(self, port):
        """Categorizar puerto (0-6) para eficiencia"""
        if port == 0:
            return 0
        elif port < 1024:
            return 1  # Privileged
        elif port in [80, 443, 22, 25, 53, 21, 23]:
            return 2  # Common services
        elif 1024 <= port <= 5000:
            return 3  # Registered
        elif 5001 <= port <= 32767:
            return 4  # Registered high
        elif 32768 <= port <= 49151:
            return 5  # Dynamic/ephemeral
        else:
            return 6  # Private/dynamic high

    def _infer_protocol(self, port):
        """Inferir protocolo por puerto (0-10)"""
        protocol_map = {
            80: 1, 8080: 1,  # HTTP
            443: 2, 8443: 2,  # HTTPS
            22: 3,  # SSH
            25: 4, 587: 4, 465: 4,  # SMTP
            53: 5,  # DNS
            21: 6,  # FTP
            23: 7,  # Telnet
            110: 8, 995: 8,  # POP3
            143: 9, 993: 9,  # IMAP
        }
        return protocol_map.get(port, 0)  # Unknown = 0

    def train_lightweight_models(self, X, y=None):
        """Entrenar modelos optimizados para CPU"""
        print(f"üîß Entrenando modelos con {len(X)} muestras...")

        start_time = time.time()

        # Preprocesamiento r√°pido
        X_processed = self.processors["scaler"].fit_transform(X)

        # Reducir dimensionalidad si es necesario
        if X_processed.shape[1] > 20:
            X_processed = self.processors["pca"].fit_transform(X_processed)

        # 1. Isolation Forest (muy r√°pido para anomal√≠as)
        print("üå≤ Entrenando Isolation Forest...")
        self.models["isolation_forest"] = IsolationForest(
            contamination=0.1,
            n_estimators=50,  # Reducido para velocidad
            n_jobs=self.cpu_config["n_jobs"],
            random_state=42
        )
        self.models["isolation_forest"].fit(X_processed)

        # 2. Random Forest (eficiente y preciso)
        if y is not None and len(np.unique(y)) > 1:
            print("üå≥ Entrenando Random Forest...")
            self.models["random_forest"] = RandomForestClassifier(
                n_estimators=50,  # Optimizado para velocidad
                max_depth=10,
                n_jobs=self.cpu_config["n_jobs"],
                random_state=42
            )
            self.models["random_forest"].fit(X_processed, y)

        # 3. XGBoost (excelente en CPU)
        if y is not None and len(np.unique(y)) > 1:
            print("üöÄ Entrenando XGBoost...")
            self.models["xgboost"] = xgb.XGBClassifier(
                n_estimators=50,
                max_depth=6,
                learning_rate=0.1,
                n_jobs=self.cpu_config["n_jobs"],
                random_state=42,
                eval_metric='logloss'
            )
            self.models["xgboost"].fit(X_processed, y)

        # 4. SGD Classifier (muy r√°pido, ideal para streaming)
        if y is not None:
            print("‚ö° Entrenando SGD Classifier...")
            self.models["sgd_classifier"] = SGDClassifier(
                loss='hinge',
                alpha=0.01,
                random_state=42,
                n_jobs=self.cpu_config["n_jobs"]
            )
            self.models["sgd_classifier"].fit(X_processed, y)

        # 5. KMeans (clustering r√°pido)
        print("üéØ Entrenando KMeans...")
        self.models["kmeans"] = KMeans(
            n_clusters=5,
            n_init=5,  # Reducido para velocidad
            random_state=42
        )
        self.models["kmeans"].fit(X_processed)

        # 6. Naive Bayes (ultrarr√°pido)
        if y is not None:
            print("üßÆ Entrenando Naive Bayes...")
            self.models["naive_bayes"] = GaussianNB()
            self.models["naive_bayes"].fit(X_processed, y)

        training_time = time.time() - start_time
        print(f"‚úÖ Entrenamiento completado en {training_time:.2f} segundos")

        return X_processed

    def predict_threat(self, features):
        """Predicci√≥n r√°pida de amenazas"""
        start_time = time.time()

        # Convertir a array
        feature_array = np.array(list(features.values())[:-1]).reshape(1, -1)  # Excluir timestamp

        # Preprocesar
        try:
            feature_array = self.processors["scaler"].transform(feature_array)
            if hasattr(self.processors["pca"], "components_"):
                feature_array = self.processors["pca"].transform(feature_array)
        except:
            # Si no est√°n entrenados los processors, usar array original
            pass

        threats = []

        # Isolation Forest (detecci√≥n de anomal√≠as)
        if self.models["isolation_forest"] is not None:
            try:
                anomaly_score = self.models["isolation_forest"].decision_function(feature_array)[0]
                is_anomaly = self.models["isolation_forest"].predict(feature_array)[0] == -1

                if is_anomaly:
                    threats.append({
                        "type": "anomaly",
                        "model": "isolation_forest",
                        "score": float(anomaly_score),
                        "severity": "medium" if anomaly_score < -0.5 else "low"
                    })
            except:
                pass

        # Random Forest
        if self.models["random_forest"] is not None:
            try:
                proba = self.models["random_forest"].predict_proba(feature_array)[0]
                max_proba = max(proba)

                if max_proba > 0.7:  # Umbral de confianza
                    threats.append({
                        "type": "classification",
                        "model": "random_forest",
                        "probability": float(max_proba),
                        "severity": "high" if max_proba > 0.9 else "medium"
                    })
            except:
                pass

        # XGBoost
        if self.models["xgboost"] is not None:
            try:
                proba = self.models["xgboost"].predict_proba(feature_array)[0]
                max_proba = max(proba)

                if max_proba > 0.8:
                    threats.append({
                        "type": "ml_classification",
                        "model": "xgboost",
                        "probability": float(max_proba),
                        "severity": "high"
                    })
            except:
                pass

        # KMeans (clustering)
        if self.models["kmeans"] is not None:
            try:
                cluster = self.models["kmeans"].predict(feature_array)[0]
                distance = self.models["kmeans"].transform(feature_array)[0][cluster]

                if distance > 2.0:  # Lejos del centro del cluster
                    threats.append({
                        "type": "outlier",
                        "model": "kmeans",
                        "distance": float(distance),
                        "cluster": int(cluster),
                        "severity": "low"
                    })
            except:
                pass

        prediction_time = time.time() - start_time
        self.performance_stats["prediction_time_ms"].append(prediction_time * 1000)

        return threats

    def process_event_batch(self, events):
        """Procesar eventos en lotes para eficiencia"""
        if not events:
            return

        features_batch = []
        for event in events:
            features = self.extract_lightweight_features(event)
            features_batch.append(features)
            self.sliding_window.append(features)

        # Procesar amenazas en lote
        for i, features in enumerate(features_batch):
            threats = self.predict_threat(features)
            if threats:
                self.handle_threat_detection(events[i], threats)

    def handle_threat_detection(self, event, threats):
        """Manejar detecci√≥n de amenazas"""
        for threat in threats:
            print(
                f"üö® AMENAZA: {threat['type']} ({threat['model']}) - {event.source_ip}:{event.src_port} ‚Üí {event.target_ip}:{event.dest_port}")

    def incremental_training(self):
        """Entrenamiento incremental con ventana deslizante"""
        if len(self.sliding_window) < 1000:
            return

        print(f"üîÑ Reentrenamiento incremental con {len(self.sliding_window)} muestras...")

        # Convertir a DataFrame
        df = pd.DataFrame(list(self.sliding_window))

        # Preparar datos
        X = df.drop(['timestamp'], axis=1).values

        # Generar etiquetas simples (para demo)
        y = self._generate_simple_labels(df)

        # Reentrenar modelos r√°pidos
        self.train_lightweight_models(X, y)

    def _generate_simple_labels(self, df):
        """Generar etiquetas simples para entrenamiento"""
        labels = []
        for _, row in df.iterrows():
            # Heur√≠stica simple para etiquetado
            if (row.get('port_category', 0) == 6 or  # High ports
                    row.get('packet_size', 0) > 8000 or  # Large packets
                    (row.get('hour', 12) < 6 or row.get('hour', 12) > 22)):  # Unusual hours
                labels.append(1)  # Suspicious
            else:
                labels.append(0)  # Normal

        return np.array(labels)

    def start_monitoring(self):
        """Iniciar monitoreo optimizado"""
        print("üöÄ Iniciando monitoreo ML ligero...")

        event_batch = []
        last_batch_process = time.time()

        try:
            while True:
                try:
                    # Recibir evento
                    message = self.socket.recv(zmq.NOBLOCK)
                    event = network_event_pb2.NetworkEvent()
                    event.ParseFromString(message)

                    event_batch.append(event)

                    # Procesar en lotes para eficiencia
                    if (len(event_batch) >= self.cpu_config["batch_size"] or
                            time.time() - last_batch_process > 5):  # M√°ximo 5 segundos

                        self.process_event_batch(event_batch)
                        event_batch = []
                        last_batch_process = time.time()

                        # Reentrenamiento peri√≥dico
                        if len(self.sliding_window) >= 2000:
                            self.incremental_training()

                except zmq.Again:
                    time.sleep(0.1)

                    # Procesar lote parcial si hay timeout
                    if event_batch and time.time() - last_batch_process > 10:
                        self.process_event_batch(event_batch)
                        event_batch = []
                        last_batch_process = time.time()

                    continue

        except KeyboardInterrupt:
            print(f"\nüõë Monitoreo detenido")
            if event_batch:  # Procesar eventos restantes
                self.process_event_batch(event_batch)


def main():
    """Funci√≥n principal optimizada"""
    detector = LightweightThreatDetector()

    if detector.connect():
        # Entrenamiento inicial con datos sint√©ticos
        print("üìö Generando datos de entrenamiento inicial...")
        X_initial = np.random.rand(1000, 15)  # 1000 muestras, 15 features
        y_initial = np.random.choice([0, 1], 1000)  # Etiquetas aleatorias

        detector.train_lightweight_models(X_initial, y_initial)

        try:
            detector.start_monitoring()
        finally:
            detector.socket.close()
            detector.context.term()


if __name__ == "__main__":
    main()