#!/usr/bin/env python3
"""
geoip_enricher_v3_ipapi_READONLY_BASIC.py - Enriquecedor GeoIP v3.0.0 + IPAPI - READONLY CAMPOS B√ÅSICOS
üåç Enhanced GeoIP Enricher para Upgraded-Happiness (VERTICAL SCALING v3.0.0 + IPAPI INTEGRATION)
üö® BUG FIX CR√çTICO: source_ip ‚Üí target_ip para geoposicionar atacantes
üåê NUEVO: Discovery autom√°tico de IP p√∫blica
üéØ NUEVO: Enriquecimiento dual (source_ip + target_ip)
üì¶ ACTUALIZADO: Protobuf v3.0.0 con CAMPOS MODERNOS √∫nicamente
üìù MEJORADO: Logging dual (consola + archivo)
üîß MODIFICADO: Lookup real MaxMind SIN hardcodeos
üåê A√ëADIDO: Soporte completo para IPAPI como proveedor de geolocalizaci√≥n
üö´ ELIMINADO: Uso de campos LEGACY deprecados
‚úÖ USADO: √öNICAMENTE campos v3.0.0 modernos (source_latitude, target_latitude, etc.)
üìñ READONLY: Campos b√°sicos del evento (1-10) - NO los modifica, solo los lee
‚úçÔ∏è WRITEONLY: Campos de geolocalizaci√≥n (54+) - √öNICAMENTE estos se modifican
"""

import zmq
import json
import time
import logging
import threading
import sys
import os
import socket
import psutil
import math
import urllib.request
import urllib.error
import ipaddress
from queue import Queue, Empty
from datetime import datetime
from pathlib import Path
from collections import deque, defaultdict
from typing import Dict, Any, Optional, Tuple, List
from threading import Event
# üì¶ Protobuf v3.0.0 - REQUERIDO - Importaci√≥n robusta
PROTOBUF_AVAILABLE = False
PROTOBUF_VERSION = "unavailable"
NetworkEventProto = None


# üîß Rutas de importaci√≥n robustas para protobuf
def import_protobuf_module():
    """Importa el m√≥dulo protobuf con m√∫ltiples estrategias"""
    global NetworkEventProto, PROTOBUF_AVAILABLE, PROTOBUF_VERSION

    # Estrategia 1: Importaci√≥n relativa desde protocols.current
    import_strategies = [
        ("protocols.current.network_event_extended_v3_pb2", "Paquete protocols.current"),
        ("protocols.network_event_extended_v3_pb2", "Paquete protocols"),
        ("network_event_extended_v3_pb2", "Importaci√≥n directa"),
    ]

    for import_path, description in import_strategies:
        try:
            NetworkEventProto = __import__(import_path, fromlist=[''])
            PROTOBUF_AVAILABLE = True
            PROTOBUF_VERSION = "v3.0.0"
            print(f"‚úÖ Protobuf v3 cargado: {description} ({import_path})")
            return True
        except ImportError:
            continue

    # Estrategia 2: A√±adir path din√°mico y importar
    current_dir = os.path.dirname(os.path.abspath(__file__))
    possible_paths = [
        os.path.join(current_dir, '..', 'protocols', 'current'),
        os.path.join(current_dir, 'protocols', 'current'),
        os.path.join(os.getcwd(), 'protocols', 'current'),
    ]

    for protocols_path in possible_paths:
        protocols_path = os.path.abspath(protocols_path)
        pb2_file = os.path.join(protocols_path, 'network_event_extended_v3_pb2.py')

        if os.path.exists(pb2_file):
            try:
                sys.path.insert(0, protocols_path)
                import network_event_extended_v3_pb2 as NetworkEventProto
                PROTOBUF_AVAILABLE = True
                PROTOBUF_VERSION = "v3.0.0"
                print(f"‚úÖ Protobuf v3 cargado desde path: {protocols_path}")
                return True
            except ImportError as e:
                sys.path.remove(protocols_path)
                continue

    return False


# Ejecutar importaci√≥n al inicio
import_protobuf_module()

# üåç MaxMind GeoIP2 para lookup real
try:
    import geoip2.database
    import geoip2.errors

    MAXMIND_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è MaxMind geoip2 no disponible - install: pip install geoip2")
    MAXMIND_AVAILABLE = False

# üì¶ Cache LRU para optimizaci√≥n
try:
    from functools import lru_cache

    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False


class PublicIPDiscovery:
    """Descubrimiento y cache de IP p√∫blica - v3.0.0"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("ip_handling", {}).get("public_ip_discovery", {})
        self.enabled = self.config.get("enabled", False)
        self.services = self.config.get("services", ["https://api.ipify.org"])
        self.timeout = self.config.get("timeout", 10)
        self.cache_duration = self.config.get("cache_duration", 3600)

        # Cache
        self._cached_public_ip = None
        self._cache_timestamp = 0

        self.logger = logging.getLogger("PublicIPDiscovery")

    def get_public_ip(self) -> Optional[str]:
        """Obtiene IP p√∫blica con cache"""
        if not self.enabled:
            return None

        # üóÑÔ∏è Verificar cache
        now = time.time()
        if (self._cached_public_ip and
                (now - self._cache_timestamp) < self.cache_duration):
            self.logger.debug(f"üóÑÔ∏è IP p√∫blica desde cache: {self._cached_public_ip}")
            return self._cached_public_ip

        # üåê Obtener IP p√∫blica de servicios
        for service in self.services:
            try:
                ip = self._fetch_public_ip(service)
                if ip:
                    self._cached_public_ip = ip
                    self._cache_timestamp = now
                    self.logger.info(f"‚úÖ IP p√∫blica obtenida de {service}: {ip}")
                    return ip
            except Exception as e:
                self.logger.warning(f"‚ùå Error obteniendo IP de {service}: {e}")
                continue

        self.logger.error("‚ùå No se pudo obtener IP p√∫blica de ning√∫n servicio")
        return None

    def _fetch_public_ip(self, service_url: str) -> Optional[str]:
        """Obtiene IP p√∫blica de un servicio espec√≠fico"""
        try:
            request = urllib.request.Request(service_url)
            with urllib.request.urlopen(request, timeout=self.timeout) as response:
                ip = response.read().decode("utf-8").strip()
                # Validar que es una IP v√°lida
                ipaddress.ip_address(ip)
                return ip
        except (urllib.error.URLError, urllib.error.HTTPError,
                ValueError, OSError) as e:
            raise Exception(f"Error fetching from {service_url}: {e}")


class IPAddressHandler:
    """Manejo avanzado de direcciones IP - v3.0.0"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config.get("ip_handling", {})
        self.private_ranges = []

        # Configurar rangos de IPs privadas
        for range_str in self.config.get("private_ip_ranges", []):
            try:
                self.private_ranges.append(ipaddress.ip_network(range_str))
            except ValueError as e:
                logging.error(f"‚ùå Error parseando rango IP privada {range_str}: {e}")

        # Discovery de IP p√∫blica
        self.public_ip_discovery = PublicIPDiscovery(config)
        self.logger = logging.getLogger("IPAddressHandler")

    def is_private_ip(self, ip_str: str) -> bool:
        """Verifica si una IP es privada"""
        try:
            ip = ipaddress.ip_address(ip_str)
            return any(ip in network for network in self.private_ranges)
        except ValueError:
            return False

    def resolve_source_ip_for_lookup(self, source_ip: str) -> Optional[str]:
        """Resuelve qu√© IP usar para lookup de source_ip (nuestra IP)"""
        if self.is_private_ip(source_ip):
            # Si es IP privada, obtener IP p√∫blica
            public_ip = self.public_ip_discovery.get_public_ip()
            if public_ip:
                self.logger.debug(f"üåê Resolviendo IP p√∫blica {public_ip} para source_ip privada {source_ip}")
                return public_ip
            else:
                # No se pudo obtener IP p√∫blica
                self.logger.warning(f"‚ö†Ô∏è No se pudo obtener IP p√∫blica para source_ip privada {source_ip}")
                return None
        else:
            # IP p√∫blica directa
            return source_ip

    def resolve_target_ip_for_lookup(self, target_ip: str) -> Optional[str]:
        """Resuelve qu√© IP usar para lookup de target_ip (atacante)"""
        if self.is_private_ip(target_ip):
            self.logger.warning(f"‚ö†Ô∏è target_ip es privada: {target_ip} - posible error en captura")
            return None
        return target_ip


class VerticalScalingManager:
    """Gestor de escalado vertical con m√©tricas de hardware espec√≠ficas"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.hardware_profile = config.get("monitoring", {}).get("vertical_scaling_metrics", {}).get("hardware_profile",
                                                                                                     "unknown")

        # üñ•Ô∏è Informaci√≥n del hardware
        self.cpu_count = psutil.cpu_count()
        self.memory_total = psutil.virtual_memory().total
        self.memory_total_gb = self.memory_total / (1024 ** 3)

        # üìä M√©tricas verticales
        self.vertical_metrics = {
            'cpu_per_core': [0.0] * self.cpu_count,
            'memory_pressure': 0.0,
            'cache_efficiency': 0.0,
            'batch_performance': 0.0,
            'hardware_utilization': 0.0,
            'last_update': time.time()
        }

        # üîß Optimizaciones espec√≠ficas
        self.vertical_config = config.get("processing", {}).get("vertical_scaling", {})
        self.leave_cores_for_system = self.vertical_config.get("leave_cores_for_system", 2)
        self.recommended_threads = min(self.cpu_count - self.leave_cores_for_system,
                                       config.get("processing", {}).get("threads", 4))

        logging.info(f"üèóÔ∏è Vertical Scaling Manager inicializado:")
        logging.info(f"   üíª Hardware: {self.hardware_profile}")
        logging.info(f"   üñ•Ô∏è CPU cores: {self.cpu_count} (usando {self.recommended_threads})")
        logging.info(f"   üß† RAM total: {self.memory_total_gb:.1f}GB")
        logging.info(f"   üéØ Cores reservados para sistema: {self.leave_cores_for_system}")

    def update_vertical_metrics(self):
        """Actualiza m√©tricas espec√≠ficas de escalado vertical"""
        try:
            # üíª CPU por core
            cpu_percents = psutil.cpu_percent(percpu=True)
            if len(cpu_percents) == self.cpu_count:
                self.vertical_metrics['cpu_per_core'] = cpu_percents

            # üß† Presi√≥n de memoria
            memory = psutil.virtual_memory()
            self.vertical_metrics['memory_pressure'] = memory.percent / 100.0

            # üñ•Ô∏è Utilizaci√≥n de hardware total
            avg_cpu = sum(cpu_percents) / len(cpu_percents) / 100.0
            memory_usage = memory.percent / 100.0
            self.vertical_metrics['hardware_utilization'] = (avg_cpu + memory_usage) / 2.0

            self.vertical_metrics['last_update'] = time.time()

        except Exception as e:
            logging.error(f"‚ùå Error actualizando m√©tricas verticales: {e}")

    def get_cpu_aware_delay(self, base_delay_ms: int) -> float:
        """Calcula delay adaptativo basado en CPU"""
        try:
            avg_cpu = sum(self.vertical_metrics['cpu_per_core']) / len(self.vertical_metrics['cpu_per_core'])

            if avg_cpu > 80.0:  # CPU alta
                return base_delay_ms * 1.5  # Delay mayor
            elif avg_cpu > 60.0:  # CPU media
                return base_delay_ms * 1.2
            else:  # CPU baja
                return base_delay_ms * 0.8  # Delay menor

        except:
            return base_delay_ms

    def get_memory_pressure_factor(self) -> float:
        """Factor de presi√≥n de memoria para ajustar caches"""
        memory_pressure = self.vertical_metrics.get('memory_pressure', 0.0)

        if memory_pressure > 0.9:  # 90%+ memoria usada
            return 0.5  # Reducir caches agresivamente
        elif memory_pressure > 0.8:  # 80%+ memoria usada
            return 0.7  # Reducir caches moderadamente
        elif memory_pressure > 0.6:  # 60%+ memoria usada
            return 0.9  # Reducir caches ligeramente
        else:
            return 1.0  # Sin reducci√≥n

    def should_enable_batch_processing(self) -> bool:
        """Determina si habilitar batch processing seg√∫n recursos"""
        cpu_avg = sum(self.vertical_metrics['cpu_per_core']) / len(self.vertical_metrics['cpu_per_core'])
        memory_ok = self.vertical_metrics['memory_pressure'] < 0.8

        return cpu_avg < 70.0 and memory_ok


class DistributedGeoIPEnricherVertical:
    """
    GeoIP Enricher distribuido optimizado para escalado vertical v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS
    üö® BUG FIX: Geoposiciona target_ip (atacantes) correctamente
    üåê NUEVO: Discovery autom√°tico de IP p√∫blica
    üéØ NUEVO: Enriquecimiento dual (source + target)
    üì¶ ACTUALIZADO: Protobuf v3.0.0 con campos MODERNOS √∫nicamente
    üìù MEJORADO: Logging dual (consola + archivo)
    üîß MODIFICADO: Lookup real MaxMind SIN hardcodeos
    üåê A√ëADIDO: Soporte completo para IPAPI
    üö´ ELIMINADO: Uso de campos LEGACY deprecados
    üìñ READONLY: Campos b√°sicos del evento (1-10) - SOLO lectura
    ‚úçÔ∏è WRITEONLY: Campos de geolocalizaci√≥n (54+) - SOLO escritura
    """

    def __init__(self, config_file: str):
        # üìÑ Cargar configuraci√≥n - SIN defaults hardcodeados
        self.config = self._load_config_strict(config_file)
        self.config_file = config_file

        # üè∑Ô∏è Identidad distribuida
        self.node_id = self.config["node_id"]
        self.process_id = os.getpid()
        self.container_id = self._get_container_id()
        self.start_time = time.time()

        # üñ•Ô∏è Informaci√≥n del sistema
        self.system_info = self._gather_system_info()

        # üèóÔ∏è Gestor de escalado vertical
        self.vertical_manager = VerticalScalingManager(self.config)

        # üìù Setup logging desde configuraci√≥n (PRIMERO)
        self.setup_logging()

        # üåê NUEVO v3.0.0: Handler de IPs con discovery p√∫blico
        self.ip_handler = IPAddressHandler(self.config)

        # üîå Setup ZeroMQ con optimizaciones verticales
        self.context = zmq.Context()
        self.input_socket = None
        self.output_socket = None
        self.setup_sockets_vertical()

        # üîÑ Backpressure desde configuraci√≥n con optimizaciones verticales
        self.backpressure_config = self.config["backpressure"]
        self.vertical_backpressure = self.backpressure_config.get("vertical_optimizations", {})

        # üì¶ Colas internas optimizadas para hardware
        self.setup_internal_queues_vertical()

        # üåç Configuraci√≥n GeoIP con optimizaciones verticales
        self.geoip_config = self.config["geoip"]
        self.vertical_geoip = self.geoip_config.get("vertical_optimizations", {})

        # üóÑÔ∏è Setup cache GeoIP optimizado
        self.setup_geoip_cache_vertical()

        # üìä M√©tricas distribuidas con m√©tricas verticales v3.0.0
        self.stats = {
            'received': 0,
            'enriched': 0,
            'sent': 0,
            'failed_lookups': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'buffer_errors': 0,
            'processing_errors': 0,
            'backpressure_activations': 0,
            'queue_overflows': 0,
            'protobuf_errors': 0,
            'pipeline_latency_total': 0.0,
            'batch_processed': 0,
            'vertical_optimizations_applied': 0,
            'cpu_aware_delays': 0,
            'memory_pressure_reductions': 0,
            # üÜï Estad√≠sticas v3.0.0 espec√≠ficas
            'source_ip_enriched': 0,
            'target_ip_enriched': 0,
            'dual_enrichment_success': 0,
            'public_ip_discoveries': 0,
            'v3_events_processed': 0,
            'maxmind_lookups': 0,
            'api_lookups': 0,
            'ipapi_lookups': 0,
            'lookup_failures': 0,
            'modern_fields_used': 0,
            'legacy_fields_avoided': 0,
            'basic_fields_read': 0,
            'basic_fields_validated': 0,
            'invalid_basic_events': 0,
            'start_time': time.time(),
            'last_stats_time': time.time()
        }

        # üéõÔ∏è Control
        self.running = True
        self.stop_event = Event()

        # üìà Batch processing inteligente
        self.batch_config = self.config.get("processing", {}).get("batch_processing", {})
        self.batch_queue = Queue(maxsize=self.batch_config.get("batch_size", 50))

        # ‚úÖ Verificar dependencias cr√≠ticas
        self._verify_dependencies()

        # üìù Log configuraci√≥n v3.0.0
        self._log_v3_configuration()

        # üåê NUEVO: Log informaci√≥n del proveedor API
        self._log_api_provider_info()

        self.logger.info(f"üåç Distributed GeoIP Enricher VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS inicializado")
        self.logger.info(f"   üè∑Ô∏è Node ID: {self.node_id}")
        self.logger.info(f"   üî¢ PID: {self.process_id}")
        self.logger.info(f"   üìÑ Config: {config_file}")
        self.logger.info(f"   üèóÔ∏è Escalado vertical: ‚úÖ")
        self.logger.info(f"   üì¶ Protobuf: {PROTOBUF_VERSION}")
        self.logger.info(f"   üåç MaxMind: {'‚úÖ' if MAXMIND_AVAILABLE else '‚ùå'}")
        self.logger.info(f"   üåê IPAPI: {'‚úÖ' if self.geoip_config.get('api', {}).get('enabled') else '‚ùå'}")
        self.logger.info(f"   üñ•Ô∏è Hardware profile: {self.vertical_manager.hardware_profile}")
        self.logger.info(f"   üö® Bug fix aplicado: target_ip geoposicionado ‚úÖ")
        self.logger.info(f"   üö´ Campos LEGACY: EVITADOS completamente ‚úÖ")
        self.logger.info(f"   ‚úÖ Campos MODERNOS: v3.0.0 √∫nicamente ‚úÖ")
        self.logger.info(f"   üìñ Campos B√ÅSICOS (1-10): SOLO lectura ‚úÖ")
        self.logger.info(f"   ‚úçÔ∏è Campos GEOLOCALIZACI√ìN (54+): SOLO escritura ‚úÖ")

    def _validate_basic_event_fields(self, event) -> bool:
        """
        üìñ NUEVO: Valida que los campos b√°sicos del evento est√©n presentes
        Estos campos DEBEN venir rellenos del promiscuous_agent
        """
        try:
            required_basic_fields = {
                'event_id': (1, str),
                'timestamp': (2, int),
                'source_ip': (3, str),
                'target_ip': (4, str),
                'packet_size': (5, int),
                'dest_port': (6, int),
                'src_port': (7, int),
                'protocol': (8, str),
                'agent_id': (9, str),
                'anomaly_score': (10, float)
            }

            missing_fields = []
            invalid_fields = []

            for field_name, (field_number, expected_type) in required_basic_fields.items():
                if not hasattr(event, field_name):
                    missing_fields.append(f"{field_name} (campo {field_number})")
                    continue

                field_value = getattr(event, field_name)

                # Verificar que no est√© vac√≠o
                if field_name in ['event_id', 'source_ip', 'target_ip', 'protocol', 'agent_id']:
                    if not field_value or field_value == '':
                        invalid_fields.append(f"{field_name} est√° vac√≠o")
                        continue

                # Verificar timestamp v√°lido
                if field_name == 'timestamp' and field_value <= 0:
                    invalid_fields.append(f"{field_name} inv√°lido: {field_value}")
                    continue

                # Verificar puertos v√°lidos
                if field_name in ['dest_port', 'src_port'] and (field_value < 0 or field_value > 65535):
                    invalid_fields.append(f"{field_name} fuera de rango: {field_value}")
                    continue

            if missing_fields or invalid_fields:
                self.logger.error(f"‚ùå Evento con campos b√°sicos inv√°lidos:")
                for field in missing_fields:
                    self.logger.error(f"   üö´ Campo faltante: {field}")
                for field in invalid_fields:
                    self.logger.error(f"   ‚ö†Ô∏è Campo inv√°lido: {field}")

                self.stats['invalid_basic_events'] += 1
                return False

            self.stats['basic_fields_validated'] += 1
            self.logger.debug(
                f"‚úÖ Campos b√°sicos validados: event_id={event.event_id}, source_ip={event.source_ip}, target_ip={event.target_ip}")
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Error validando campos b√°sicos: {e}")
            self.stats['invalid_basic_events'] += 1
            return False

    def _log_api_provider_info(self):
        """üìù NUEVO: Log informaci√≥n del proveedor de API configurado"""
        api_config = self.geoip_config.get("api", {})
        if api_config.get("enabled", False):
            provider = api_config.get("provider", "unknown")
            has_key = bool(api_config.get("api_key"))

            self.logger.info(f"üåê Proveedor API configurado: {provider}")
            self.logger.info(f"   üîë API Key: {'‚úÖ Configurada' if has_key else '‚ùå No configurada (plan gratuito)'}")
            self.logger.info(f"   ‚è±Ô∏è Timeout: {api_config.get('timeout_seconds', 5)}s")
            self.logger.info(f"   üîÑ Max retries: {api_config.get('max_retries', 1)}")

            if provider == "ipapi" and not has_key:
                self.logger.info("   ‚ÑπÔ∏è IPAPI plan gratuito: 1000 requests/month")
            elif provider == "ipapi" and has_key:
                self.logger.info("   ‚ÑπÔ∏è IPAPI plan pago: l√≠mites seg√∫n suscripci√≥n")

    def _log_v3_configuration(self):
        """Log configuraci√≥n espec√≠fica v3.0.0"""
        processing_config = self.config.get("processing", {})
        self.logger.info("üéØ Configuraci√≥n de enriquecimiento v3.0.0 + READONLY CAMPOS B√ÅSICOS:")
        self.logger.info(f"   üè† source_ip: {'‚úÖ' if processing_config.get('geolocate_source_ip') else '‚ùå'}")
        self.logger.info(f"   üéØ target_ip: {'‚úÖ' if processing_config.get('geolocate_target_ip') else '‚ùå'}")
        self.logger.info(
            f"   ‚≠ê Prioridad: {'target_ip' if processing_config.get('prioritize_target_ip') else 'source_ip'}")
        self.logger.info(f"   üåê IP p√∫blica discovery: {'‚úÖ' if self.ip_handler.public_ip_discovery.enabled else '‚ùå'}")
        self.logger.info(f"   üì¶ Protobuf version: {PROTOBUF_VERSION}")
        self.logger.info(f"   üåç GeoIP method: {self.geoip_config.get('lookup_method', 'unknown')}")
        self.logger.info(f"   ‚ö° Performance mode: {self.geoip_config.get('performance_mode', 'speed')}")
        self.logger.info(f"   üö´ Campos legacy: EVITADOS (latitude/longitude legacy)")
        self.logger.info(f"   ‚úÖ Campos modernos: source_latitude, target_latitude, etc.")
        self.logger.info(f"   üìñ Campos b√°sicos (1-10): SOLO lectura desde promiscuous_agent")
        self.logger.info(f"   ‚úçÔ∏è Campos geolocalizaci√≥n (54+): SOLO escritura por GeoIP enricher")

        if self.ip_handler.public_ip_discovery.enabled:
            services = self.ip_handler.public_ip_discovery.services
            self.logger.info(f"   üîó Servicios IP discovery: {len(services)} configurados")
            for service in services:
                self.logger.debug(f"     - {service}")

    def _load_config_strict(self, config_file: str) -> Dict[str, Any]:
        """Carga configuraci√≥n SIN proporcionar defaults"""
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
        except FileNotFoundError:
            raise RuntimeError(f"‚ùå Archivo de configuraci√≥n no encontrado: {config_file}")
        except json.JSONDecodeError as e:
            raise RuntimeError(f"‚ùå Error parseando JSON: {e}")

        # ‚úÖ Validar campos cr√≠ticos incluyendo verticales
        required_fields = [
            "node_id", "network", "zmq", "backpressure", "processing",
            "geoip", "logging", "monitoring", "distributed"
        ]

        for field in required_fields:
            if field not in config:
                raise RuntimeError(f"‚ùå Campo requerido faltante en config: {field}")

        return config

    def _get_container_id(self) -> Optional[str]:
        """Obtiene ID del contenedor si est√° ejecut√°ndose en uno"""
        try:
            with open('/proc/self/cgroup', 'r') as f:
                content = f.read()
                for line in content.split('\n'):
                    if 'docker' in line:
                        return line.split('/')[-1][:12]
            return None
        except:
            return None

    def _gather_system_info(self) -> Dict[str, Any]:
        """Recolecta informaci√≥n del sistema con detalles verticales"""
        return {
            'hostname': socket.gethostname(),
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': round(psutil.virtual_memory().total / (1024 ** 3), 2),
            'cpu_freq_max': psutil.cpu_freq().max if psutil.cpu_freq() else 0,
            'load_avg': os.getloadavg() if hasattr(os, 'getloadavg') else [0, 0, 0]
        }

    def _verify_dependencies(self):
        """Verifica que las dependencias cr√≠ticas est√©n disponibles"""
        issues = []

        if not PROTOBUF_AVAILABLE:
            issues.append("‚ùå Protobuf network_event_extended_v3_pb2 no disponible")

        if not MAXMIND_AVAILABLE:
            issues.append("‚ö†Ô∏è MaxMind geoip2 no disponible - install: pip install geoip2")

        if not CACHE_AVAILABLE:
            issues.append("‚ö†Ô∏è LRU Cache no disponible - rendimiento reducido")

        if issues:
            for issue in issues:
                print(issue)
            if not PROTOBUF_AVAILABLE:
                raise RuntimeError("‚ùå Protobuf v3 es cr√≠tico para el funcionamiento")

    def setup_logging(self):
        """Setup logging dual (consola + archivo) con formato de una l√≠nea"""
        log_config = self.config["logging"]

        # üìù Configurar nivel
        level = getattr(logging, log_config["level"].upper())

        # üè∑Ô∏è Formato compacto de una l√≠nea
        log_format = f"%(asctime)s | {self.node_id} | PID:{self.process_id} | %(levelname)-8s | %(name)-20s | %(message)s"
        formatter = logging.Formatter(log_format)

        # üîß Setup logger principal
        self.logger = logging.getLogger(f"geoip_enricher_{self.node_id}")
        self.logger.setLevel(level)

        # Limpiar handlers existentes
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)

        # üñ•Ô∏è Handler de consola
        handlers_config = log_config.get("handlers", {})
        console_config = handlers_config.get("console", {})

        if console_config.get("enabled", True):
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            console_handler.setLevel(getattr(logging, console_config.get("level", "INFO").upper()))
            self.logger.addHandler(console_handler)
            self.logger.debug("üñ•Ô∏è Console logging habilitado")

        # üìÅ Handler de archivo
        file_config = handlers_config.get("file", {})
        if file_config.get("enabled", False):
            log_file = file_config.get("path", "logs/geoip_enricher.log")

            # Crear directorio si no existe
            log_dir = os.path.dirname(log_file)
            if log_dir:
                os.makedirs(log_dir, exist_ok=True)

            file_handler = logging.FileHandler(log_file)
            file_handler.setFormatter(formatter)
            file_handler.setLevel(getattr(logging, file_config.get("level", "DEBUG").upper()))
            self.logger.addHandler(file_handler)
            self.logger.info(f"üìÅ File logging habilitado: {log_file}")

        self.logger.propagate = False
        self.logger.info("üìù Logging dual configurado correctamente (consola + archivo, formato de una l√≠nea)")

    def setup_sockets_vertical(self):
        """Configuraci√≥n ZMQ con optimizaciones verticales"""
        network_config = self.config["network"]
        zmq_config = self.config["zmq"]
        vertical_opts = zmq_config.get("vertical_scaling_optimizations", {})

        try:
            # üîß Configurar contexto ZMQ con optimizaciones verticales
            if vertical_opts.get("io_threads"):
                self.context = zmq.Context(vertical_opts["io_threads"])

            # üì• Socket de entrada (PULL) - CONNECT al promiscuous_agent
            input_config = network_config["input_socket"]
            self.input_socket = self.context.socket(zmq.PULL)
            self.input_socket.setsockopt(zmq.RCVHWM, zmq_config["rcvhwm"])
            self.input_socket.setsockopt(zmq.RCVTIMEO, zmq_config["recv_timeout_ms"])

            # üîß Optimizaciones verticales para input
            if vertical_opts.get("tcp_keepalive"):
                self.input_socket.setsockopt(zmq.TCP_KEEPALIVE, 1)
                if vertical_opts.get("tcp_keepalive_idle"):
                    self.input_socket.setsockopt(zmq.TCP_KEEPALIVE_IDLE, vertical_opts["tcp_keepalive_idle"])

            if vertical_opts.get("immediate"):
                self.input_socket.setsockopt(zmq.IMMEDIATE, 1)

            # CONNECT al puerto del promiscuous_agent
            input_address = f"tcp://{input_config['address']}:{input_config['port']}"
            self.input_socket.connect(input_address)

            # üì§ Socket de salida (PUSH) - BIND para ml_detector
            output_config = network_config["output_socket"]
            self.output_socket = self.context.socket(zmq.PUSH)
            self.output_socket.setsockopt(zmq.SNDHWM, zmq_config["sndhwm"])
            self.output_socket.setsockopt(zmq.LINGER, zmq_config["linger_ms"])
            self.output_socket.setsockopt(zmq.SNDTIMEO, zmq_config["send_timeout_ms"])

            # üîß Optimizaciones verticales para output
            if vertical_opts.get("immediate"):
                self.output_socket.setsockopt(zmq.IMMEDIATE, 1)

            # BIND para que ml_detector se conecte
            output_address = f"tcp://*:{output_config['port']}"
            self.output_socket.bind(output_address)

            self.logger.info(f"üîå Sockets ZMQ VERTICAL v3.0.0 configurados:")
            self.logger.info(f"   üì• Input: CONNECT to {input_address}")
            self.logger.info(f"   üì§ Output: BIND on {output_address}")
            self.logger.info(f"   üåä RCVHWM: {zmq_config['rcvhwm']}, SNDHWM: {zmq_config['sndhwm']}")
            self.logger.info(f"   üèóÔ∏è IO Threads: {vertical_opts.get('io_threads', 1)}")
            self.logger.info(f"   ‚ö° TCP Optimizations: {'‚úÖ' if vertical_opts.get('tcp_keepalive') else '‚ùå'}")

        except Exception as e:
            raise RuntimeError(f"‚ùå Error configurando sockets ZMQ verticales: {e}")

    def setup_internal_queues_vertical(self):
        """Configuraci√≥n de colas internas optimizadas para hardware"""
        proc_config = self.config["processing"]

        # üìä Ajustar tama√±os seg√∫n memoria disponible
        memory_factor = self.vertical_manager.get_memory_pressure_factor()

        base_protobuf_size = proc_config["protobuf_queue_size"]
        base_internal_size = proc_config["internal_queue_size"]

        adjusted_protobuf_size = int(base_protobuf_size * memory_factor)
        adjusted_internal_size = int(base_internal_size * memory_factor)

        # üìã Cola principal para eventos protobuf sin procesar
        self.protobuf_queue = Queue(maxsize=adjusted_protobuf_size)

        # üìã Cola para eventos enriquecidos listos para env√≠o
        self.enriched_queue = Queue(maxsize=adjusted_internal_size)

        self.logger.info(f"üìã Colas internas VERTICAL v3.0.0 configuradas:")
        self.logger.info(f"   üì¶ Protobuf queue: {adjusted_protobuf_size} (factor: {memory_factor:.2f})")
        self.logger.info(f"   üåç Enriched queue: {adjusted_internal_size}")
        self.logger.info(
            f"   üß† Memory pressure: {self.vertical_manager.vertical_metrics['memory_pressure'] * 100:.1f}%")

    def setup_geoip_cache_vertical(self):
        """Configura cache GeoIP optimizado para escalado vertical"""
        geoip_config = self.config["geoip"]
        vertical_opts = geoip_config.get("vertical_optimizations", {})
        performance_mode = geoip_config.get("performance_mode", "speed")

        if geoip_config.get("cache_enabled", False) and CACHE_AVAILABLE:
            # üìä Ajustar cache size seg√∫n memoria y optimizaciones
            base_cache_size = geoip_config.get("cache_size", 1000)

            if vertical_opts.get("optimized_for_32gb_ram"):
                # Optimizar para 32GB RAM
                base_cache_size = min(base_cache_size, 20000)  # No exceder 20K entradas

            memory_factor = self.vertical_manager.get_memory_pressure_factor()

            # Ajustar seg√∫n performance mode
            if performance_mode == "speed":
                final_cache_size = int(base_cache_size * memory_factor * 1.5)  # Cache m√°s grande
            else:  # precision
                final_cache_size = int(base_cache_size * memory_factor * 0.5)  # Cache m√°s peque√±o

            # üóÑÔ∏è Crear cache LRU optimizado
            @lru_cache(maxsize=final_cache_size)
            def cached_lookup(ip_address: str) -> Optional[Dict[str, Any]]:
                return self._direct_geoip_lookup(ip_address)

            self.cached_geoip_lookup = cached_lookup
            self.cache_enabled = True

            self.logger.info(f"üóÑÔ∏è Cache GeoIP VERTICAL v3.0.0 habilitado:")
            self.logger.info(f"   üìä Cache size: {final_cache_size} entradas")
            self.logger.info(f"   üß† Memory factor: {memory_factor:.2f}")
            self.logger.info(f"   ‚ö° Performance mode: {performance_mode}")
            self.logger.info(f"   üèóÔ∏è 32GB optimized: {'‚úÖ' if vertical_opts.get('optimized_for_32gb_ram') else '‚ùå'}")
        else:
            self.cache_enabled = False
            self.logger.info("üóÑÔ∏è Cache GeoIP deshabilitado")

    # ============================================================
    # üÜï NUEVOS M√âTODOS PARA LOOKUP REAL COMPLETO v3.0.0 + IPAPI
    # ============================================================

    def get_complete_geoip_info(self, ip_address: str) -> Optional[Dict[str, Any]]:
        """
        üåç NUEVO v3.0.0: Obtiene informaci√≥n geogr√°fica completa
        Devuelve diccionario completo o None si falla completamente
        """
        if not ip_address or ip_address == 'unknown':
            return None

        try:
            # üóÑÔ∏è Verificar cache si est√° habilitado
            if self.cache_enabled:
                result = self.cached_geoip_lookup(ip_address)
                if result:
                    self.stats['cache_hits'] += 1
                    return result
                else:
                    self.stats['cache_misses'] += 1
            else:
                self.stats['cache_misses'] += 1
                return self._direct_geoip_lookup(ip_address)

        except Exception as e:
            self.logger.warning(f"‚ùå Error lookup GeoIP completo para {ip_address}: {e}")
            self.stats['lookup_failures'] += 1
            return None

    def _direct_geoip_lookup(self, ip_address: str) -> Optional[Dict[str, Any]]:
        """
        üîß MODIFICADO v3.0.0: Lookup directo con informaci√≥n completa
        SIN hardcodeos - usa MaxMind primary, IPAPI fallback
        """
        geoip_config = self.config["geoip"]
        lookup_method = geoip_config.get("lookup_method", "maxmind")
        fallback_method = geoip_config.get("fallback_method", "ipapi")

        # üéØ Intentar m√©todo primario
        if lookup_method == "maxmind":
            result = self._maxmind_lookup(ip_address)
            if result and result.get('latitude') is not None:
                return result
        elif lookup_method == "ipapi" or lookup_method == "api":
            result = self._api_lookup(ip_address)
            if result and result.get('latitude') is not None:
                return result

        # üîÑ Intentar m√©todo de fallback
        if fallback_method and fallback_method != lookup_method:
            self.logger.debug(f"üîÑ Trying fallback method {fallback_method} for {ip_address}")
            if fallback_method == "maxmind":
                result = self._maxmind_lookup(ip_address)
                if result and result.get('latitude') is not None:
                    return result
            elif fallback_method == "ipapi" or fallback_method == "api":
                result = self._api_lookup(ip_address)
                if result and result.get('latitude') is not None:
                    return result

        # ‚ùå Lookup fallido completamente
        self.logger.warning(f"‚ùå No se pudo geoposicionar {ip_address} con ning√∫n m√©todo")
        self.stats['lookup_failures'] += 1
        return None

    def _maxmind_lookup(self, ip_address: str) -> Optional[Dict[str, Any]]:
        """üåç NUEVO: Lookup usando MaxMind GeoLite2-City database"""
        if not MAXMIND_AVAILABLE:
            return None

        try:
            maxmind_config = self.geoip_config.get("maxmind", {})
            if not maxmind_config.get("enabled", False):
                return None

            db_path = maxmind_config.get("database_path", "data/GeoLite2-City.mmdb")

            with geoip2.database.Reader(db_path) as reader:
                response = reader.city(ip_address)

                self.stats['maxmind_lookups'] += 1

                return {
                    'latitude': float(response.location.latitude) if response.location.latitude else None,
                    'longitude': float(response.location.longitude) if response.location.longitude else None,
                    'city': response.city.name or '',
                    'country': response.country.name or '',
                    'country_code': response.country.iso_code or '',
                    'region': response.subdivisions.most_specific.name or '',
                    'timezone': response.location.time_zone or '',
                    'lookup_method': 'maxmind',
                    'accuracy_radius': response.location.accuracy_radius,
                    'postal_code': response.postal.code or ''
                }

        except geoip2.errors.AddressNotFoundError:
            self.logger.debug(f"üîç MaxMind: IP {ip_address} no encontrada en database")
            return None
        except FileNotFoundError:
            self.logger.error(f"‚ùå MaxMind database no encontrada: {maxmind_config.get('database_path')}")
            return None
        except Exception as e:
            self.logger.warning(f"‚ùå MaxMind lookup error para {ip_address}: {e}")
            return None

    def _api_lookup(self, ip_address: str) -> Optional[Dict[str, Any]]:
        """üåê MODIFICADO: Lookup usando API externa con soporte multi-proveedor (IPAPI)"""
        try:
            api_config = self.geoip_config.get("api", {})
            if not api_config.get("enabled", False):
                return None

            provider = api_config.get("provider", "ipgeolocation").lower()
            timeout = api_config.get("timeout_seconds", 5.0)
            max_retries = api_config.get("max_retries", 1)
            api_key = api_config.get("api_key")

            # üåê Construir URL espec√≠fica del proveedor
            if provider == "ipapi":
                base_url = api_config.get("base_url", "https://ipapi.co")
                if api_key:
                    url = f"{base_url}/{ip_address}/json/?key={api_key}"
                else:
                    url = f"{base_url}/{ip_address}/json/"
            else:
                # Fallback para otros proveedores (IPGeolocation, etc.)
                url = api_config.get("url", "").format(api_key=api_key, ip=ip_address)

            # üîÑ Intentar lookup con reintentos
            for attempt in range(max_retries + 1):
                try:
                    request = urllib.request.Request(url)
                    request.add_header('User-Agent', 'GeoIP-Enricher-v3.0.0')

                    with urllib.request.urlopen(request, timeout=timeout) as response:
                        raw_data = response.read().decode('utf-8')
                        data = json.loads(raw_data)

                        # üîç Verificar errores espec√≠ficos del proveedor
                        if provider == "ipapi":
                            if data.get('error'):
                                raise Exception(f"IPAPI error: {data.get('reason', 'Unknown error')}")

                            # Mapeo para IPAPI
                            result = {
                                'latitude': float(data.get('latitude')) if data.get('latitude') else None,
                                'longitude': float(data.get('longitude')) if data.get('longitude') else None,
                                'city': data.get('city', ''),
                                'country': data.get('country_name', ''),
                                'country_code': data.get('country_code', data.get('country', '')),
                                'region': data.get('region', ''),
                                'timezone': data.get('timezone', ''),
                                'lookup_method': 'ipapi',
                                'isp': data.get('org', ''),
                                'organization': data.get('org', ''),
                                'postal_code': data.get('postal', ''),
                            }

                            # Contabilizar lookup IPAPI espec√≠ficamente
                            self.stats['ipapi_lookups'] += 1
                        else:
                            # Mapeo para IPGeolocation (original)
                            result = {
                                'latitude': float(data.get('latitude')) if data.get('latitude') else None,
                                'longitude': float(data.get('longitude')) if data.get('longitude') else None,
                                'city': data.get('city', ''),
                                'country': data.get('country_name', ''),
                                'country_code': data.get('country_code2', ''),
                                'region': data.get('state_prov', ''),
                                'timezone': data.get('time_zone', {}).get('name', '') if isinstance(
                                    data.get('time_zone'), dict) else data.get('time_zone', ''),
                                'lookup_method': 'api',
                                'isp': data.get('isp', ''),
                                'organization': data.get('organization', '')
                            }

                        if result.get('latitude') is not None and result.get('longitude') is not None:
                            self.stats['api_lookups'] += 1
                            self.logger.debug(
                                f"‚úÖ {provider} lookup exitoso para {ip_address}: {result['city']}, {result['country']}")
                            return result
                        else:
                            self.logger.warning(f"‚ö†Ô∏è {provider} lookup sin coordenadas para {ip_address}")
                            return None

                except urllib.error.HTTPError as e:
                    if e.code == 429:  # Rate limit
                        self.logger.warning(f"‚ö†Ô∏è Rate limit en {provider} para {ip_address}")
                        if attempt < max_retries:
                            time.sleep(2 ** attempt)
                            continue
                    elif e.code == 403:
                        self.logger.error(f"‚ùå API key inv√°lida en {provider}")
                        break
                    else:
                        if attempt < max_retries:
                            time.sleep(1)
                            continue

                except Exception as e:
                    if attempt < max_retries:
                        time.sleep(1)
                        continue
                    break

            self.logger.warning(f"‚ùå {provider} lookup fallido para {ip_address} despu√©s de {max_retries + 1} intentos")
            return None

        except Exception as e:
            self.logger.error(f"‚ùå Error cr√≠tico en API lookup para {ip_address}: {e}")
            return None

    # ============================================================
    # üîß ENRIQUECIMIENTO CON VALIDACI√ìN DE CAMPOS B√ÅSICOS
    # ============================================================

    def enrich_protobuf_event_vertical_v3_readonly_basic(self, protobuf_data: bytes) -> Optional[bytes]:
        """
        üö® VERSI√ìN v3.0.0 READONLY CAMPOS B√ÅSICOS - USA √öNICAMENTE CAMPOS v3.0.0
        üìñ VALIDA: Que campos b√°sicos (1-10) est√©n presentes del promiscuous_agent
        üìñ LEE: √önicamente campos b√°sicos necesarios (source_ip, target_ip, timestamp)
        ‚úçÔ∏è ESCRIBE: √önicamente campos de geolocalizaci√≥n (54+)
        üö´ NO TOCA: Ning√∫n campo b√°sico del evento (1-10)
        """
        if not PROTOBUF_AVAILABLE:
            raise RuntimeError("‚ùå Protobuf v3 no disponible")

        try:
            # üì¶ Deserializar evento protobuf v3.0.0
            event = NetworkEventProto.NetworkEvent()
            event.ParseFromString(protobuf_data)

            # üìä Contabilizar evento v3 procesado
            self.stats['v3_events_processed'] += 1

            # ‚úÖ VALIDAR que campos b√°sicos est√©n presentes del promiscuous_agent
            if not self._validate_basic_event_fields(event):
                self.logger.error(f"‚ùå Evento rechazado: campos b√°sicos inv√°lidos o faltantes")
                return None

            # üìñ LEER campos b√°sicos necesarios (NO modificar)
            source_ip = event.source_ip  # Campo 3 - SOLO LECTURA
            target_ip = event.target_ip  # Campo 4 - SOLO LECTURA
            event_timestamp = event.timestamp  # Campo 2 - SOLO LECTURA
            event_id = event.event_id  # Campo 1 - SOLO LECTURA

            self.stats['basic_fields_read'] += 1
            self.logger.debug(
                f"üìñ Campos b√°sicos le√≠dos: event_id={event_id}, source_ip={source_ip}, target_ip={target_ip}")

            # üîß Configuraci√≥n de procesamiento v3.0.0
            processing_config = self.config.get("processing", {})
            geolocate_source = processing_config.get("geolocate_source_ip", True)
            geolocate_target = processing_config.get("geolocate_target_ip", True)
            prioritize_target = processing_config.get("prioritize_target_ip", True)

            # üåç Variables para informaci√≥n completa
            source_geoip_info = None
            target_geoip_info = None
            enrichment_success = False

            # üéØ CORRECCI√ìN CR√çTICA: Geoposicionar target_ip (atacante) PRIMERO con lookup real
            if geolocate_target and target_ip and target_ip != 'unknown':
                target_ip_to_lookup = self.ip_handler.resolve_target_ip_for_lookup(target_ip)
                if target_ip_to_lookup:
                    target_geoip_info = self.get_complete_geoip_info(target_ip_to_lookup)
                    if target_geoip_info and target_geoip_info.get('latitude') is not None:
                        self.stats['target_ip_enriched'] += 1
                        self.logger.debug(
                            f"‚úÖ target_ip geoposicionada: {target_ip} ‚Üí lat:{target_geoip_info['latitude']}, lon:{target_geoip_info['longitude']}, city:{target_geoip_info.get('city', 'N/A')}")
                        enrichment_success = True
                    else:
                        self.logger.warning(f"‚ùå No se pudo geoposicionar target_ip: {target_ip}")
                else:
                    self.logger.warning(f"‚ö†Ô∏è target_ip no v√°lida para lookup: {target_ip}")

            # üè† Geoposicionar source_ip (nuestra IP) con lookup real
            if geolocate_source and source_ip and source_ip != 'unknown':
                source_ip_to_lookup = self.ip_handler.resolve_source_ip_for_lookup(source_ip)
                if source_ip_to_lookup:
                    source_geoip_info = self.get_complete_geoip_info(source_ip_to_lookup)
                    if source_geoip_info and source_geoip_info.get('latitude') is not None:
                        self.stats['source_ip_enriched'] += 1
                        self.logger.debug(
                            f"‚úÖ source_ip geoposicionada: {source_ip} ‚Üí lat:{source_geoip_info['latitude']}, lon:{source_geoip_info['longitude']}, city:{source_geoip_info.get('city', 'N/A')}")
                        enrichment_success = True

                        # Si obtuvimos IP p√∫blica, contabilizar
                        if source_ip_to_lookup != source_ip:
                            self.stats['public_ip_discoveries'] += 1
                    else:
                        self.logger.warning(f"‚ùå No se pudo geoposicionar source_ip: {source_ip}")
                else:
                    self.logger.warning(f"‚ö†Ô∏è No se pudo resolver IP p√∫blica para source_ip privada: {source_ip}")

            # ============================================================
            # ‚úçÔ∏è ESCRIBIR √öNICAMENTE CAMPOS v3.0.0 DE GEOLOCALIZACI√ìN (54+)
            # üö´ NO TOCAR NING√öN CAMPO B√ÅSICO (1-10)
            # ============================================================

            # üè† SOURCE IP - CAMPOS v3.0.0 MODERNOS (SOLO ESCRITURA)
            if source_geoip_info and source_geoip_info.get('latitude') is not None:
                # üìç Coordenadas source (campos 54, 55)
                event.source_latitude = source_geoip_info['latitude']
                event.source_longitude = source_geoip_info['longitude']

                # üåç Informaci√≥n geogr√°fica source (campos 58-62)
                event.source_city = source_geoip_info.get('city', '')
                event.source_country = source_geoip_info.get('country', '')
                event.source_country_code = source_geoip_info.get('country_code', '')
                event.source_region = source_geoip_info.get('region', '')
                event.source_timezone = source_geoip_info.get('timezone', '')

                # üîç Estado de enriquecimiento (campo 68)
                event.source_ip_enriched = True

                # üè¢ ISP informaci√≥n source (campo 85)
                if source_geoip_info.get('isp'):
                    event.source_isp = source_geoip_info['isp']

                self.stats['modern_fields_used'] += 1
                self.logger.debug(f"‚úÖ Campos MODERNOS escritos para source_ip: {source_ip}")

            else:
                # Source no enriquecida
                event.source_ip_enriched = False

            # üéØ TARGET IP - CAMPOS v3.0.0 MODERNOS (SOLO ESCRITURA)
            if target_geoip_info and target_geoip_info.get('latitude') is not None:
                # üìç Coordenadas target (campos 56, 57)
                event.target_latitude = target_geoip_info['latitude']
                event.target_longitude = target_geoip_info['longitude']

                # üåç Informaci√≥n geogr√°fica target (campos 63-67)
                event.target_city = target_geoip_info.get('city', '')
                event.target_country = target_geoip_info.get('country', '')
                event.target_country_code = target_geoip_info.get('country_code', '')
                event.target_region = target_geoip_info.get('region', '')
                event.target_timezone = target_geoip_info.get('timezone', '')

                # üîç Estado de enriquecimiento (campo 69)
                event.target_ip_enriched = True

                # üè¢ ISP informaci√≥n target (campo 86)
                if target_geoip_info.get('isp'):
                    event.target_isp = target_geoip_info['isp']

                self.stats['modern_fields_used'] += 1
                self.logger.debug(f"‚úÖ Campos MODERNOS escritos para target_ip: {target_ip}")

            else:
                # Target no enriquecida
                event.target_ip_enriched = False

            # üîç ESTADO DE ENRIQUECIMIENTO v3.0.0 (campos 70, 71)
            if prioritize_target and target_geoip_info:
                event.geoip_primary_source = "target"
            elif source_geoip_info:
                event.geoip_primary_source = "source"
            elif target_geoip_info:
                event.geoip_primary_source = "target"
            else:
                event.geoip_primary_source = "none"

            # Dual enrichment success (campo 71)
            event.dual_enrichment_success = bool(source_geoip_info and target_geoip_info)

            # üåê DISCOVERY DE IP P√öBLICA v3.0.0 (campos 72-76)
            if (source_geoip_info and
                    source_ip and
                    self.ip_handler.is_private_ip(source_ip)):

                public_ip = self.ip_handler.public_ip_discovery.get_public_ip()
                if public_ip:
                    event.public_ip_discovered = True
                    event.original_source_ip = source_ip
                    event.discovered_public_ip = public_ip
                    event.ip_discovery_service = "discovery_service"
                    event.ip_discovery_timestamp = int(time.time() * 1000)
            else:
                event.public_ip_discovered = False

            # Contabilizar enriquecimiento dual exitoso
            if source_geoip_info and target_geoip_info:
                self.stats['dual_enrichment_success'] += 1

            # üö´ ASEGURAR QUE NO SE TOCAN CAMPOS LEGACY DEPRECADOS
            # NO tocar event.latitude (campo 11) - LEGACY
            # NO tocar event.longitude (campo 12) - LEGACY
            # NO tocar event.legacy_compatibility_mode (campo 94)
            self.stats['legacy_fields_avoided'] += 1

            # ============================================================
            # üîß METADATOS DE ENRIQUECIMIENTO v3.0.0 (campos 81-84)
            # ============================================================

            event.geoip_enricher_version = "3.0.0_ipapi_readonly_basic"
            event.geoip_method = self.geoip_config.get("lookup_method", "maxmind")
            event.fallback_coordinates_used = False  # Nunca usamos fallback hardcoded

            # Determinar fuente de datos
            if source_geoip_info or target_geoip_info:
                primary_info = target_geoip_info if target_geoip_info else source_geoip_info
                lookup_method = primary_info.get('lookup_method', 'unknown')
                if lookup_method == 'maxmind':
                    event.geoip_data_source = "GeoLite2"
                elif lookup_method == 'ipapi':
                    event.geoip_data_source = "ipapi"
                else:
                    event.geoip_data_source = "api"

            # üìä M√âTRICAS DE RENDIMIENTO v3.0.0 (usar timestamp le√≠do, no modificar)
            current_time_ms = int(time.time() * 1000)
            if event_timestamp > 0:
                processing_time = current_time_ms - event_timestamp
                event.geoip_lookup_latency_ms = max(0.0, float(processing_time))

            # üÜî Informaci√≥n espec√≠fica del pipeline
            event.geoip_enricher_pid = self.process_id
            event.geoip_enricher_timestamp = current_time_ms

            # üìä M√©tricas del pipeline (si est√°n disponibles)
            if hasattr(event, 'promiscuous_timestamp') and event.promiscuous_timestamp > 0:
                pipeline_latency = current_time_ms - event.promiscuous_timestamp
                event.processing_latency_ms = float(pipeline_latency)

            # üéØ Path del pipeline
            if hasattr(event, 'pipeline_path') and event.pipeline_path:
                event.pipeline_path += "->geoip_v3.0.0_readonly"
            else:
                event.pipeline_path = "promiscuous->geoip_v3.0.0_readonly"

            # Incrementar hops si el campo existe
            if hasattr(event, 'pipeline_hops'):
                event.pipeline_hops += 1
            else:
                event.pipeline_hops = 1

            # üè∑Ô∏è Tags v3.0.0 (si est√°n disponibles)
            if hasattr(event, 'component_tags'):
                event.component_tags.append(f"geoip_enricher_v3_readonly_{self.node_id}")

            if hasattr(event, 'component_metadata'):
                event.component_metadata["geoip_version"] = "3.0.0_readonly_basic"
                event.component_metadata["dual_ip_enrichment"] = "true"
                event.component_metadata["bug_fix_applied"] = "target_ip_prioritized"
                event.component_metadata["protobuf_version"] = PROTOBUF_VERSION
                event.component_metadata["lookup_real"] = "true"
                event.component_metadata["no_hardcoded_coords"] = "true"
                event.component_metadata["ipapi_support"] = "true"
                event.component_metadata["modern_fields_only"] = "true"
                event.component_metadata["legacy_fields_avoided"] = "true"
                event.component_metadata["basic_fields_readonly"] = "true"

            # üîÑ Estado del componente
            if hasattr(event, 'component_status'):
                event.component_status = "healthy_v3_readonly"

            # üîÑ Serializar evento enriquecido
            return event.SerializeToString()

        except Exception as e:
            self.stats['protobuf_errors'] += 1
            self.logger.error(f"‚ùå Error enriquecimiento v3.0.0 READONLY BASIC: {e}")
            return None

    # ================================================================
    # üîÑ RESTO DE M√âTODOS SIN CAMBIOS IMPORTANTES (Threading, etc.)
    # ================================================================

    def lookup_geoip_coordinates_vertical(self, ip_address: str) -> Optional[Tuple[float, float]]:
        """üîß ACTUALIZADO: Mantener compatibilidad con m√©todo legacy"""
        geoip_info = self.get_complete_geoip_info(ip_address)
        if geoip_info and geoip_info.get('latitude') is not None:
            return (geoip_info['latitude'], geoip_info['longitude'])
        return None

    def receive_protobuf_events_vertical(self):
        """Thread de recepci√≥n con optimizaciones verticales"""
        self.logger.info(
            "üì° Iniciando thread de recepci√≥n protobuf VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS...")

        consecutive_errors = 0
        queue_full_count = 0

        while self.running:
            try:
                # üìä Actualizar m√©tricas verticales peri√≥dicamente
                if time.time() % 5 < 0.1:  # Cada ~5 segundos
                    self.vertical_manager.update_vertical_metrics()

                # üì® Recibir evento protobuf
                protobuf_data = self.input_socket.recv(zmq.NOBLOCK)
                self.stats['received'] += 1
                consecutive_errors = 0

                # üîç Verificar presi√≥n seg√∫n hardware
                current_queue_usage = self.protobuf_queue.qsize() / self.protobuf_queue.maxsize
                cpu_pressure = sum(self.vertical_manager.vertical_metrics['cpu_per_core']) / len(
                    self.vertical_manager.vertical_metrics['cpu_per_core'])

                if current_queue_usage > 0.8 or cpu_pressure > 75.0:
                    queue_full_count += 1
                    if queue_full_count % 20 == 0:
                        self.logger.warning(
                            f"üî¥ Presi√≥n VERTICAL: Cola {current_queue_usage * 100:.1f}%, CPU {cpu_pressure:.1f}%")

                # üìã A√±adir a cola con estrategia vertical
                try:
                    queue_config = self.config["processing"].get("queue_overflow_handling", {})
                    queue_timeout = queue_config.get("max_queue_wait_ms", 100) / 1000.0

                    self.protobuf_queue.put(protobuf_data, timeout=queue_timeout)
                    queue_full_count = 0

                except:
                    self.stats['queue_overflows'] += 1

                    # üîÑ Aplicar estrategias verticales de overflow
                    if cpu_pressure > 80.0:  # CPU muy alta
                        # Descartar evento para aliviar presi√≥n
                        self.stats['vertical_optimizations_applied'] += 1
                        self.logger.debug("üîß Evento descartado por alta presi√≥n de CPU")

                    if queue_config.get("log_drops", True) and self.stats['queue_overflows'] % 50 == 0:
                        self.logger.warning(
                            f"‚ö†Ô∏è {self.stats['queue_overflows']} eventos descartados por presi√≥n vertical")

            except zmq.Again:
                continue
            except zmq.ZMQError as e:
                consecutive_errors += 1
                if consecutive_errors % 10 == 0:
                    self.logger.error(f"‚ùå Error ZMQ recepci√≥n vertical ({consecutive_errors}): {e}")
                time.sleep(0.1)

    def process_protobuf_events_vertical(self):
        """Thread de procesamiento con optimizaciones verticales"""
        self.logger.info("‚öôÔ∏è Iniciando thread de procesamiento VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS...")

        queue_timeout = self.config["processing"]["queue_timeout_seconds"]

        while self.running:
            try:
                # üìã Obtener evento protobuf de la cola
                protobuf_data = self.protobuf_queue.get(timeout=queue_timeout)

                # üîÑ Medir latencia de procesamiento
                start_time = time.time()

                # üåç Enriquecer con optimizaciones verticales v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS
                enriched_protobuf = self.enrich_protobuf_event_vertical_v3_readonly_basic(protobuf_data)

                if enriched_protobuf:
                    # üìä M√©tricas de latencia
                    processing_time = (time.time() - start_time) * 1000  # ms
                    self.stats['pipeline_latency_total'] += processing_time

                    self.stats['enriched'] += 1

                    # üìã A√±adir a cola de eventos enriquecidos
                    try:
                        self.enriched_queue.put(enriched_protobuf, timeout=queue_timeout)
                    except:
                        self.stats['queue_overflows'] += 1

                self.protobuf_queue.task_done()

            except Empty:
                continue
            except Exception as e:
                self.logger.error(f"‚ùå Error procesamiento vertical v3.0.0 READONLY BASIC: {e}")
                self.stats['processing_errors'] += 1

    def send_event_with_backpressure_vertical(self, enriched_data: bytes) -> bool:
        """Env√≠o con backpressure adaptativo vertical v3.0.0"""
        bp_config = self.backpressure_config
        vertical_opts = self.vertical_backpressure
        max_retries = bp_config["max_retries"]

        for attempt in range(max_retries + 1):
            try:
                self.output_socket.send(enriched_data, zmq.NOBLOCK)
                return True

            except zmq.Again:
                self.stats['buffer_errors'] += 1

                if attempt == max_retries:
                    return False

                # üîÑ Aplicar backpressure vertical adaptativo
                if not self._apply_backpressure_vertical(attempt):
                    return False

                continue

            except zmq.ZMQError as e:
                self.logger.error(f"‚ùå Error ZMQ env√≠o vertical: {e}")
                return False

        return False

    def _apply_backpressure_vertical(self, attempt: int) -> bool:
        """Aplica backpressure adaptativo seg√∫n CPU y memoria v3.0.0"""
        bp_config = self.backpressure_config
        vertical_opts = self.vertical_backpressure

        if not bp_config["enabled"]:
            return False

        if attempt >= bp_config["max_retries"]:
            return False

        # üîÑ Delay base desde configuraci√≥n
        delays = bp_config["retry_delays_ms"]
        base_delay_ms = delays[attempt] if attempt < len(delays) else delays[-1]

        # üîß Aplicar adaptaciones verticales
        if vertical_opts.get("cpu_aware_backpressure", False):
            adapted_delay = self.vertical_manager.get_cpu_aware_delay(base_delay_ms)
            self.stats['cpu_aware_delays'] += 1
        else:
            adapted_delay = base_delay_ms

        time.sleep(adapted_delay / 1000.0)
        self.stats['backpressure_activations'] += 1

        return True

    def send_enriched_events(self):
        """Thread de env√≠o est√°ndar v3.0.0"""
        self.logger.info("üì§ Iniciando thread de env√≠o vertical v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS...")
        queue_timeout = self.config["processing"]["queue_timeout_seconds"]

        while self.running:
            try:
                enriched_protobuf = self.enriched_queue.get(timeout=queue_timeout)
                success = self.send_event_with_backpressure_vertical(enriched_protobuf)
                if success:
                    self.stats['sent'] += 1
                self.enriched_queue.task_done()
            except Empty:
                continue
            except Exception as e:
                self.logger.error(f"‚ùå Error env√≠o vertical: {e}")

    def monitor_performance_vertical(self):
        """Thread de monitoreo con m√©tricas verticales v3.0.0"""
        monitoring_config = self.config["monitoring"]
        interval = monitoring_config["stats_interval_seconds"]

        while self.running:
            time.sleep(interval)
            if not self.running:
                break

            # üìä Actualizar m√©tricas verticales
            self.vertical_manager.update_vertical_metrics()
            self._log_performance_stats_vertical_v3_readonly_basic()
            self._check_performance_alerts_vertical()

    def _log_performance_stats_vertical_v3_readonly_basic(self):
        """Log de estad√≠sticas con m√©tricas verticales v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS"""
        now = time.time()
        interval = now - self.stats['last_stats_time']

        # üìä Rates
        recv_rate = self.stats['received'] / interval if interval > 0 else 0
        enrich_rate = self.stats['enriched'] / interval if interval > 0 else 0
        send_rate = self.stats['sent'] / interval if interval > 0 else 0

        # üìä Latencia promedio
        avg_latency = 0.0
        if self.stats['enriched'] > 0:
            avg_latency = self.stats['pipeline_latency_total'] / self.stats['enriched']

        # üìä Cache hit rate
        total_lookups = self.stats['cache_hits'] + self.stats['cache_misses']
        cache_hit_rate = (self.stats['cache_hits'] / total_lookups * 100) if total_lookups > 0 else 0

        # üñ•Ô∏è M√©tricas verticales
        vertical_metrics = self.vertical_manager.vertical_metrics
        cpu_avg = sum(vertical_metrics['cpu_per_core']) / len(vertical_metrics['cpu_per_core'])

        # üéØ Estad√≠sticas v3.0.0
        dual_success_rate = 0.0
        if self.stats['enriched'] > 0:
            dual_success_rate = (self.stats['dual_enrichment_success'] / self.stats['enriched']) * 100

        # üìñ Estad√≠sticas de validaci√≥n de campos b√°sicos
        basic_validation_rate = 0.0
        if self.stats['basic_fields_read'] > 0:
            basic_validation_rate = (self.stats['basic_fields_validated'] / self.stats['basic_fields_read']) * 100

        self.logger.info(f"üìä GeoIP Enricher VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS Stats:")
        self.logger.info(f"   üì® Recibidos: {self.stats['received']} ({recv_rate:.1f}/s)")
        self.logger.info(f"   üåç Enriquecidos: {self.stats['enriched']} ({enrich_rate:.1f}/s)")
        self.logger.info(f"   üì§ Enviados: {self.stats['sent']} ({send_rate:.1f}/s)")
        self.logger.info(f"   üè† Source IP enriquecidas: {self.stats['source_ip_enriched']}")
        self.logger.info(f"   üéØ Target IP enriquecidas: {self.stats['target_ip_enriched']}")
        self.logger.info(
            f"   üéØ‚ûïüè† Enriquecimiento dual: {self.stats['dual_enrichment_success']} ({dual_success_rate:.1f}%)")
        self.logger.info(f"   üåê Discoveries IP p√∫blica: {self.stats['public_ip_discoveries']}")
        self.logger.info(f"   üì¶ Eventos v3 procesados: {self.stats['v3_events_processed']}")
        self.logger.info(f"   üåç MaxMind lookups: {self.stats['maxmind_lookups']}")
        self.logger.info(f"   üåê API lookups (general): {self.stats['api_lookups']}")
        self.logger.info(f"   üåê IPAPI lookups: {self.stats['ipapi_lookups']}")
        self.logger.info(f"   ‚ùå Lookup failures: {self.stats['lookup_failures']}")
        self.logger.info(f"   ‚úÖ Campos MODERNOS usados: {self.stats['modern_fields_used']}")
        self.logger.info(f"   üö´ Campos LEGACY evitados: {self.stats['legacy_fields_avoided']}")
        self.logger.info(f"   üìñ Campos B√ÅSICOS le√≠dos: {self.stats['basic_fields_read']}")
        self.logger.info(
            f"   ‚úÖ Campos B√ÅSICOS validados: {self.stats['basic_fields_validated']} ({basic_validation_rate:.1f}%)")
        self.logger.info(f"   ‚ùå Eventos con campos b√°sicos inv√°lidos: {self.stats['invalid_basic_events']}")
        self.logger.info(f"   üóÑÔ∏è Cache: {cache_hit_rate:.1f}% hit rate")
        self.logger.info(f"   ‚è±Ô∏è Latencia promedio: {avg_latency:.1f}ms")
        self.logger.info(f"   üñ•Ô∏è CPU promedio: {cpu_avg:.1f}%")
        self.logger.info(f"   üß† Memory pressure: {vertical_metrics['memory_pressure'] * 100:.1f}%")
        self.logger.info(f"   üèóÔ∏è Hardware utilization: {vertical_metrics['hardware_utilization'] * 100:.1f}%")
        self.logger.info(f"   üìã Colas: protobuf={self.protobuf_queue.qsize()}, enriched={self.enriched_queue.qsize()}")
        self.logger.info(f"   üîß Optimizaciones verticales: {self.stats['vertical_optimizations_applied']}")
        self.logger.info(f"   üîÑ Delays adaptativos: {self.stats['cpu_aware_delays']}")

        # üåê Estad√≠sticas espec√≠ficas del proveedor API
        api_config = self.geoip_config.get("api", {})
        if api_config.get("enabled", False):
            provider = api_config.get("provider", "unknown")
            self.logger.info(f"   üåê Proveedor API activo: {provider}")

        # Reset stats
        for key in ['received', 'enriched', 'sent', 'failed_lookups', 'cache_hits', 'cache_misses',
                    'buffer_errors', 'backpressure_activations', 'queue_overflows', 'protobuf_errors',
                    'vertical_optimizations_applied', 'cpu_aware_delays', 'source_ip_enriched',
                    'target_ip_enriched', 'dual_enrichment_success', 'public_ip_discoveries', 'v3_events_processed',
                    'maxmind_lookups', 'api_lookups', 'ipapi_lookups', 'lookup_failures', 'modern_fields_used',
                    'legacy_fields_avoided', 'basic_fields_read', 'basic_fields_validated', 'invalid_basic_events']:
            self.stats[key] = 0

        self.stats['pipeline_latency_total'] = 0.0
        self.stats['last_stats_time'] = now

    def _check_performance_alerts_vertical(self):
        """Alertas de performance verticales"""
        monitoring_config = self.config["monitoring"]
        alerts = monitoring_config.get("alerts", {})
        vertical_metrics = self.vertical_manager.vertical_metrics

        # üö® Alertas espec√≠ficas de escalado vertical
        cpu_avg = sum(vertical_metrics['cpu_per_core']) / len(vertical_metrics['cpu_per_core'])
        if cpu_avg > alerts.get("max_cpu_sustained_percent", 80.0):
            self.logger.warning(f"üö® ALERTA VERTICAL: CPU sostenido alto ({cpu_avg:.1f}%)")

        memory_pressure = vertical_metrics['memory_pressure'] * 100
        memory_threshold = alerts.get("max_memory_usage_mb", 1024) / (
                self.vertical_manager.memory_total_gb * 1024) * 100
        if memory_pressure > memory_threshold:
            self.logger.warning(f"üö® ALERTA VERTICAL: Presi√≥n de memoria alta ({memory_pressure:.1f}%)")

        hardware_util = vertical_metrics['hardware_utilization'] * 100
        if hardware_util > 85.0:
            self.logger.warning(f"üö® ALERTA VERTICAL: Utilizaci√≥n de hardware alta ({hardware_util:.1f}%)")

        # üö® Alerta espec√≠fica de campos b√°sicos inv√°lidos
        if self.stats.get('invalid_basic_events', 0) > 0:
            self.logger.warning(
                f"üö® ALERTA: {self.stats['invalid_basic_events']} eventos con campos b√°sicos inv√°lidos recibidos")

    def run(self):
        """Ejecutar el enriquecedor vertical v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS"""
        self.logger.info("üöÄ Iniciando Distributed GeoIP Enricher VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS...")

        threads = []

        # Thread de recepci√≥n vertical
        recv_thread = threading.Thread(target=self.receive_protobuf_events_vertical, name="VerticalReceiver")
        threads.append(recv_thread)

        # Threads de procesamiento vertical
        num_threads = self.vertical_manager.recommended_threads
        for i in range(num_threads):
            proc_thread = threading.Thread(target=self.process_protobuf_events_vertical, name=f"VerticalProcessor-{i}")
            threads.append(proc_thread)

        # Threads de env√≠o
        num_send_threads = self.config["processing"].get("send_threads", 2)
        for i in range(num_send_threads):
            send_thread = threading.Thread(target=self.send_enriched_events, name=f"VerticalSender-{i}")
            threads.append(send_thread)

        # Thread de monitoreo vertical
        monitor_thread = threading.Thread(target=self.monitor_performance_vertical, name="VerticalMonitor")
        threads.append(monitor_thread)

        # üöÄ Iniciar todos los threads
        for thread in threads:
            thread.start()

        self.logger.info(
            f"‚úÖ GeoIP Enricher VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS iniciado con {len(threads)} threads")
        self.logger.info(f"   üì° Recepci√≥n: 1 thread")
        self.logger.info(
            f"   ‚öôÔ∏è Procesamiento: {num_threads} threads (optimizado para {self.vertical_manager.cpu_count} cores)")
        self.logger.info(f"   üì§ Env√≠o: {num_send_threads} threads")
        self.logger.info(f"   üñ•Ô∏è Hardware: {self.vertical_manager.hardware_profile}")
        self.logger.info(f"   üì¶ Protobuf: {PROTOBUF_VERSION}")
        self.logger.info(f"   üåç MaxMind: {'‚úÖ' if MAXMIND_AVAILABLE else '‚ùå'}")
        self.logger.info(f"   üåê IPAPI: {'‚úÖ' if self.geoip_config.get('api', {}).get('enabled') else '‚ùå'}")
        self.logger.info(f"   üö® Bug fix: target_ip geoposicionamiento ‚úÖ")
        self.logger.info(f"   üåê IP discovery: {'‚úÖ' if self.ip_handler.public_ip_discovery.enabled else '‚ùå'}")
        self.logger.info(f"   üîß Lookup real: ‚úÖ SIN hardcodeos")
        self.logger.info(f"   üö´ Campos legacy: EVITADOS completamente")
        self.logger.info(f"   ‚úÖ Campos modernos: v3.0.0 √∫nicamente")
        self.logger.info(f"   üìñ Campos b√°sicos (1-10): SOLO lectura")
        self.logger.info(f"   ‚úçÔ∏è Campos geolocalizaci√≥n (54+): SOLO escritura")

        try:
            while self.running:
                time.sleep(1)
        except KeyboardInterrupt:
            self.logger.info("üõë Deteniendo GeoIP Enricher VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS...")

        self.shutdown(threads)

    def shutdown(self, threads):
        """Cierre graceful vertical v3.0.0"""
        self.running = False
        self.stop_event.set()

        runtime = time.time() - self.stats['start_time']
        total_v3_events = self.stats.get('v3_events_processed', 0)
        total_dual_success = self.stats.get('dual_enrichment_success', 0)
        total_maxmind = self.stats.get('maxmind_lookups', 0)
        total_api = self.stats.get('api_lookups', 0)
        total_ipapi = self.stats.get('ipapi_lookups', 0)
        total_failures = self.stats.get('lookup_failures', 0)
        total_modern_fields = self.stats.get('modern_fields_used', 0)
        total_legacy_avoided = self.stats.get('legacy_fields_avoided', 0)
        total_basic_read = self.stats.get('basic_fields_read', 0)
        total_basic_validated = self.stats.get('basic_fields_validated', 0)
        total_invalid_basic = self.stats.get('invalid_basic_events', 0)

        self.logger.info(f"üìä Stats finales VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS - Runtime: {runtime:.1f}s")
        self.logger.info(f"   üì¶ Total eventos v3 procesados: {total_v3_events}")
        self.logger.info(f"   üéØ‚ûïüè† Total enriquecimiento dual exitoso: {total_dual_success}")
        self.logger.info(f"   üåç Total MaxMind lookups: {total_maxmind}")
        self.logger.info(f"   üåê Total API lookups: {total_api}")
        self.logger.info(f"   üåê Total IPAPI lookups: {total_ipapi}")
        self.logger.info(f"   ‚ùå Total lookup failures: {total_failures}")
        self.logger.info(f"   ‚úÖ Total campos MODERNOS usados: {total_modern_fields}")
        self.logger.info(f"   üö´ Total campos LEGACY evitados: {total_legacy_avoided}")
        self.logger.info(f"   üìñ Total campos B√ÅSICOS le√≠dos: {total_basic_read}")
        self.logger.info(f"   ‚úÖ Total campos B√ÅSICOS validados: {total_basic_validated}")
        self.logger.info(f"   ‚ùå Total eventos con campos b√°sicos inv√°lidos: {total_invalid_basic}")

        for thread in threads:
            thread.join(timeout=5)

        if self.input_socket:
            self.input_socket.close()
        if self.output_socket:
            self.output_socket.close()
        self.context.term()

        self.logger.info(
            "‚úÖ Distributed GeoIP Enricher VERTICAL v3.0.0 + IPAPI + READONLY CAMPOS B√ÅSICOS cerrado correctamente")


# üöÄ Main
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("‚ùå Uso: python geoip_enricher_v3_ipapi_READONLY_BASIC.py <config.json>")
        print("üí° Ejemplo: python geoip_enricher_v3_ipapi_READONLY_BASIC.py geoip_enricher_config.json")
        sys.exit(1)

    config_file = sys.argv[1]

    try:
        enricher = DistributedGeoIPEnricherVertical(config_file)
        enricher.run()
    except Exception as e:
        print(f"‚ùå Error fatal: {e}")
        import traceback

        traceback.print_exc()
        print(f"Stack trace completo: {traceback.format_exc()}")

        sys.exit(1)