# test_protobuf_research.py
"""
Test completo y benchmark del sistema de investigaciÃ³n Protocol Buffers.
Verifica setup, performance y prepara para comparaciÃ³n con otros protocolos.
"""

import time
import os
import sys
import statistics
import json
from typing import List, Dict, Any
from dataclasses import asdict

# AÃ±adir paths necesarios
sys.path.insert(0, 'src')
sys.path.insert(0, 'src/protocols')


# Test de imports principales
def test_imports():
    """Verifica que todos los imports funcionen correctamente."""
    print("ðŸ§ª Testing imports del sistema de investigaciÃ³n...")

    try:
        from base_interfaces import (
            EventData, SerializationMetrics, SerializationProtocol,
            CompressionAlgorithm, EncryptionAlgorithm,
            ResearchDataGenerator, SerializerFactory
        )
        print("âœ… Interfaces base importadas")

        from protobuf.protobuf_serializer import ProtobufEventSerializer
        print("âœ… Serializer Protocol Buffers importado")

        # Verificar que Protocol Buffers estÃ¡ disponible
        import scada_events_pb2 as pb
        print("âœ… Schema Protocol Buffers disponible")

        return True

    except ImportError as e:
        print(f"âŒ Error importando: {e}")
        print("ðŸ’¡ Ejecuta: bash setup_research_environment.sh")
        return False


def test_protobuf_serialization():
    """Test bÃ¡sico de serializaciÃ³n Protocol Buffers."""
    print("\nðŸ”§ Testing serializaciÃ³n Protocol Buffers...")

    from base_interfaces import ResearchDataGenerator, CompressionAlgorithm, EncryptionAlgorithm
    from protobuff.protobuf_serializer import ProtobufEventSerializer

    # Crear serializer con diferentes configuraciones
    configs = [
        {"name": "Sin compresiÃ³n ni cifrado", "compression": CompressionAlgorithm.NONE,
         "encryption": EncryptionAlgorithm.NONE},
        {"name": "Solo LZ4", "compression": CompressionAlgorithm.LZ4, "encryption": EncryptionAlgorithm.NONE},
        {"name": "LZ4 + ChaCha20", "compression": CompressionAlgorithm.LZ4, "encryption": EncryptionAlgorithm.CHACHA20}
    ]

    # Generar eventos de prueba
    generator = ResearchDataGenerator()
    security_events = generator.generate_security_events(5)
    scada_events = generator.generate_scada_alarms(5)
    network_events = generator.generate_network_anomalies(5)

    all_events = security_events + scada_events + network_events

    print(f"   Eventos generados: {len(all_events)}")

    results = {}

    for config in configs:
        print(f"\n   ConfiguraciÃ³n: {config['name']}")

        # Crear serializer
        encryption_key = os.urandom(32) if config['encryption'] != EncryptionAlgorithm.NONE else None

        # Nota: Para esta prueba, simplificamos y solo probamos casos bÃ¡sicos
        if config['compression'] == CompressionAlgorithm.NONE or config['encryption'] == EncryptionAlgorithm.NONE:
            # Para la implementaciÃ³n simplificada inicial
            serializer = ProtobufEventSerializer(
                compression=CompressionAlgorithm.LZ4,
                encryption=EncryptionAlgorithm.CHACHA20 if encryption_key else EncryptionAlgorithm.NONE,
                encryption_key=encryption_key
            )
        else:
            serializer = ProtobufEventSerializer(
                compression=config['compression'],
                encryption=config['encryption'],
                encryption_key=encryption_key
            )

        # Test con cada tipo de evento
        config_results = []

        for i, event in enumerate(all_events[:3]):  # Solo primeros 3 para prueba rÃ¡pida
            try:
                # Serializar
                serialized_data, ser_metrics = serializer.serialize(event)

                # Deserializar
                deserialized_event, deser_metrics = serializer.deserialize(serialized_data)

                if deserialized_event:
                    config_results.append({
                        "event_type": event.event_type,
                        "original_size": ser_metrics.original_size_bytes,
                        "serialized_size": ser_metrics.serialized_size_bytes,
                        "compression_ratio": ser_metrics.compression_ratio,
                        "serialization_time_us": ser_metrics.serialization_time_ns / 1000,
                        "deserialization_time_us": deser_metrics.deserialization_time_ns / 1000,
                        "total_time_us": (
                                                     ser_metrics.total_processing_time_ns + deser_metrics.total_processing_time_ns) / 1000
                    })
                    print(
                        f"     âœ… {event.event_type}: {len(serialized_data)} bytes, {ser_metrics.total_processing_time_ns / 1000:.1f}Î¼s")
                else:
                    print(f"     âŒ {event.event_type}: Error en deserializaciÃ³n")

            except Exception as e:
                print(f"     âŒ {event.event_type}: Error - {e}")

        results[config['name']] = config_results

    return results


def benchmark_performance():
    """Benchmark de performance con diferentes cargas de trabajo."""
    print("\nðŸ“Š Benchmark de performance Protocol Buffers...")

    from base_interfaces import ResearchDataGenerator
    from protobuf.protobuf_serializer import ProtobufEventSerializer

    # ConfiguraciÃ³n del benchmark
    event_counts = [100, 1000, 5000]
    encryption_key = os.urandom(32)

    serializer = ProtobufEventSerializer(
        compression=CompressionAlgorithm.LZ4,
        encryption=EncryptionAlgorithm.CHACHA20,
        encryption_key=encryption_key
    )

    benchmark_results = {}

    for count in event_counts:
        print(f"\n   Benchmarking con {count:,} eventos...")

        # Generar carga de trabajo mixta
        generator = ResearchDataGenerator()
        events = generator.generate_mixed_workload(count)

        # Benchmark de serializaciÃ³n
        start_time = time.time()
        serialized_events = []
        total_serialized_size = 0
        serialization_times = []

        for event in events:
            ser_start = time.time_ns()
            serialized_data, metrics = serializer.serialize(event)
            ser_end = time.time_ns()

            serialized_events.append(serialized_data)
            total_serialized_size += len(serialized_data)
            serialization_times.append(ser_end - ser_start)

        serialization_total_time = time.time() - start_time

        # Benchmark de deserializaciÃ³n
        start_time = time.time()
        deserialization_times = []
        successful_deserializations = 0

        for serialized_data in serialized_events:
            deser_start = time.time_ns()
            deserialized_event, metrics = serializer.deserialize(serialized_data)
            deser_end = time.time_ns()

            if deserialized_event:
                successful_deserializations += 1
            deserialization_times.append(deser_end - deser_start)

        deserialization_total_time = time.time() - start_time

        # Calcular estadÃ­sticas
        events_per_second_ser = count / serialization_total_time
        events_per_second_deser = count / deserialization_total_time
        avg_event_size = total_serialized_size / count
        throughput_mbps = (total_serialized_size / (1024 * 1024)) / serialization_total_time

        benchmark_results[count] = {
            "events_processed": count,
            "successful_deserializations": successful_deserializations,
            "serialization": {
                "total_time_seconds": serialization_total_time,
                "events_per_second": events_per_second_ser,
                "avg_time_us": statistics.mean(serialization_times) / 1000,
                "median_time_us": statistics.median(serialization_times) / 1000,
                "p95_time_us": sorted(serialization_times)[int(len(serialization_times) * 0.95)] / 1000,
                "p99_time_us": sorted(serialization_times)[int(len(serialization_times) * 0.99)] / 1000
            },
            "deserialization": {
                "total_time_seconds": deserialization_total_time,
                "events_per_second": events_per_second_deser,
                "avg_time_us": statistics.mean(deserialization_times) / 1000,
                "median_time_us": statistics.median(deserialization_times) / 1000,
                "success_rate": successful_deserializations / count
            },
            "size_metrics": {
                "total_size_bytes": total_serialized_size,
                "avg_event_size_bytes": avg_event_size,
                "throughput_mbps": throughput_mbps
            }
        }

        print(f"     SerializaciÃ³n: {events_per_second_ser:,.0f} eventos/segundo")
        print(f"     DeserializaciÃ³n: {events_per_second_deser:,.0f} eventos/segundo")
        print(f"     TamaÃ±o promedio: {avg_event_size:.1f} bytes/evento")
        print(f"     Throughput: {throughput_mbps:.1f} MB/s")

    return benchmark_results


def compare_event_types():
    """Compara performance entre diferentes tipos de eventos."""
    print("\nðŸ“ˆ ComparaciÃ³n por tipo de evento...")

    from base_interfaces import ResearchDataGenerator
    from protobuf.protobuf_serializer import ProtobufEventSerializer

    encryption_key = os.urandom(32)
    serializer = ProtobufEventSerializer(
        compression=CompressionAlgorithm.LZ4,
        encryption=EncryptionAlgorithm.CHACHA20,
        encryption_key=encryption_key
    )

    generator = ResearchDataGenerator()

    # Generar diferentes tipos de eventos
    event_types = [
        ("Security Events", generator.generate_security_events(100)),
        ("SCADA Alarms", generator.generate_scada_alarms(100)),
        ("Network Anomalies", generator.generate_network_anomalies(100))
    ]

    comparison_results = {}

    for event_type_name, events in event_types:
        print(f"\n   {event_type_name}:")

        sizes = []
        serialization_times = []
        compression_ratios = []

        for event in events[:50]:  # Test con 50 eventos por tipo
            serialized_data, metrics = serializer.serialize(event)

            sizes.append(len(serialized_data))
            serialization_times.append(metrics.serialization_time_ns / 1000)  # Î¼s
            compression_ratios.append(metrics.compression_ratio)

        comparison_results[event_type_name] = {
            "avg_size_bytes": statistics.mean(sizes),
            "avg_serialization_time_us": statistics.mean(serialization_times),
            "avg_compression_ratio": statistics.mean(compression_ratios),
            "size_std": statistics.stdev(sizes) if len(sizes) > 1 else 0,
            "time_std": statistics.stdev(serialization_times) if len(serialization_times) > 1 else 0
        }

        print(
            f"     TamaÃ±o promedio: {statistics.mean(sizes):.1f} Â± {statistics.stdev(sizes) if len(sizes) > 1 else 0:.1f} bytes")
        print(
            f"     Tiempo promedio: {statistics.mean(serialization_times):.1f} Â± {statistics.stdev(serialization_times) if len(serialization_times) > 1 else 0:.1f} Î¼s")
        print(f"     CompresiÃ³n: {statistics.mean(compression_ratios):.1f}x")

    return comparison_results


def save_results(results: Dict[str, Any]):
    """Guarda resultados del benchmark en archivo JSON."""
    os.makedirs("research_results/benchmarks", exist_ok=True)

    timestamp = int(time.time())
    filename = f"research_results/benchmarks/protobuf_benchmark_{timestamp}.json"

    with open(filename, 'w') as f:
        json.dump(results, f, indent=2, default=str)

    print(f"\nðŸ’¾ Resultados guardados en: {filename}")


def main():
    """FunciÃ³n principal del test de investigaciÃ³n."""
    print("ðŸ”¬ SCADA Protocol Research - Protocol Buffers Test Suite")
    print("=" * 70)

    # Test 1: Verificar imports y setup
    if not test_imports():
        print("\nðŸ’¥ Setup incompleto. Ejecuta setup_research_environment.sh")
        return False

    # Test 2: SerializaciÃ³n bÃ¡sica
    try:
        serialization_results = test_protobuf_serialization()
        print("âœ… Test de serializaciÃ³n completado")
    except Exception as e:
        print(f"âŒ Error en test de serializaciÃ³n: {e}")
        return False

    # Test 3: Benchmark de performance
    try:
        performance_results = benchmark_performance()
        print("âœ… Benchmark de performance completado")
    except Exception as e:
        print(f"âŒ Error en benchmark: {e}")
        return False

    # Test 4: ComparaciÃ³n por tipos de eventos
    try:
        comparison_results = compare_event_types()
        print("âœ… ComparaciÃ³n por tipos completada")
    except Exception as e:
        print(f"âŒ Error en comparaciÃ³n: {e}")
        return False

    # Compilar resultados finales
    final_results = {
        "protocol": "Protocol Buffers + LZ4 + ChaCha20",
        "timestamp": time.time(),
        "test_date": time.strftime("%Y-%m-%d %H:%M:%S"),
        "serialization_tests": serialization_results,
        "performance_benchmarks": performance_results,
        "event_type_comparison": comparison_results,
        "system_info": {
            "python_version": sys.version,
            "platform": sys.platform
        }
    }

    # Guardar resultados
    save_results(final_results)

    # Resumen ejecutivo
    print("\n" + "=" * 70)
    print("ðŸ“‹ RESUMEN EJECUTIVO - Protocol Buffers")
    print("=" * 70)

    # Extraer mÃ©tricas clave del benchmark mÃ¡s grande
    if performance_results:
        largest_test = max(performance_results.keys())
        key_metrics = performance_results[largest_test]

        print(f"ðŸŽ¯ Performance con {largest_test:,} eventos:")
        print(f"   SerializaciÃ³n: {key_metrics['serialization']['events_per_second']:,.0f} eventos/segundo")
        print(f"   DeserializaciÃ³n: {key_metrics['deserialization']['events_per_second']:,.0f} eventos/segundo")
        print(f"   Throughput: {key_metrics['size_metrics']['throughput_mbps']:.1f} MB/s")
        print(f"   TamaÃ±o promedio: {key_metrics['size_metrics']['avg_event_size_bytes']:.1f} bytes/evento")
        print(f"   Latencia P95: {key_metrics['serialization']['p95_time_us']:.1f} Î¼s")
        print(f"   Tasa Ã©xito: {key_metrics['deserialization']['success_rate'] * 100:.1f}%")

    # EvaluaciÃ³n vs targets
    target_eps = 100000  # 100K eventos/segundo
    if performance_results and largest_test:
        actual_eps = key_metrics['serialization']['events_per_second']
        target_met = actual_eps >= target_eps

        print(f"\nðŸŽ¯ EvaluaciÃ³n vs Targets:")
        print(f"   Target: >{target_eps:,} eventos/segundo")
        print(f"   Actual: {actual_eps:,.0f} eventos/segundo")
        print(f"   Status: {'âœ… TARGET MET' if target_met else 'âŒ Below target'}")

        if target_met:
            print("\nðŸŽ‰ Â¡Protocol Buffers cumple los targets de performance!")
            print("âœ… Listo para comparaciÃ³n con otros protocolos")
        else:
            print("\nâš ï¸  Performance por debajo del target")
            print("ðŸ’¡ Considera ajustar configuraciÃ³n o optimizar")

    print(f"\nðŸ“ PrÃ³ximos pasos:")
    print("   1. Implementar MessagePack + LZ4 + ChaCha20")
    print("   2. Implementar Apache Arrow + LZ4 + ChaCha20")
    print("   3. Ejecutar benchmark comparativo")
    print("   4. Integrar con pipeline ML")

    return True


if __name__ == "__main__":
    success = main()

    if success:
        print("\nðŸš€ Test de investigaciÃ³n Protocol Buffers completado exitosamente!")
        print("ðŸ“Š Resultados guardados en research_results/benchmarks/")
    else:
        print("\nðŸ’¥ Test fallÃ³. Revisar setup y dependencias.")
        sys.exit(1)


# ============================================================
# SCRIPT DE SETUP RÃPIDO
# ============================================================

def quick_setup():
    """Setup rÃ¡pido para testing."""
    print("âš¡ Setup rÃ¡pido de testing...")

    # Crear directorios necesarios
    os.makedirs("src/protocols/protobuf", exist_ok=True)
    os.makedirs("src/common", exist_ok=True)
    os.makedirs("research_results/benchmarks", exist_ok=True)

    # Crear __init__.py files
    init_files = [
        "src/__init__.py",
        "src/protocols/__init__.py",
        "src/protocols/protobuf/__init__.py",
        "src/common/__init__.py"
    ]

    for init_file in init_files:
        if not os.path.exists(init_file):
            with open(init_file, 'w') as f:
                f.write("# Auto-generated __init__.py\n")

    print("âœ… Estructura bÃ¡sica creada")


if __name__ == "__main__" and len(sys.argv) > 1 and sys.argv[1] == "--quick-setup":
    quick_setup()